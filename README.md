# Knowledge Distillation for LLMs: Thesis Experiment Suite

A complete, reproducible experiment suite for investigating knowledge distillation methods applied to Large Language Models. Designed for macOS Apple Silicon (MPS) with automatic CPU fallback.

## ğŸ“‹ Overview

This repository contains all code and notebooks needed to reproduce the experiments from **Chapter 3-4** of the thesis:

- **Tasks:** SST-2 (sentiment classification), SQuAD v1.1 (extractive QA)
- **KD Methods:**
  - **B0:** Baseline fine-tuning (no distillation)
  - **KD1:** Logit-based distillation (soft targets)
  - **KD2:** Sequence-level distillation (teacher-generated outputs)
  - **KD3:** Feature-based distillation (hidden state matching)
- **Models:**
  - Teacher: Qwen2.5-3B-Instruct (fallback) or larger 7B/8B models
  - Student S1: TinyLlama-1.1B-Chat
  - Student S2: ~350M quantized proxy (optional)

## ğŸš€ Quick Start

### 1. Clone and Setup Environment

```bash
cd /Users/pjere/Workshop/thesis-exp

# Create virtual environment
python3.11 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Environment Variables

```bash
# Copy example and edit with your HuggingFace token
cp .env.example .env

# Edit .env and set HF_TOKEN
```

Required variables in `.env`:
```
HF_TOKEN=your_huggingface_token_here
```

### 3. Run Notebooks

Launch Jupyter and run notebooks **in order** (01 â†’ 07):

```bash
jupyter notebook notebooks/
```

| Notebook | Purpose | Runtime (FAST) |
|----------|---------|----------------|
| 01_environment_setup | Verify setup, create directories | ~1 min |
| 02_data_prep_sst2 | Prepare SST-2 dataset | ~2 min |
| 03_data_prep_squad | Prepare SQuAD dataset | ~3 min |
| 04_teacher_cache_outputs | Cache teacher logits/answers | ~15-30 min |
| 05_train_baseline_and_kd1 | Baseline + logit KD training | ~30-60 min |
| 06_train_kd2_and_kd3 | Sequence + feature KD training | ~30-60 min |
| 07_benchmark_and_plots | Benchmarks, figures, tables | ~10 min |

## ğŸ“ Project Structure

```
thesis-exp/
â”œâ”€â”€ .env                    # Environment variables (HF_TOKEN, model names)
â”œâ”€â”€ .env.example            # Template for .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ experiment.yaml     # Main experiment configuration
â”‚
â”œâ”€â”€ src/                    # Python modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # Configuration loader
â”‚   â”œâ”€â”€ utils_seed.py       # Reproducibility utilities
â”‚   â”œâ”€â”€ data_sst2.py        # SST-2 data processing
â”‚   â”œâ”€â”€ data_squad.py       # SQuAD data processing
â”‚   â”œâ”€â”€ teacher_cache.py    # Teacher output caching
â”‚   â”œâ”€â”€ kd_losses.py        # KD loss functions
â”‚   â”œâ”€â”€ trainers.py         # Custom HF trainers
â”‚   â”œâ”€â”€ bench.py            # Efficiency benchmarking
â”‚   â”œâ”€â”€ plots.py            # Thesis figure generation
â”‚   â”œâ”€â”€ stats.py            # Statistical tests
â”‚   â””â”€â”€ io.py               # Run registry, CSV/JSON I/O
â”‚
â”œâ”€â”€ notebooks/              # Experiment notebooks
â”‚   â”œâ”€â”€ 01_environment_setup.ipynb
â”‚   â”œâ”€â”€ 02_data_prep_sst2.ipynb
â”‚   â”œâ”€â”€ 03_data_prep_squad.ipynb
â”‚   â”œâ”€â”€ 04_teacher_cache_outputs.ipynb
â”‚   â”œâ”€â”€ 05_train_baseline_and_kd1.ipynb
â”‚   â”œâ”€â”€ 06_train_kd2_and_kd3.ipynb
â”‚   â””â”€â”€ 07_benchmark_and_plots.ipynb
â”‚
â””â”€â”€ results/                # Generated outputs
    â”œâ”€â”€ processed_data/     # Tokenized datasets
    â”œâ”€â”€ teacher_cache/      # Cached teacher outputs
    â”œâ”€â”€ models/             # Trained model checkpoints
    â”œâ”€â”€ raw_runs/           # Individual run results
    â”œâ”€â”€ summary/            # Aggregated tables (CSV)
    â””â”€â”€ figures/            # Thesis figures (PNG)
```

## âš™ï¸ Configuration

### FAST vs FULL Mode

Edit `configs/experiment.yaml`:

```yaml
fast_mode: true   # Quick runs with small subsets (default)
# fast_mode: false  # Full experiments for thesis
```

| Setting | FAST Mode | FULL Mode |
|---------|-----------|-----------|
| SST-2 train | 500 samples | Full (~67k) |
| SQuAD train | 200 samples | Full (~87k) |
| Epochs | 1 | 3 |
| KD1 Grid | Tâˆˆ{2,4}, Î±âˆˆ{0.3,0.5} | Tâˆˆ{1,2,4,8}, Î±âˆˆ{0.1,0.3,0.5,0.7} |
| Seeds | [42] | [42, 123, 456] |

### Changing Models

Override in `.env`:
```bash
TEACHER_PRIMARY=meta-llama/Llama-3.1-8B-Instruct
TEACHER_FALLBACK=Qwen/Qwen2.5-3B-Instruct
STUDENT_S1=TinyLlama/TinyLlama-1.1B-Chat-v1.0
```

## ğŸ macOS Apple Silicon Notes

This suite is optimized for MPS (Metal Performance Shaders):

- **fp32 precision** used throughout (fp16/bf16 can be unstable on MPS)
- **Gradient checkpointing** enabled for memory efficiency
- **LoRA/PEFT** for parameter-efficient training
- **Periodic cache clearing** via `torch.mps.empty_cache()`
- **Automatic fallback** to smaller teacher if OOM occurs

### Memory Requirements

| Mode | Recommended RAM | Teacher |
|------|-----------------|---------|
| FAST | 16GB+ | 3B fallback |
| FULL | 32GB+ | 7B/8B primary |

## ğŸ“Š Outputs

### Tables (Chapter 4)

Generated in `results/summary/`:
- `table_4_1_main_results.csv` - Main performance comparison
- `table_4_2_kd1_ablation.csv` - Temperature Ã— Alpha grid
- `table_4_3_significance.csv` - Statistical tests (t-test, Cohen's d)
- `benchmarks.csv` - Latency, throughput, memory

### Figures (Chapter 4)

Generated in `results/figures/`:
- `fig_4_1_performance_vs_size.png`
- `fig_4_2_latency.png`
- `fig_4_3_kd_comparison.png`
- `fig_4_4_pareto.png`
- `fig_4_5_kd_gain.png`
- `fig_4_6_memory.png`

## ğŸ”„ Reproducibility

- **Seeds:** Configurable in `experiment.yaml` (default: 42, 123, 456)
- **Idempotent runs:** RunRegistry skips completed experiments
- **Deterministic:** `set_seed()` applied before each training run
- **Version pinning:** See `requirements.txt`

## ğŸ› ï¸ Troubleshooting

### "MPS backend out of memory"
1. Reduce batch size in notebook training args
2. Enable `fast_mode: true` in config
3. Use smaller fallback teacher
4. Close other applications

### "Model not found"
1. Verify HF_TOKEN in `.env`
2. Check model name spelling
3. Some models require access approval on HuggingFace

### Slow training on MPS
- This is expected; MPS is slower than CUDA for LLMs
- Use FAST mode for development
- Run FULL mode overnight or on cloud GPU

## ğŸ“ Citation

If you use this code in your research, please cite:

```bibtex
@mastersthesis{pauljere2026kd,
  title={Knowledge Distillation for Efficient Large Language Models},
  author={Paul Jere},
  year={2026},
  school={WSB University}
}
```

## ğŸ“„ License

MIT License - See LICENSE file for details.
