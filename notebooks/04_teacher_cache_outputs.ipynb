{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84fb4c57",
   "metadata": {},
   "source": [
    "# 04 - Teacher Output Caching\n",
    "\n",
    "**Thesis Section Reference:** Chapter 3.8 - Knowledge Distillation Methods\n",
    "\n",
    "This notebook caches teacher model outputs for knowledge distillation:\n",
    "1. **KD1 (Logit-based):** Cache soft logits for both SST-2 and SQuAD\n",
    "2. **KD2 (Sequence-level):** Generate and cache teacher answers for SQuAD\n",
    "3. **KD3 (Feature-based):** Cache hidden states (memory-safe chunked saving)\n",
    "\n",
    "## Memory Management Notes\n",
    "- Uses fp32 for MPS stability\n",
    "- Gradient checkpointing enabled\n",
    "- Periodic cache clearing\n",
    "- Automatic fallback to smaller teacher if OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7610fab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: FAST\n",
      "Device: mps\n",
      "Seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Standard setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "ROOT_DIR = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(ROOT_DIR / \"src\"))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(ROOT_DIR / \".env\")\n",
    "\n",
    "from config import load_config\n",
    "from utils_seed import set_seed\n",
    "\n",
    "config = load_config(str(ROOT_DIR / \"configs\" / \"experiment.yaml\"))\n",
    "config.ensure_dirs()\n",
    "\n",
    "SEED = config.get_seeds()[0]\n",
    "set_seed(SEED)\n",
    "\n",
    "# Device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Mode: {'FAST' if config.fast_mode else 'FULL'}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa6b8458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/pjere/Workshop/thesis-exp/results/processed_data\n",
      "Cache directory: /Users/pjere/Workshop/thesis-exp/results/teacher_cache\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "DATA_DIR = ROOT_DIR / \"results\" / \"processed_data\"\n",
    "CACHE_DIR = ROOT_DIR / \"results\" / \"teacher_cache\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Cache directory: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "849db3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache status:\n",
      "  sst2_logits: ✗ needs generation\n",
      "  squad_logits: ✗ needs generation\n",
      "  squad_answers: ✗ needs generation\n",
      "  hidden_states: ✗ needs generation\n"
     ]
    }
   ],
   "source": [
    "# Check what already exists\n",
    "cache_status = {\n",
    "    \"sst2_logits\": (CACHE_DIR / \"sst2_logits.pt\").exists(),\n",
    "    \"squad_logits\": (CACHE_DIR / \"squad_logits.pt\").exists(),\n",
    "    \"squad_answers\": (CACHE_DIR / \"squad_teacher_answers.json\").exists(),\n",
    "    \"hidden_states\": any(CACHE_DIR.glob(\"hidden_states_*.pt\"))\n",
    "}\n",
    "\n",
    "print(\"Cache status:\")\n",
    "for name, exists in cache_status.items():\n",
    "    status = \"✓ exists\" if exists else \"✗ needs generation\"\n",
    "    print(f\"  {name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ab14e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load: meta-llama/Llama-3.2-8B-Instruct\n",
      "Failed to load meta-llama/Llama-3.2-8B-Instruct: meta-llama/Llama-3.2-8B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n",
      "Attempting to load: Qwen/Qwen2.5-3B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3a669e9ea1464fb29690c73df2b3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully loaded: Qwen/Qwen2.5-3B-Instruct\n",
      "\n",
      "Teacher model parameters: 3.09B\n"
     ]
    }
   ],
   "source": [
    "# Load teacher model with fallback\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_teacher_model(config, device):\n",
    "    \"\"\"Load teacher with automatic fallback to smaller model.\"\"\"\n",
    "    # Try primary teacher first\n",
    "    primary = os.getenv(\"TEACHER_PRIMARY\", config.teacher.primary)\n",
    "    fallback = os.getenv(\"TEACHER_FALLBACK\", config.teacher.local_fallback)\n",
    "    \n",
    "    for model_name in [primary, fallback]:\n",
    "        try:\n",
    "            print(f\"Attempting to load: {model_name}\")\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "            )\n",
    "            \n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                dtype=torch.float32,  # fp32 for MPS stability\n",
    "                cache_dir=str(ROOT_DIR / \"hf_cache\"),\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Enable gradient checkpointing for memory\n",
    "            if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "                model.gradient_checkpointing_enable()\n",
    "            \n",
    "            print(f\"✓ Successfully loaded: {model_name}\")\n",
    "            return model, tokenizer, model_name\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {model_name}: {e}\")\n",
    "            if device.type == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "            gc.collect()\n",
    "            continue\n",
    "    \n",
    "    raise RuntimeError(\"Failed to load any teacher model\")\n",
    "\n",
    "# Only load if we need to generate something\n",
    "needs_teacher = not all(cache_status.values())\n",
    "\n",
    "if needs_teacher:\n",
    "    teacher_model, teacher_tokenizer, teacher_name = load_teacher_model(config, DEVICE)\n",
    "    print(f\"\\nTeacher model parameters: {sum(p.numel() for p in teacher_model.parameters()) / 1e9:.2f}B\")\n",
    "else:\n",
    "    print(\"All caches exist, skipping teacher loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45146afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed datasets...\n",
      "  SST-2 train: 2000 examples\n",
      "  SQuAD train: 2000 examples\n"
     ]
    }
   ],
   "source": [
    "# Load processed datasets\n",
    "from datasets import load_from_disk\n",
    "\n",
    "if needs_teacher:\n",
    "    print(\"Loading processed datasets...\")\n",
    "    \n",
    "    sst2_train = load_from_disk(str(DATA_DIR / \"sst2_train\"))\n",
    "    squad_train = load_from_disk(str(DATA_DIR / \"squad_train\"))\n",
    "    \n",
    "    print(f\"  SST-2 train: {len(sst2_train)} examples\")\n",
    "    print(f\"  SQuAD train: {len(squad_train)} examples\")\n",
    "    \n",
    "    # Load prompts for KD2\n",
    "    with open(DATA_DIR / \"squad_train_prompts.json\", \"r\") as f:\n",
    "        squad_prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf280a",
   "metadata": {},
   "source": [
    "## KD1: Cache Teacher Logits\n",
    "\n",
    "For logit-based KD, we need the teacher's output logits for each training example.\n",
    "We'll store the top-k logits to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e62375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching SST-2 teacher logits...\n",
      "Using memory-efficient chunked approach with top-k storage\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81182b7df13468ca661edd3be78ec82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SST-2 logits:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved chunk 0\n",
      "  Saved chunk 1\n",
      "  Saved chunk 2\n",
      "  Saved chunk 3\n",
      "  Saved chunk 4\n",
      "  Saved chunk 5\n",
      "  Saved chunk 6\n",
      "  Saved chunk 7\n",
      "  Saved chunk 8\n",
      "  Saved chunk 9\n",
      "  Saved chunk 10\n",
      "  Saved chunk 11\n",
      "  Saved chunk 12\n",
      "  Saved chunk 13\n",
      "  Saved chunk 14\n",
      "  Saved chunk 15\n",
      "  Saved chunk 16\n",
      "  Saved chunk 17\n",
      "  Saved chunk 18\n",
      "  Saved chunk 19\n",
      "  Saved chunk 20\n",
      "  Saved chunk 21\n",
      "  Saved chunk 22\n",
      "  Saved chunk 23\n",
      "  Saved chunk 24\n",
      "  Saved chunk 25\n",
      "  Saved chunk 26\n",
      "  Saved chunk 27\n",
      "  Saved chunk 28\n",
      "  Saved chunk 29\n",
      "  Saved chunk 30\n",
      "  Saved chunk 31\n",
      "  Saved chunk 32\n",
      "  Saved chunk 33\n",
      "  Saved chunk 34\n",
      "  Saved chunk 35\n",
      "  Saved chunk 36\n",
      "  Saved chunk 37\n",
      "  Saved chunk 38\n",
      "  Saved chunk 39\n",
      "✓ Saved SST-2 logits in 40 chunks\n"
     ]
    }
   ],
   "source": [
    "# Cache SST-2 teacher logits (memory-efficient approach)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if not cache_status[\"sst2_logits\"]:\n",
    "    print(\"Caching SST-2 teacher logits...\")\n",
    "    print(\"Using memory-efficient chunked approach with top-k storage\")\n",
    "    \n",
    "    TOP_K = 50  # Only store top-50 logits per position to save memory\n",
    "    CHUNK_SIZE = 50  # Save every 50 examples\n",
    "    \n",
    "    all_logits = []\n",
    "    all_indices = []\n",
    "    chunk_idx = 0\n",
    "    \n",
    "    teacher_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(sst2_train)), desc=\"SST-2 logits\"):\n",
    "            example = sst2_train[i]\n",
    "            \n",
    "            # Handle both tensor and list formats\n",
    "            if isinstance(example[\"input_ids\"], torch.Tensor):\n",
    "                input_ids = example[\"input_ids\"].unsqueeze(0).to(DEVICE)\n",
    "                attention_mask = example[\"attention_mask\"].unsqueeze(0).to(DEVICE)\n",
    "            else:\n",
    "                input_ids = torch.tensor([example[\"input_ids\"]], device=DEVICE)\n",
    "                attention_mask = torch.tensor([example[\"attention_mask\"]], device=DEVICE)\n",
    "            \n",
    "            outputs = teacher_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=False\n",
    "            )\n",
    "            \n",
    "            # Get top-k logits only (huge memory savings)\n",
    "            logits = outputs.logits[0]  # [seq_len, vocab_size]\n",
    "            top_values, top_indices = torch.topk(logits, k=TOP_K, dim=-1)\n",
    "            \n",
    "            all_logits.append(top_values.cpu().half())\n",
    "            all_indices.append(top_indices.cpu().int())\n",
    "            \n",
    "            # Clear intermediate tensors\n",
    "            del outputs, logits, top_values, top_indices, input_ids, attention_mask\n",
    "            \n",
    "            # Save chunk periodically\n",
    "            if len(all_logits) >= CHUNK_SIZE:\n",
    "                chunk_data = {\n",
    "                    \"logits\": [l for l in all_logits],\n",
    "                    \"indices\": [idx for idx in all_indices]\n",
    "                }\n",
    "                torch.save(chunk_data, CACHE_DIR / f\"sst2_logits_chunk_{chunk_idx}.pt\")\n",
    "                print(f\"  Saved chunk {chunk_idx}\")\n",
    "                all_logits = []\n",
    "                all_indices = []\n",
    "                chunk_idx += 1\n",
    "                \n",
    "                if DEVICE.type == \"mps\":\n",
    "                    torch.mps.empty_cache()\n",
    "                gc.collect()\n",
    "    \n",
    "    # Save final chunk\n",
    "    if all_logits:\n",
    "        chunk_data = {\n",
    "            \"logits\": all_logits,\n",
    "            \"indices\": all_indices\n",
    "        }\n",
    "        torch.save(chunk_data, CACHE_DIR / f\"sst2_logits_chunk_{chunk_idx}.pt\")\n",
    "        print(f\"  Saved final chunk {chunk_idx}\")\n",
    "        chunk_idx += 1\n",
    "    \n",
    "    # Save metadata\n",
    "    meta = {\"num_chunks\": chunk_idx, \"top_k\": TOP_K, \"task\": \"sst2\"}\n",
    "    torch.save(meta, CACHE_DIR / \"sst2_logits.pt\")  # Marker file with metadata\n",
    "    \n",
    "    print(f\"✓ Saved SST-2 logits in {chunk_idx} chunks\")\n",
    "    \n",
    "    if DEVICE.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"✓ SST-2 logits already cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cf7a97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching SQuAD teacher logits...\n",
      "Using memory-efficient chunked approach with top-k storage\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4c4eb12f0f430bbb5af561a23fc603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SQuAD logits:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved chunk 0\n",
      "  Saved chunk 1\n",
      "  Saved chunk 2\n",
      "  Saved chunk 3\n",
      "  Saved chunk 4\n",
      "  Saved chunk 5\n",
      "  Saved chunk 6\n",
      "  Saved chunk 7\n",
      "  Saved chunk 8\n",
      "  Saved chunk 9\n",
      "  Saved chunk 10\n",
      "  Saved chunk 11\n",
      "  Saved chunk 12\n",
      "  Saved chunk 13\n",
      "  Saved chunk 14\n",
      "  Saved chunk 15\n",
      "  Saved chunk 16\n",
      "  Saved chunk 17\n",
      "  Saved chunk 18\n",
      "  Saved chunk 19\n",
      "  Saved chunk 20\n",
      "  Saved chunk 21\n",
      "  Saved chunk 22\n",
      "  Saved chunk 23\n",
      "  Saved chunk 24\n",
      "  Saved chunk 25\n",
      "  Saved chunk 26\n",
      "  Saved chunk 27\n",
      "  Saved chunk 28\n",
      "  Saved chunk 29\n",
      "  Saved chunk 30\n",
      "  Saved chunk 31\n",
      "  Saved chunk 32\n",
      "  Saved chunk 33\n",
      "  Saved chunk 34\n",
      "  Saved chunk 35\n",
      "  Saved chunk 36\n",
      "  Saved chunk 37\n",
      "  Saved chunk 38\n",
      "  Saved chunk 39\n",
      "  Saved chunk 40\n",
      "  Saved chunk 41\n",
      "  Saved chunk 42\n",
      "  Saved chunk 43\n",
      "  Saved chunk 44\n",
      "  Saved chunk 45\n",
      "  Saved chunk 46\n",
      "  Saved chunk 47\n",
      "  Saved chunk 48\n",
      "  Saved chunk 49\n",
      "  Saved chunk 50\n",
      "  Saved chunk 51\n",
      "  Saved chunk 52\n",
      "  Saved chunk 53\n",
      "  Saved chunk 54\n",
      "  Saved chunk 55\n",
      "  Saved chunk 56\n",
      "  Saved chunk 57\n",
      "  Saved chunk 58\n",
      "  Saved chunk 59\n",
      "  Saved chunk 60\n",
      "  Saved chunk 61\n",
      "  Saved chunk 62\n",
      "  Saved chunk 63\n",
      "  Saved chunk 64\n",
      "  Saved chunk 65\n",
      "  Saved chunk 66\n",
      "  Saved chunk 67\n",
      "  Saved chunk 68\n",
      "  Saved chunk 69\n",
      "  Saved chunk 70\n",
      "  Saved chunk 71\n",
      "  Saved chunk 72\n",
      "  Saved chunk 73\n",
      "  Saved chunk 74\n",
      "  Saved chunk 75\n",
      "  Saved chunk 76\n",
      "  Saved chunk 77\n",
      "  Saved chunk 78\n",
      "  Saved chunk 79\n",
      "✓ Saved SQuAD logits in 80 chunks\n"
     ]
    }
   ],
   "source": [
    "# Cache SQuAD teacher logits (memory-efficient approach)\n",
    "if not cache_status[\"squad_logits\"]:\n",
    "    print(\"Caching SQuAD teacher logits...\")\n",
    "    print(\"Using memory-efficient chunked approach with top-k storage\")\n",
    "    \n",
    "    TOP_K = 50\n",
    "    CHUNK_SIZE = 25  # Smaller chunks for longer sequences\n",
    "    \n",
    "    all_logits = []\n",
    "    all_indices = []\n",
    "    chunk_idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(squad_train)), desc=\"SQuAD logits\"):\n",
    "            example = squad_train[i]\n",
    "            \n",
    "            if isinstance(example[\"input_ids\"], torch.Tensor):\n",
    "                input_ids = example[\"input_ids\"].unsqueeze(0).to(DEVICE)\n",
    "                attention_mask = example[\"attention_mask\"].unsqueeze(0).to(DEVICE)\n",
    "            else:\n",
    "                input_ids = torch.tensor([example[\"input_ids\"]], device=DEVICE)\n",
    "                attention_mask = torch.tensor([example[\"attention_mask\"]], device=DEVICE)\n",
    "            \n",
    "            outputs = teacher_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=False\n",
    "            )\n",
    "            \n",
    "            logits = outputs.logits[0]\n",
    "            top_values, top_indices = torch.topk(logits, k=TOP_K, dim=-1)\n",
    "            \n",
    "            all_logits.append(top_values.cpu().half())\n",
    "            all_indices.append(top_indices.cpu().int())\n",
    "            \n",
    "            del outputs, logits, top_values, top_indices, input_ids, attention_mask\n",
    "            \n",
    "            if len(all_logits) >= CHUNK_SIZE:\n",
    "                chunk_data = {\n",
    "                    \"logits\": all_logits,\n",
    "                    \"indices\": all_indices\n",
    "                }\n",
    "                torch.save(chunk_data, CACHE_DIR / f\"squad_logits_chunk_{chunk_idx}.pt\")\n",
    "                print(f\"  Saved chunk {chunk_idx}\")\n",
    "                all_logits = []\n",
    "                all_indices = []\n",
    "                chunk_idx += 1\n",
    "                \n",
    "                if DEVICE.type == \"mps\":\n",
    "                    torch.mps.empty_cache()\n",
    "                gc.collect()\n",
    "    \n",
    "    if all_logits:\n",
    "        chunk_data = {\n",
    "            \"logits\": all_logits,\n",
    "            \"indices\": all_indices\n",
    "        }\n",
    "        torch.save(chunk_data, CACHE_DIR / f\"squad_logits_chunk_{chunk_idx}.pt\")\n",
    "        print(f\"  Saved final chunk {chunk_idx}\")\n",
    "        chunk_idx += 1\n",
    "    \n",
    "    meta = {\"num_chunks\": chunk_idx, \"top_k\": TOP_K, \"task\": \"squad\"}\n",
    "    torch.save(meta, CACHE_DIR / \"squad_logits.pt\")\n",
    "    \n",
    "    print(f\"✓ Saved SQuAD logits in {chunk_idx} chunks\")\n",
    "    \n",
    "    if DEVICE.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"✓ SQuAD logits already cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab3b0c",
   "metadata": {},
   "source": [
    "## KD2: Generate Teacher Answers (Sequence-level KD)\n",
    "\n",
    "For sequence-level KD on SQuAD, we generate teacher's predicted answers.\n",
    "The student learns to mimic the teacher's generated sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be6f36dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating teacher answers for SQuAD...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900bd29aaa384e608d12ade362ecf2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Generated 2000 teacher answers\n",
      "\n",
      "Sample teacher answers:\n",
      "  Q: What percentage of Egyptians polled support death ...\n",
      "  Teacher: According to the context provided, 84% of Egyptian...\n",
      "  Gold: 84%\n",
      "\n",
      "  Q: Ann Arbor ranks 1st among what goods sold?\n",
      "\n",
      "Answer...\n",
      "  Teacher: Ann Arbor ranks 1st among U.S. cities in the numbe...\n",
      "  Gold: books\n",
      "\n",
      "  Q: In developing countries, who makes most of the spe...\n",
      "  Teacher: In developing countries, the executive branch make...\n",
      "  Gold: the executive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate teacher answers for SQuAD\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if not cache_status[\"squad_answers\"]:\n",
    "    print(\"Generating teacher answers for SQuAD...\")\n",
    "    \n",
    "    teacher_answers = []\n",
    "    \n",
    "    # Generation config\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"do_sample\": False,  # Greedy for reproducibility\n",
    "        \"pad_token_id\": teacher_tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": teacher_tokenizer.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    batch_size = 1  # One at a time for MPS stability\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(squad_prompts), batch_size), desc=\"Generating\"):\n",
    "            batch = squad_prompts[i:i+batch_size]\n",
    "            \n",
    "            for item in batch:\n",
    "                inputs = teacher_tokenizer(\n",
    "                    item[\"prompt\"],\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=config.get_max_length(\"squad\") - 64,\n",
    "                    truncation=True\n",
    "                ).to(DEVICE)\n",
    "                \n",
    "                outputs = teacher_model.generate(\n",
    "                    **inputs,\n",
    "                    **gen_config\n",
    "                )\n",
    "                \n",
    "                # Decode only new tokens\n",
    "                generated = teacher_tokenizer.decode(\n",
    "                    outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "                    skip_special_tokens=True\n",
    "                ).strip()\n",
    "                \n",
    "                teacher_answers.append({\n",
    "                    \"id\": item[\"id\"],\n",
    "                    \"prompt\": item[\"prompt\"],\n",
    "                    \"teacher_answer\": generated,\n",
    "                    \"gold_answers\": item[\"gold_answers\"]\n",
    "                })\n",
    "            \n",
    "            # Periodic cleanup\n",
    "            if i % 50 == 0 and DEVICE.type == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "    \n",
    "    # Save\n",
    "    with open(CACHE_DIR / \"squad_teacher_answers.json\", \"w\") as f:\n",
    "        json.dump(teacher_answers, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Generated {len(teacher_answers)} teacher answers\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\nSample teacher answers:\")\n",
    "    for sample in teacher_answers[:3]:\n",
    "        print(f\"  Q: {sample['prompt'].split('Question:')[1].split('Context:')[0].strip()[:50]}...\")\n",
    "        print(f\"  Teacher: {sample['teacher_answer'][:50]}...\")\n",
    "        print(f\"  Gold: {sample['gold_answers'][0]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"✓ SQuAD teacher answers already cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447049c0",
   "metadata": {},
   "source": [
    "## KD3: Cache Hidden States (Feature-based KD)\n",
    "\n",
    "For feature-based KD, we cache teacher's hidden states from selected layers.\n",
    "This is memory-intensive, so we:\n",
    "1. Store only selected layers (e.g., every 4th layer)\n",
    "2. Use float16 for storage\n",
    "3. Save in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a85a268e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching hidden states for KD3...\n",
      "Note: This is memory-intensive. Using chunked saving.\n",
      "  Caching layers: [0, 4, 8, 12, 16, 20, 24, 28, 32] (of 36 total)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0597510e63b540738489995ae8a1a2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting hidden states:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved chunk 0 (100 examples)\n",
      "  Saved chunk 1 (100 examples)\n",
      "  Saved chunk 2 (100 examples)\n",
      "  Saved chunk 3 (100 examples)\n",
      "  Saved chunk 4 (100 examples)\n",
      "  Saved chunk 5 (100 examples)\n",
      "  Saved chunk 6 (100 examples)\n",
      "  Saved chunk 7 (100 examples)\n",
      "  Saved chunk 8 (100 examples)\n",
      "  Saved chunk 9 (100 examples)\n",
      "  Saved chunk 10 (100 examples)\n",
      "  Saved chunk 11 (100 examples)\n",
      "  Saved chunk 12 (100 examples)\n",
      "  Saved chunk 13 (100 examples)\n",
      "  Saved chunk 14 (100 examples)\n",
      "  Saved chunk 15 (100 examples)\n",
      "  Saved chunk 16 (100 examples)\n",
      "  Saved chunk 17 (100 examples)\n",
      "  Saved chunk 18 (100 examples)\n",
      "  Saved chunk 19 (100 examples)\n",
      "\n",
      "✓ Saved hidden states in 21 chunks\n"
     ]
    }
   ],
   "source": [
    "# Cache hidden states (memory-safe)\n",
    "if not cache_status[\"hidden_states\"]:\n",
    "    print(\"Caching hidden states for KD3...\")\n",
    "    print(\"Note: This is memory-intensive. Using chunked saving.\")\n",
    "    \n",
    "    # Configuration\n",
    "    LAYER_STRIDE = 4  # Cache every 4th layer\n",
    "    CHUNK_SIZE = 100  # Save every 100 examples\n",
    "    USE_SST2_ONLY = config.fast_mode  # In fast mode, only cache SST-2\n",
    "    \n",
    "    dataset = sst2_train\n",
    "    task_name = \"sst2\"\n",
    "    \n",
    "    num_layers = teacher_model.config.num_hidden_layers\n",
    "    selected_layers = list(range(0, num_layers, LAYER_STRIDE))\n",
    "    print(f\"  Caching layers: {selected_layers} (of {num_layers} total)\")\n",
    "    \n",
    "    all_hidden_states = []\n",
    "    chunk_idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataset)), desc=\"Extracting hidden states\"):\n",
    "            example = dataset[i]\n",
    "            \n",
    "            # Prepare input\n",
    "            input_ids = torch.tensor([example[\"input_ids\"]], device=DEVICE)\n",
    "            attention_mask = torch.tensor([example[\"attention_mask\"]], device=DEVICE)\n",
    "            \n",
    "            # Get hidden states\n",
    "            outputs = teacher_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            \n",
    "            # Extract selected layers and convert to float16\n",
    "            layer_states = []\n",
    "            for layer_idx in selected_layers:\n",
    "                # Mean pool over sequence length to reduce memory\n",
    "                layer_output = outputs.hidden_states[layer_idx + 1]  # +1 for embeddings\n",
    "                pooled = layer_output.mean(dim=1)  # [1, hidden_size]\n",
    "                layer_states.append(pooled.cpu().half())\n",
    "            \n",
    "            # Stack layers: [num_selected_layers, hidden_size]\n",
    "            stacked = torch.cat(layer_states, dim=0)\n",
    "            all_hidden_states.append(stacked)\n",
    "            \n",
    "            # Save chunk\n",
    "            if len(all_hidden_states) >= CHUNK_SIZE:\n",
    "                chunk_tensor = torch.stack(all_hidden_states)\n",
    "                torch.save(chunk_tensor, CACHE_DIR / f\"hidden_states_{task_name}_{chunk_idx}.pt\")\n",
    "                print(f\"  Saved chunk {chunk_idx} ({len(all_hidden_states)} examples)\")\n",
    "                all_hidden_states = []\n",
    "                chunk_idx += 1\n",
    "                \n",
    "                if DEVICE.type == \"mps\":\n",
    "                    torch.mps.empty_cache()\n",
    "    \n",
    "    # Save final chunk\n",
    "    if all_hidden_states:\n",
    "        chunk_tensor = torch.stack(all_hidden_states)\n",
    "        torch.save(chunk_tensor, CACHE_DIR / f\"hidden_states_{task_name}_{chunk_idx}.pt\")\n",
    "        print(f\"  Saved chunk {chunk_idx} ({len(all_hidden_states)} examples)\")\n",
    "    \n",
    "    # Save metadata\n",
    "    hidden_state_meta = {\n",
    "        \"task\": task_name,\n",
    "        \"num_chunks\": chunk_idx + 1,\n",
    "        \"selected_layers\": selected_layers,\n",
    "        \"total_layers\": num_layers,\n",
    "        \"hidden_size\": teacher_model.config.hidden_size,\n",
    "        \"pooling\": \"mean\",\n",
    "        \"dtype\": \"float16\"\n",
    "    }\n",
    "    \n",
    "    with open(CACHE_DIR / f\"hidden_states_{task_name}_meta.json\", \"w\") as f:\n",
    "        json.dump(hidden_state_meta, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Saved hidden states in {chunk_idx + 1} chunks\")\n",
    "else:\n",
    "    print(\"✓ Hidden states already cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "749b8ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up...\n",
      "✓ Memory freed\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "if needs_teacher:\n",
    "    print(\"Cleaning up...\")\n",
    "    del teacher_model\n",
    "    if 'cache' in dir():\n",
    "        del cache\n",
    "    if DEVICE.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"✓ Memory freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b5d1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying cached files...\n",
      "\n",
      "Cache directory: /Users/pjere/Workshop/thesis-exp/results/teacher_cache\n",
      "Total size: 514.70 MB\n",
      "\n",
      "  .gitkeep: 0.00 MB\n",
      "  hidden_states_sst2_0.pt: 3.52 MB\n",
      "  hidden_states_sst2_1.pt: 3.52 MB\n",
      "  hidden_states_sst2_10.pt: 3.52 MB\n",
      "  hidden_states_sst2_11.pt: 3.52 MB\n",
      "  hidden_states_sst2_12.pt: 3.52 MB\n",
      "  hidden_states_sst2_13.pt: 3.52 MB\n",
      "  hidden_states_sst2_14.pt: 3.52 MB\n",
      "  hidden_states_sst2_15.pt: 3.52 MB\n",
      "  hidden_states_sst2_16.pt: 3.52 MB\n",
      "  hidden_states_sst2_17.pt: 3.52 MB\n",
      "  hidden_states_sst2_18.pt: 3.52 MB\n",
      "  hidden_states_sst2_19.pt: 3.52 MB\n",
      "  hidden_states_sst2_2.pt: 3.52 MB\n",
      "  hidden_states_sst2_3.pt: 3.52 MB\n",
      "  hidden_states_sst2_4.pt: 3.52 MB\n",
      "  hidden_states_sst2_5.pt: 3.52 MB\n",
      "  hidden_states_sst2_6.pt: 3.52 MB\n",
      "  hidden_states_sst2_7.pt: 3.52 MB\n",
      "  hidden_states_sst2_8.pt: 3.52 MB\n",
      "  hidden_states_sst2_9.pt: 3.52 MB\n",
      "  hidden_states_sst2_meta.json: 0.00 MB\n",
      "  squad_logits.pt: 0.00 MB\n",
      "  squad_logits_chunk_0.pt: 3.68 MB\n",
      "  squad_logits_chunk_1.pt: 3.68 MB\n",
      "  squad_logits_chunk_10.pt: 3.68 MB\n",
      "  squad_logits_chunk_11.pt: 3.68 MB\n",
      "  squad_logits_chunk_12.pt: 3.68 MB\n",
      "  squad_logits_chunk_13.pt: 3.68 MB\n",
      "  squad_logits_chunk_14.pt: 3.68 MB\n",
      "  squad_logits_chunk_15.pt: 3.68 MB\n",
      "  squad_logits_chunk_16.pt: 3.68 MB\n",
      "  squad_logits_chunk_17.pt: 3.68 MB\n",
      "  squad_logits_chunk_18.pt: 3.68 MB\n",
      "  squad_logits_chunk_19.pt: 3.68 MB\n",
      "  squad_logits_chunk_2.pt: 3.68 MB\n",
      "  squad_logits_chunk_20.pt: 3.68 MB\n",
      "  squad_logits_chunk_21.pt: 3.68 MB\n",
      "  squad_logits_chunk_22.pt: 3.68 MB\n",
      "  squad_logits_chunk_23.pt: 3.68 MB\n",
      "  squad_logits_chunk_24.pt: 3.68 MB\n",
      "  squad_logits_chunk_25.pt: 3.68 MB\n",
      "  squad_logits_chunk_26.pt: 3.68 MB\n",
      "  squad_logits_chunk_27.pt: 3.68 MB\n",
      "  squad_logits_chunk_28.pt: 3.68 MB\n",
      "  squad_logits_chunk_29.pt: 3.68 MB\n",
      "  squad_logits_chunk_3.pt: 3.68 MB\n",
      "  squad_logits_chunk_30.pt: 3.68 MB\n",
      "  squad_logits_chunk_31.pt: 3.68 MB\n",
      "  squad_logits_chunk_32.pt: 3.68 MB\n",
      "  squad_logits_chunk_33.pt: 3.68 MB\n",
      "  squad_logits_chunk_34.pt: 3.68 MB\n",
      "  squad_logits_chunk_35.pt: 3.68 MB\n",
      "  squad_logits_chunk_36.pt: 3.68 MB\n",
      "  squad_logits_chunk_37.pt: 3.68 MB\n",
      "  squad_logits_chunk_38.pt: 3.68 MB\n",
      "  squad_logits_chunk_39.pt: 3.68 MB\n",
      "  squad_logits_chunk_4.pt: 3.68 MB\n",
      "  squad_logits_chunk_40.pt: 3.68 MB\n",
      "  squad_logits_chunk_41.pt: 3.68 MB\n",
      "  squad_logits_chunk_42.pt: 3.68 MB\n",
      "  squad_logits_chunk_43.pt: 3.68 MB\n",
      "  squad_logits_chunk_44.pt: 3.68 MB\n",
      "  squad_logits_chunk_45.pt: 3.68 MB\n",
      "  squad_logits_chunk_46.pt: 3.68 MB\n",
      "  squad_logits_chunk_47.pt: 3.68 MB\n",
      "  squad_logits_chunk_48.pt: 3.68 MB\n",
      "  squad_logits_chunk_49.pt: 3.68 MB\n",
      "  squad_logits_chunk_5.pt: 3.68 MB\n",
      "  squad_logits_chunk_50.pt: 3.68 MB\n",
      "  squad_logits_chunk_51.pt: 3.68 MB\n",
      "  squad_logits_chunk_52.pt: 3.68 MB\n",
      "  squad_logits_chunk_53.pt: 3.68 MB\n",
      "  squad_logits_chunk_54.pt: 3.68 MB\n",
      "  squad_logits_chunk_55.pt: 3.68 MB\n",
      "  squad_logits_chunk_56.pt: 3.68 MB\n",
      "  squad_logits_chunk_57.pt: 3.68 MB\n",
      "  squad_logits_chunk_58.pt: 3.68 MB\n",
      "  squad_logits_chunk_59.pt: 3.68 MB\n",
      "  squad_logits_chunk_6.pt: 3.68 MB\n",
      "  squad_logits_chunk_60.pt: 3.68 MB\n",
      "  squad_logits_chunk_61.pt: 3.68 MB\n",
      "  squad_logits_chunk_62.pt: 3.68 MB\n",
      "  squad_logits_chunk_63.pt: 3.68 MB\n",
      "  squad_logits_chunk_64.pt: 3.68 MB\n",
      "  squad_logits_chunk_65.pt: 3.68 MB\n",
      "  squad_logits_chunk_66.pt: 3.68 MB\n",
      "  squad_logits_chunk_67.pt: 3.68 MB\n",
      "  squad_logits_chunk_68.pt: 3.68 MB\n",
      "  squad_logits_chunk_69.pt: 3.68 MB\n",
      "  squad_logits_chunk_7.pt: 3.68 MB\n",
      "  squad_logits_chunk_70.pt: 3.68 MB\n",
      "  squad_logits_chunk_71.pt: 3.68 MB\n",
      "  squad_logits_chunk_72.pt: 3.68 MB\n",
      "  squad_logits_chunk_73.pt: 3.68 MB\n",
      "  squad_logits_chunk_74.pt: 3.68 MB\n",
      "  squad_logits_chunk_75.pt: 3.68 MB\n",
      "  squad_logits_chunk_76.pt: 3.68 MB\n",
      "  squad_logits_chunk_77.pt: 3.68 MB\n",
      "  squad_logits_chunk_78.pt: 3.68 MB\n",
      "  squad_logits_chunk_79.pt: 3.68 MB\n",
      "  squad_logits_chunk_8.pt: 3.68 MB\n",
      "  squad_logits_chunk_9.pt: 3.68 MB\n",
      "  squad_teacher_answers.json: 2.68 MB\n",
      "  sst2_logits.pt: 0.00 MB\n",
      "  sst2_logits_chunk_0.pt: 3.69 MB\n",
      "  sst2_logits_chunk_1.pt: 3.69 MB\n",
      "  sst2_logits_chunk_10.pt: 3.69 MB\n",
      "  sst2_logits_chunk_11.pt: 3.69 MB\n",
      "  sst2_logits_chunk_12.pt: 3.69 MB\n",
      "  sst2_logits_chunk_13.pt: 3.69 MB\n",
      "  sst2_logits_chunk_14.pt: 3.69 MB\n",
      "  sst2_logits_chunk_15.pt: 3.69 MB\n",
      "  sst2_logits_chunk_16.pt: 3.69 MB\n",
      "  sst2_logits_chunk_17.pt: 3.69 MB\n",
      "  sst2_logits_chunk_18.pt: 3.69 MB\n",
      "  sst2_logits_chunk_19.pt: 3.69 MB\n",
      "  sst2_logits_chunk_2.pt: 3.69 MB\n",
      "  sst2_logits_chunk_20.pt: 3.69 MB\n",
      "  sst2_logits_chunk_21.pt: 3.69 MB\n",
      "  sst2_logits_chunk_22.pt: 3.69 MB\n",
      "  sst2_logits_chunk_23.pt: 3.69 MB\n",
      "  sst2_logits_chunk_24.pt: 3.69 MB\n",
      "  sst2_logits_chunk_25.pt: 3.69 MB\n",
      "  sst2_logits_chunk_26.pt: 3.69 MB\n",
      "  sst2_logits_chunk_27.pt: 3.69 MB\n",
      "  sst2_logits_chunk_28.pt: 3.69 MB\n",
      "  sst2_logits_chunk_29.pt: 3.69 MB\n",
      "  sst2_logits_chunk_3.pt: 3.69 MB\n",
      "  sst2_logits_chunk_30.pt: 3.69 MB\n",
      "  sst2_logits_chunk_31.pt: 3.69 MB\n",
      "  sst2_logits_chunk_32.pt: 3.69 MB\n",
      "  sst2_logits_chunk_33.pt: 3.69 MB\n",
      "  sst2_logits_chunk_34.pt: 3.69 MB\n",
      "  sst2_logits_chunk_35.pt: 3.69 MB\n",
      "  sst2_logits_chunk_36.pt: 3.69 MB\n",
      "  sst2_logits_chunk_37.pt: 3.69 MB\n",
      "  sst2_logits_chunk_38.pt: 3.69 MB\n",
      "  sst2_logits_chunk_39.pt: 3.69 MB\n",
      "  sst2_logits_chunk_4.pt: 3.69 MB\n",
      "  sst2_logits_chunk_5.pt: 3.69 MB\n",
      "  sst2_logits_chunk_6.pt: 3.69 MB\n",
      "  sst2_logits_chunk_7.pt: 3.69 MB\n",
      "  sst2_logits_chunk_8.pt: 3.69 MB\n",
      "  sst2_logits_chunk_9.pt: 3.69 MB\n"
     ]
    }
   ],
   "source": [
    "# Verify all caches\n",
    "print(\"Verifying cached files...\")\n",
    "print()\n",
    "\n",
    "cache_files = list(CACHE_DIR.iterdir())\n",
    "total_size = sum(f.stat().st_size for f in cache_files if f.is_file())\n",
    "\n",
    "print(f\"Cache directory: {CACHE_DIR}\")\n",
    "print(f\"Total size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "print()\n",
    "\n",
    "for f in sorted(cache_files):\n",
    "    if f.is_file():\n",
    "        size_mb = f.stat().st_size / 1024 / 1024\n",
    "        print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bee5aedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEACHER OUTPUT CACHING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Teacher Model: Qwen/Qwen2.5-3B-Instruct\n",
      "Mode: FAST\n",
      "\n",
      "Cached Outputs:\n",
      "  KD1 (Logit-based):\n",
      "    - sst2_logits.pt\n",
      "    - squad_logits.pt\n",
      "\n",
      "  KD2 (Sequence-level):\n",
      "    - squad_teacher_answers.json\n",
      "\n",
      "  KD3 (Feature-based):\n",
      "    - hidden_states_*.pt (chunked)\n",
      "\n",
      "Next Steps:\n",
      "  1. Run 05_train_baseline_and_kd1.ipynb for baseline and logit KD\n",
      "  2. Run 06_train_kd2_and_kd3.ipynb for sequence and feature KD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"TEACHER OUTPUT CACHING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Teacher Model: {teacher_name if 'teacher_name' in dir() else 'N/A (used cache)'}\n",
    "Mode: {'FAST' if config.fast_mode else 'FULL'}\n",
    "\n",
    "Cached Outputs:\n",
    "  KD1 (Logit-based):\n",
    "    - sst2_logits.pt\n",
    "    - squad_logits.pt\n",
    "  \n",
    "  KD2 (Sequence-level):\n",
    "    - squad_teacher_answers.json\n",
    "  \n",
    "  KD3 (Feature-based):\n",
    "    - hidden_states_*.pt (chunked)\n",
    "\n",
    "Next Steps:\n",
    "  1. Run 05_train_baseline_and_kd1.ipynb for baseline and logit KD\n",
    "  2. Run 06_train_kd2_and_kd3.ipynb for sequence and feature KD\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
