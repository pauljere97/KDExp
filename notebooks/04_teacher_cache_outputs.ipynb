{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84fb4c57",
   "metadata": {},
   "source": [
    "# 04 - Teacher Output Caching\n",
    "\n",
    "**Thesis Section Reference:** Chapter 3.8 - Knowledge Distillation Methods\n",
    "\n",
    "This notebook caches teacher model outputs for knowledge distillation:\n",
    "1. **KD1 (Logit-based):** Cache soft logits for both SST-2 and SQuAD\n",
    "2. **KD2 (Sequence-level):** Generate and cache teacher answers for SQuAD\n",
    "3. **KD3 (Feature-based):** Cache hidden states (memory-safe chunked saving)\n",
    "\n",
    "## Memory Management Notes\n",
    "- Uses fp32 for MPS stability\n",
    "- Gradient checkpointing enabled\n",
    "- Periodic cache clearing\n",
    "- Automatic fallback to smaller teacher if OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7610fab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: FAST\n",
      "Device: mps\n",
      "Seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Standard setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "ROOT_DIR = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(ROOT_DIR / \"src\"))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(ROOT_DIR / \".env\")\n",
    "\n",
    "from config import load_config\n",
    "from utils_seed import set_seed\n",
    "\n",
    "config = load_config(str(ROOT_DIR / \"configs\" / \"experiment.yaml\"))\n",
    "config.ensure_dirs()\n",
    "\n",
    "SEED = config.get_seeds()[0]\n",
    "set_seed(SEED)\n",
    "\n",
    "# Device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Mode: {'FAST' if config.fast_mode else 'FULL'}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa6b8458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /Users/pjere/Workshop/thesis-exp/results/processed_data\n",
      "Cache directory: /Users/pjere/Workshop/thesis-exp/results/teacher_cache\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "DATA_DIR = ROOT_DIR / \"results\" / \"processed_data\"\n",
    "CACHE_DIR = ROOT_DIR / \"results\" / \"teacher_cache\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Cache directory: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "849db3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache status:\n",
      "  sst2_logits: ✗ needs generation\n",
      "  squad_logits: ✗ needs generation\n",
      "  squad_answers: ✗ needs generation\n",
      "  hidden_states: ✗ needs generation\n"
     ]
    }
   ],
   "source": [
    "# Check what already exists\n",
    "cache_status = {\n",
    "    \"sst2_logits\": (CACHE_DIR / \"sst2_logits.pt\").exists(),\n",
    "    \"squad_logits\": (CACHE_DIR / \"squad_logits.pt\").exists(),\n",
    "    \"squad_answers\": (CACHE_DIR / \"squad_teacher_answers.json\").exists(),\n",
    "    \"hidden_states\": any(CACHE_DIR.glob(\"hidden_states_*.pt\"))\n",
    "}\n",
    "\n",
    "print(\"Cache status:\")\n",
    "for name, exists in cache_status.items():\n",
    "    status = \"✓ exists\" if exists else \"✗ needs generation\"\n",
    "    print(f\"  {name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83ab14e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load: meta-llama/Llama-3.2-8B-Instruct\n",
      "Failed to load meta-llama/Llama-3.2-8B-Instruct: meta-llama/Llama-3.2-8B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n",
      "Attempting to load: Qwen/Qwen2.5-3B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df3f7404d224637b162b7849597dc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully loaded: Qwen/Qwen2.5-3B-Instruct\n",
      "\n",
      "Teacher model parameters: 3.09B\n"
     ]
    }
   ],
   "source": [
    "# Load teacher model with fallback\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_teacher_model(config, device):\n",
    "    \"\"\"Load teacher with automatic fallback to smaller model.\"\"\"\n",
    "    # Try primary teacher first\n",
    "    primary = os.getenv(\"TEACHER_PRIMARY\", config.teacher.primary)\n",
    "    fallback = os.getenv(\"TEACHER_FALLBACK\", config.teacher.local_fallback)\n",
    "    \n",
    "    for model_name in [primary, fallback]:\n",
    "        try:\n",
    "            print(f\"Attempting to load: {model_name}\")\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "            )\n",
    "            \n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                dtype=torch.float32,  # fp32 for MPS stability\n",
    "                cache_dir=str(ROOT_DIR / \"hf_cache\"),\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Enable gradient checkpointing for memory\n",
    "            if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "                model.gradient_checkpointing_enable()\n",
    "            \n",
    "            print(f\"✓ Successfully loaded: {model_name}\")\n",
    "            return model, tokenizer, model_name\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {model_name}: {e}\")\n",
    "            if device.type == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "            gc.collect()\n",
    "            continue\n",
    "    \n",
    "    raise RuntimeError(\"Failed to load any teacher model\")\n",
    "\n",
    "# Only load if we need to generate something\n",
    "needs_teacher = not all(cache_status.values())\n",
    "\n",
    "if needs_teacher:\n",
    "    teacher_model, teacher_tokenizer, teacher_name = load_teacher_model(config, DEVICE)\n",
    "    print(f\"\\nTeacher model parameters: {sum(p.numel() for p in teacher_model.parameters()) / 1e9:.2f}B\")\n",
    "else:\n",
    "    print(\"All caches exist, skipping teacher loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45146afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed datasets...\n",
      "  SST-2 train: 2000 examples\n",
      "  SQuAD train: 2000 examples\n"
     ]
    }
   ],
   "source": [
    "# Load processed datasets\n",
    "from datasets import load_from_disk\n",
    "\n",
    "if needs_teacher:\n",
    "    print(\"Loading processed datasets...\")\n",
    "    \n",
    "    sst2_train = load_from_disk(str(DATA_DIR / \"sst2_train\"))\n",
    "    squad_train = load_from_disk(str(DATA_DIR / \"squad_train\"))\n",
    "    \n",
    "    print(f\"  SST-2 train: {len(sst2_train)} examples\")\n",
    "    print(f\"  SQuAD train: {len(squad_train)} examples\")\n",
    "    \n",
    "    # Load prompts for KD2\n",
    "    with open(DATA_DIR / \"squad_train_prompts.json\", \"r\") as f:\n",
    "        squad_prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf280a",
   "metadata": {},
   "source": [
    "## KD1: Cache Teacher Logits\n",
    "\n",
    "For logit-based KD, we need the teacher's output logits for each training example.\n",
    "We'll store the top-k logits to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e62375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching SST-2 teacher logits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching logits:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     21\u001b[39m sst2_dataloader = DataLoader(\n\u001b[32m     22\u001b[39m     sst2_train,\n\u001b[32m     23\u001b[39m     batch_size=batch_size,\n\u001b[32m     24\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Cache logits using the correct API\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mcache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mteacher_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43msst2_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msst2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mteacher_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Saved SST-2 logits to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCACHE_DIR\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Clean up\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workshop/thesis-exp/src/teacher_cache.py:177\u001b[39m, in \u001b[36mTeacherCache.cache_logits\u001b[39m\u001b[34m(self, model, tokenizer, dataloader, task, split, model_name, device, dtype)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader, desc=\u001b[33m\"\u001b[39m\u001b[33mCaching logits\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m         input_ids = \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(device)\n\u001b[32m    178\u001b[39m         attention_mask = batch[\u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m].to(device)\n\u001b[32m    180\u001b[39m         outputs = model(\n\u001b[32m    181\u001b[39m             input_ids=input_ids,\n\u001b[32m    182\u001b[39m             attention_mask=attention_mask,\n\u001b[32m    183\u001b[39m             output_hidden_states=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    184\u001b[39m         )\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# Cache SST-2 teacher logits\n",
    "from teacher_cache import TeacherCache, TeacherCacheConfig\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "if not cache_status[\"sst2_logits\"]:\n",
    "    print(\"Caching SST-2 teacher logits...\")\n",
    "    \n",
    "    # Create cache config\n",
    "    cache_config = TeacherCacheConfig(\n",
    "        cache_dir=str(CACHE_DIR),\n",
    "        cache_logits=True,\n",
    "        cache_answers=True,\n",
    "        cache_hiddens=True\n",
    "    )\n",
    "    cache = TeacherCache(cache_config)\n",
    "    \n",
    "    # Use smaller batch size for MPS\n",
    "    batch_size = 2 if DEVICE.type == \"mps\" else 4\n",
    "    \n",
    "    # Set dataset format to PyTorch tensors\n",
    "    sst2_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    \n",
    "    # Create dataloader\n",
    "    sst2_dataloader = DataLoader(\n",
    "        sst2_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Cache logits using the correct API\n",
    "    cache.cache_logits(\n",
    "        model=teacher_model,\n",
    "        tokenizer=teacher_tokenizer,\n",
    "        dataloader=sst2_dataloader,\n",
    "        task=\"sst2\",\n",
    "        split=\"train\",\n",
    "        model_name=teacher_name,\n",
    "        device=str(DEVICE),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Saved SST-2 logits to: {CACHE_DIR / 'logits'}\")\n",
    "    \n",
    "    # Clean up\n",
    "    if DEVICE.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"✓ SST-2 logits already cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache SQuAD teacher logits\n",
    "if not cache_status[\"squad_logits\"]:\n",
    "    print(\"Caching SQuAD teacher logits...\")\n",
    "    \n",
    "    if 'cache' not in dir():\n",
    "        cache_config = TeacherCacheConfig(\n",
    "            cache_dir=str(CACHE_DIR),\n",
    "            cache_logits=True,\n",
    "            cache_answers=True,\n",
    "            cache_hiddens=True\n",
    "        )\n",
    "        cache = TeacherCache(cache_config)\n",
    "    \n",
    "    batch_size = 1 if DEVICE.type == \"mps\" else 2  # SQuAD has longer sequences\n",
    "    \n",
    "    # Set dataset format to PyTorch tensors\n",
    "    squad_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    \n",
    "    # Create dataloader\n",
    "    squad_dataloader = DataLoader(\n",
    "        squad_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    cache.cache_logits(\n",
    "        model=teacher_model,\n",
    "        tokenizer=teacher_tokenizer,\n",
    "        dataloader=squad_dataloader,\n",
    "        task=\"squad\",\n",
    "        split=\"train\",\n",
    "        model_name=teacher_name,\n",
    "        device=str(DEVICE),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Saved SQuAD logits to: {CACHE_DIR / 'logits'}\")\n",
    "    \n",
    "    if DEVICE.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"✓ SQuAD logits already cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab3b0c",
   "metadata": {},
   "source": [
    "## KD2: Generate Teacher Answers (Sequence-level KD)\n",
    "\n",
    "For sequence-level KD on SQuAD, we generate teacher's predicted answers.\n",
    "The student learns to mimic the teacher's generated sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate teacher answers for SQuAD\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if not cache_status[\"squad_answers\"]:\n",
    "    print(\"Generating teacher answers for SQuAD...\")\n",
    "    \n",
    "    teacher_answers = []\n",
    "    \n",
    "    # Generation config\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"do_sample\": False,  # Greedy for reproducibility\n",
    "        \"pad_token_id\": teacher_tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": teacher_tokenizer.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    batch_size = 1  # One at a time for MPS stability\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(squad_prompts), batch_size), desc=\"Generating\"):\n",
    "            batch = squad_prompts[i:i+batch_size]\n",
    "            \n",
    "            for item in batch:\n",
    "                inputs = teacher_tokenizer(\n",
    "                    item[\"prompt\"],\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=config.get_max_length(\"squad\") - 64,\n",
    "                    truncation=True\n",
    "                ).to(DEVICE)\n",
    "                \n",
    "                outputs = teacher_model.generate(\n",
    "                    **inputs,\n",
    "                    **gen_config\n",
    "                )\n",
    "                \n",
    "                # Decode only new tokens\n",
    "                generated = teacher_tokenizer.decode(\n",
    "                    outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "                    skip_special_tokens=True\n",
    "                ).strip()\n",
    "                \n",
    "                teacher_answers.append({\n",
    "                    \"id\": item[\"id\"],\n",
    "                    \"prompt\": item[\"prompt\"],\n",
    "                    \"teacher_answer\": generated,\n",
    "                    \"gold_answers\": item[\"gold_answers\"]\n",
    "                })\n",
    "            \n",
    "            # Periodic cleanup\n",
    "            if i % 50 == 0 and DEVICE.type == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "    \n",
    "    # Save\n",
    "    with open(CACHE_DIR / \"squad_teacher_answers.json\", \"w\") as f:\n",
    "        json.dump(teacher_answers, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Generated {len(teacher_answers)} teacher answers\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\nSample teacher answers:\")\n",
    "    for sample in teacher_answers[:3]:\n",
    "        print(f\"  Q: {sample['prompt'].split('Question:')[1].split('Context:')[0].strip()[:50]}...\")\n",
    "        print(f\"  Teacher: {sample['teacher_answer'][:50]}...\")\n",
    "        print(f\"  Gold: {sample['gold_answers'][0]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"✓ SQuAD teacher answers already cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447049c0",
   "metadata": {},
   "source": [
    "## KD3: Cache Hidden States (Feature-based KD)\n",
    "\n",
    "For feature-based KD, we cache teacher's hidden states from selected layers.\n",
    "This is memory-intensive, so we:\n",
    "1. Store only selected layers (e.g., every 4th layer)\n",
    "2. Use float16 for storage\n",
    "3. Save in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache hidden states (memory-safe)\n",
    "if not cache_status[\"hidden_states\"]:\n",
    "    print(\"Caching hidden states for KD3...\")\n",
    "    print(\"Note: This is memory-intensive. Using chunked saving.\")\n",
    "    \n",
    "    # Configuration\n",
    "    LAYER_STRIDE = 4  # Cache every 4th layer\n",
    "    CHUNK_SIZE = 100  # Save every 100 examples\n",
    "    USE_SST2_ONLY = config.fast_mode  # In fast mode, only cache SST-2\n",
    "    \n",
    "    dataset = sst2_train\n",
    "    task_name = \"sst2\"\n",
    "    \n",
    "    num_layers = teacher_model.config.num_hidden_layers\n",
    "    selected_layers = list(range(0, num_layers, LAYER_STRIDE))\n",
    "    print(f\"  Caching layers: {selected_layers} (of {num_layers} total)\")\n",
    "    \n",
    "    all_hidden_states = []\n",
    "    chunk_idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataset)), desc=\"Extracting hidden states\"):\n",
    "            example = dataset[i]\n",
    "            \n",
    "            # Prepare input\n",
    "            input_ids = torch.tensor([example[\"input_ids\"]], device=DEVICE)\n",
    "            attention_mask = torch.tensor([example[\"attention_mask\"]], device=DEVICE)\n",
    "            \n",
    "            # Get hidden states\n",
    "            outputs = teacher_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            \n",
    "            # Extract selected layers and convert to float16\n",
    "            layer_states = []\n",
    "            for layer_idx in selected_layers:\n",
    "                # Mean pool over sequence length to reduce memory\n",
    "                layer_output = outputs.hidden_states[layer_idx + 1]  # +1 for embeddings\n",
    "                pooled = layer_output.mean(dim=1)  # [1, hidden_size]\n",
    "                layer_states.append(pooled.cpu().half())\n",
    "            \n",
    "            # Stack layers: [num_selected_layers, hidden_size]\n",
    "            stacked = torch.cat(layer_states, dim=0)\n",
    "            all_hidden_states.append(stacked)\n",
    "            \n",
    "            # Save chunk\n",
    "            if len(all_hidden_states) >= CHUNK_SIZE:\n",
    "                chunk_tensor = torch.stack(all_hidden_states)\n",
    "                torch.save(chunk_tensor, CACHE_DIR / f\"hidden_states_{task_name}_{chunk_idx}.pt\")\n",
    "                print(f\"  Saved chunk {chunk_idx} ({len(all_hidden_states)} examples)\")\n",
    "                all_hidden_states = []\n",
    "                chunk_idx += 1\n",
    "                \n",
    "                if DEVICE.type == \"mps\":\n",
    "                    torch.mps.empty_cache()\n",
    "    \n",
    "    # Save final chunk\n",
    "    if all_hidden_states:\n",
    "        chunk_tensor = torch.stack(all_hidden_states)\n",
    "        torch.save(chunk_tensor, CACHE_DIR / f\"hidden_states_{task_name}_{chunk_idx}.pt\")\n",
    "        print(f\"  Saved chunk {chunk_idx} ({len(all_hidden_states)} examples)\")\n",
    "    \n",
    "    # Save metadata\n",
    "    hidden_state_meta = {\n",
    "        \"task\": task_name,\n",
    "        \"num_chunks\": chunk_idx + 1,\n",
    "        \"selected_layers\": selected_layers,\n",
    "        \"total_layers\": num_layers,\n",
    "        \"hidden_size\": teacher_model.config.hidden_size,\n",
    "        \"pooling\": \"mean\",\n",
    "        \"dtype\": \"float16\"\n",
    "    }\n",
    "    \n",
    "    with open(CACHE_DIR / f\"hidden_states_{task_name}_meta.json\", \"w\") as f:\n",
    "        json.dump(hidden_state_meta, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Saved hidden states in {chunk_idx + 1} chunks\")\n",
    "else:\n",
    "    print(\"✓ Hidden states already cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749b8ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "if needs_teacher:\n",
    "    print(\"Cleaning up...\")\n",
    "    del teacher_model\n",
    "    if 'cache' in dir():\n",
    "        del cache\n",
    "    if DEVICE.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"✓ Memory freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all caches\n",
    "print(\"Verifying cached files...\")\n",
    "print()\n",
    "\n",
    "cache_files = list(CACHE_DIR.iterdir())\n",
    "total_size = sum(f.stat().st_size for f in cache_files if f.is_file())\n",
    "\n",
    "print(f\"Cache directory: {CACHE_DIR}\")\n",
    "print(f\"Total size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "print()\n",
    "\n",
    "for f in sorted(cache_files):\n",
    "    if f.is_file():\n",
    "        size_mb = f.stat().st_size / 1024 / 1024\n",
    "        print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"TEACHER OUTPUT CACHING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Teacher Model: {teacher_name if 'teacher_name' in dir() else 'N/A (used cache)'}\n",
    "Mode: {'FAST' if config.fast_mode else 'FULL'}\n",
    "\n",
    "Cached Outputs:\n",
    "  KD1 (Logit-based):\n",
    "    - sst2_logits.pt\n",
    "    - squad_logits.pt\n",
    "  \n",
    "  KD2 (Sequence-level):\n",
    "    - squad_teacher_answers.json\n",
    "  \n",
    "  KD3 (Feature-based):\n",
    "    - hidden_states_*.pt (chunked)\n",
    "\n",
    "Next Steps:\n",
    "  1. Run 05_train_baseline_and_kd1.ipynb for baseline and logit KD\n",
    "  2. Run 06_train_kd2_and_kd3.ipynb for sequence and feature KD\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
