{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84fb4c57",
   "metadata": {},
   "source": [
    "# 04 - Teacher Output Caching\n",
    "\n",
    "**Thesis Section Reference:** Chapter 3.8 - Knowledge Distillation Methods\n",
    "\n",
    "This notebook caches teacher model outputs for knowledge distillation:\n",
    "1. **KD1 (Logit-based):** Cache soft logits for both SST-2 and SQuAD\n",
    "2. **KD2 (Sequence-level):** Generate and cache teacher answers for SQuAD\n",
    "3. **KD3 (Feature-based):** Cache hidden states (memory-safe chunked saving)\n",
    "\n",
    "## Memory Management Notes\n",
    "- Uses fp32 for MPS stability\n",
    "- Gradient checkpointing enabled\n",
    "- Periodic cache clearing\n",
    "- Automatic fallback to smaller teacher if OOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "ROOT_DIR = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(ROOT_DIR / \"src\"))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(ROOT_DIR / \".env\")\n",
    "\n",
    "from config import load_config\n",
    "from utils_seed import set_seed\n",
    "\n",
    "config = load_config(str(ROOT_DIR / \"configs\" / \"experiment.yaml\"))\n",
    "config.ensure_dirs()\n",
    "\n",
    "SEED = config.get_seeds()[0]\n",
    "set_seed(SEED)\n",
    "\n",
    "# Device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Mode: {'FAST' if config.fast_mode else 'FULL'}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b8458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "DATA_DIR = ROOT_DIR / \"results\" / \"processed_data\"\n",
    "CACHE_DIR = ROOT_DIR / \"results\" / \"teacher_cache\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Cache directory: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849db3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what already exists\n",
    "cache_status = {\n",
    "    \"sst2_logits\": (CACHE_DIR / \"sst2_logits.pt\").exists(),\n",
    "    \"squad_logits\": (CACHE_DIR / \"squad_logits.pt\").exists(),\n",
    "    \"squad_answers\": (CACHE_DIR / \"squad_teacher_answers.json\").exists(),\n",
    "    \"hidden_states\": any(CACHE_DIR.glob(\"hidden_states_*.pt\"))\n",
    "}\n",
    "\n",
    "print(\"Cache status:\")\n",
    "for name, exists in cache_status.items():\n",
    "    status = \"✓ exists\" if exists else \"✗ needs generation\"\n",
    "    print(f\"  {name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load teacher model with fallback\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_teacher_model(config, device):\n",
    "    \"\"\"Load teacher with automatic fallback to smaller model.\"\"\"\n",
    "    # Try primary teacher first\n",
    "    primary = os.getenv(\"TEACHER_PRIMARY\", config.teacher.primary)\n",
    "    fallback = os.getenv(\"TEACHER_FALLBACK\", config.teacher.fallback)\n",
    "    \n",
    "    for model_name in [primary, fallback]:\n",
    "        try:\n",
    "            print(f\"Attempting to load: {model_name}\")\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "            )\n",
    "            \n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float32,  # fp32 for MPS stability\n",
    "                cache_dir=str(ROOT_DIR / \"hf_cache\"),\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            model = model.to(device)\n",
    "            model.eval()\n",
    "            \n",
    "            # Enable gradient checkpointing for memory\n",
    "            if hasattr(model, 'gradient_checkpointing_enable'):\n",
    "                model.gradient_checkpointing_enable()\n",
    "            \n",
    "            print(f\"✓ Successfully loaded: {model_name}\")\n",
    "            return model, tokenizer, model_name\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {model_name}: {e}\")\n",
    "            if device.type == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "            gc.collect()\n",
    "            continue\n",
    "    \n",
    "    raise RuntimeError(\"Failed to load any teacher model\")\n",
    "\n",
    "# Only load if we need to generate something\n",
    "needs_teacher = not all(cache_status.values())\n",
    "\n",
    "if needs_teacher:\n",
    "    teacher_model, teacher_tokenizer, teacher_name = load_teacher_model(config, DEVICE)\n",
    "    print(f\"\\nTeacher model parameters: {sum(p.numel() for p in teacher_model.parameters()) / 1e9:.2f}B\")\n",
    "else:\n",
    "    print(\"All caches exist, skipping teacher loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45146afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed datasets\n",
    "from datasets import load_from_disk\n",
    "\n",
    "if needs_teacher:\n",
    "    print(\"Loading processed datasets...\")\n",
    "    \n",
    "    sst2_train = load_from_disk(str(DATA_DIR / \"sst2_train\"))\n",
    "    squad_train = load_from_disk(str(DATA_DIR / \"squad_train\"))\n",
    "    \n",
    "    print(f\"  SST-2 train: {len(sst2_train)} examples\")\n",
    "    print(f\"  SQuAD train: {len(squad_train)} examples\")\n",
    "    \n",
    "    # Load prompts for KD2\n",
    "    with open(DATA_DIR / \"squad_train_prompts.json\", \"r\") as f:\n",
    "        squad_prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf280a",
   "metadata": {},
   "source": [
    "## KD1: Cache Teacher Logits\n",
    "\n",
    "For logit-based KD, we need the teacher's output logits for each training example.\n",
    "We'll store the top-k logits to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e62375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache SST-2 teacher logits\n",
    "from teacher_cache import TeacherCache\n",
    "\n",
    "if not cache_status[\"sst2_logits\"]:\n",
    "    print(\"Caching SST-2 teacher logits...\")\n",
    "    \n",
    "    cache = TeacherCache(\n",
    "        model=teacher_model,\n",
    "        tokenizer=teacher_tokenizer,\n",
    "        device=DEVICE,\n",
    "        cache_dir=CACHE_DIR\n",
    "    )\n",
    "    \n",
    "    # Use smaller batch size for MPS\n",
    "    batch_size = 2 if DEVICE.type == \"mps\" else 4\n",
    "    \n",
    "    sst2_logits = cache.cache_logits(\n",
    "        dataset=sst2_train,\n",
    "        batch_size=batch_size,\n",
    "        top_k=100,  # Store only top-100 logits per position\n",
    "        task_name=\"sst2\"\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    torch.save(sst2_logits, CACHE_DIR / \"sst2_logits.pt\")\n",
    "    print(f\"✓ Saved SST-2 logits: {CACHE_DIR / 'sst2_logits.pt'}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del sst2_logits\n",
    "    if DEVICE.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"✓ SST-2 logits already cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf7a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache SQuAD teacher logits\n",
    "if not cache_status[\"squad_logits\"]:\n",
    "    print(\"Caching SQuAD teacher logits...\")\n",
    "    \n",
    "    if 'cache' not in dir():\n",
    "        cache = TeacherCache(\n",
    "            model=teacher_model,\n",
    "            tokenizer=teacher_tokenizer,\n",
    "            device=DEVICE,\n",
    "            cache_dir=CACHE_DIR\n",
    "        )\n",
    "    \n",
    "    batch_size = 1 if DEVICE.type == \"mps\" else 2  # SQuAD has longer sequences\n",
    "    \n",
    "    squad_logits = cache.cache_logits(\n",
    "        dataset=squad_train,\n",
    "        batch_size=batch_size,\n",
    "        top_k=100,\n",
    "        task_name=\"squad\"\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    torch.save(squad_logits, CACHE_DIR / \"squad_logits.pt\")\n",
    "    print(f\"✓ Saved SQuAD logits: {CACHE_DIR / 'squad_logits.pt'}\")\n",
    "    \n",
    "    del squad_logits\n",
    "    if DEVICE.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"✓ SQuAD logits already cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab3b0c",
   "metadata": {},
   "source": [
    "## KD2: Generate Teacher Answers (Sequence-level KD)\n",
    "\n",
    "For sequence-level KD on SQuAD, we generate teacher's predicted answers.\n",
    "The student learns to mimic the teacher's generated sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate teacher answers for SQuAD\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if not cache_status[\"squad_answers\"]:\n",
    "    print(\"Generating teacher answers for SQuAD...\")\n",
    "    \n",
    "    teacher_answers = []\n",
    "    \n",
    "    # Generation config\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 64,\n",
    "        \"do_sample\": False,  # Greedy for reproducibility\n",
    "        \"pad_token_id\": teacher_tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": teacher_tokenizer.eos_token_id,\n",
    "    }\n",
    "    \n",
    "    batch_size = 1  # One at a time for MPS stability\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(squad_prompts), batch_size), desc=\"Generating\"):\n",
    "            batch = squad_prompts[i:i+batch_size]\n",
    "            \n",
    "            for item in batch:\n",
    "                inputs = teacher_tokenizer(\n",
    "                    item[\"prompt\"],\n",
    "                    return_tensors=\"pt\",\n",
    "                    max_length=config.get_max_length(\"squad\") - 64,\n",
    "                    truncation=True\n",
    "                ).to(DEVICE)\n",
    "                \n",
    "                outputs = teacher_model.generate(\n",
    "                    **inputs,\n",
    "                    **gen_config\n",
    "                )\n",
    "                \n",
    "                # Decode only new tokens\n",
    "                generated = teacher_tokenizer.decode(\n",
    "                    outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "                    skip_special_tokens=True\n",
    "                ).strip()\n",
    "                \n",
    "                teacher_answers.append({\n",
    "                    \"id\": item[\"id\"],\n",
    "                    \"prompt\": item[\"prompt\"],\n",
    "                    \"teacher_answer\": generated,\n",
    "                    \"gold_answers\": item[\"gold_answers\"]\n",
    "                })\n",
    "            \n",
    "            # Periodic cleanup\n",
    "            if i % 50 == 0 and DEVICE.type == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "    \n",
    "    # Save\n",
    "    with open(CACHE_DIR / \"squad_teacher_answers.json\", \"w\") as f:\n",
    "        json.dump(teacher_answers, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Generated {len(teacher_answers)} teacher answers\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"\\nSample teacher answers:\")\n",
    "    for sample in teacher_answers[:3]:\n",
    "        print(f\"  Q: {sample['prompt'].split('Question:')[1].split('Context:')[0].strip()[:50]}...\")\n",
    "        print(f\"  Teacher: {sample['teacher_answer'][:50]}...\")\n",
    "        print(f\"  Gold: {sample['gold_answers'][0]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"✓ SQuAD teacher answers already cached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447049c0",
   "metadata": {},
   "source": [
    "## KD3: Cache Hidden States (Feature-based KD)\n",
    "\n",
    "For feature-based KD, we cache teacher's hidden states from selected layers.\n",
    "This is memory-intensive, so we:\n",
    "1. Store only selected layers (e.g., every 4th layer)\n",
    "2. Use float16 for storage\n",
    "3. Save in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache hidden states (memory-safe)\n",
    "if not cache_status[\"hidden_states\"]:\n",
    "    print(\"Caching hidden states for KD3...\")\n",
    "    print(\"Note: This is memory-intensive. Using chunked saving.\")\n",
    "    \n",
    "    # Configuration\n",
    "    LAYER_STRIDE = 4  # Cache every 4th layer\n",
    "    CHUNK_SIZE = 100  # Save every 100 examples\n",
    "    USE_SST2_ONLY = config.fast_mode  # In fast mode, only cache SST-2\n",
    "    \n",
    "    dataset = sst2_train\n",
    "    task_name = \"sst2\"\n",
    "    \n",
    "    num_layers = teacher_model.config.num_hidden_layers\n",
    "    selected_layers = list(range(0, num_layers, LAYER_STRIDE))\n",
    "    print(f\"  Caching layers: {selected_layers} (of {num_layers} total)\")\n",
    "    \n",
    "    all_hidden_states = []\n",
    "    chunk_idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataset)), desc=\"Extracting hidden states\"):\n",
    "            example = dataset[i]\n",
    "            \n",
    "            # Prepare input\n",
    "            input_ids = torch.tensor([example[\"input_ids\"]], device=DEVICE)\n",
    "            attention_mask = torch.tensor([example[\"attention_mask\"]], device=DEVICE)\n",
    "            \n",
    "            # Get hidden states\n",
    "            outputs = teacher_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            \n",
    "            # Extract selected layers and convert to float16\n",
    "            layer_states = []\n",
    "            for layer_idx in selected_layers:\n",
    "                # Mean pool over sequence length to reduce memory\n",
    "                layer_output = outputs.hidden_states[layer_idx + 1]  # +1 for embeddings\n",
    "                pooled = layer_output.mean(dim=1)  # [1, hidden_size]\n",
    "                layer_states.append(pooled.cpu().half())\n",
    "            \n",
    "            # Stack layers: [num_selected_layers, hidden_size]\n",
    "            stacked = torch.cat(layer_states, dim=0)\n",
    "            all_hidden_states.append(stacked)\n",
    "            \n",
    "            # Save chunk\n",
    "            if len(all_hidden_states) >= CHUNK_SIZE:\n",
    "                chunk_tensor = torch.stack(all_hidden_states)\n",
    "                torch.save(chunk_tensor, CACHE_DIR / f\"hidden_states_{task_name}_{chunk_idx}.pt\")\n",
    "                print(f\"  Saved chunk {chunk_idx} ({len(all_hidden_states)} examples)\")\n",
    "                all_hidden_states = []\n",
    "                chunk_idx += 1\n",
    "                \n",
    "                if DEVICE.type == \"mps\":\n",
    "                    torch.mps.empty_cache()\n",
    "    \n",
    "    # Save final chunk\n",
    "    if all_hidden_states:\n",
    "        chunk_tensor = torch.stack(all_hidden_states)\n",
    "        torch.save(chunk_tensor, CACHE_DIR / f\"hidden_states_{task_name}_{chunk_idx}.pt\")\n",
    "        print(f\"  Saved chunk {chunk_idx} ({len(all_hidden_states)} examples)\")\n",
    "    \n",
    "    # Save metadata\n",
    "    hidden_state_meta = {\n",
    "        \"task\": task_name,\n",
    "        \"num_chunks\": chunk_idx + 1,\n",
    "        \"selected_layers\": selected_layers,\n",
    "        \"total_layers\": num_layers,\n",
    "        \"hidden_size\": teacher_model.config.hidden_size,\n",
    "        \"pooling\": \"mean\",\n",
    "        \"dtype\": \"float16\"\n",
    "    }\n",
    "    \n",
    "    with open(CACHE_DIR / f\"hidden_states_{task_name}_meta.json\", \"w\") as f:\n",
    "        json.dump(hidden_state_meta, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Saved hidden states in {chunk_idx + 1} chunks\")\n",
    "else:\n",
    "    print(\"✓ Hidden states already cached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749b8ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "if needs_teacher:\n",
    "    print(\"Cleaning up...\")\n",
    "    del teacher_model\n",
    "    if 'cache' in dir():\n",
    "        del cache\n",
    "    if DEVICE.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"✓ Memory freed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all caches\n",
    "print(\"Verifying cached files...\")\n",
    "print()\n",
    "\n",
    "cache_files = list(CACHE_DIR.iterdir())\n",
    "total_size = sum(f.stat().st_size for f in cache_files if f.is_file())\n",
    "\n",
    "print(f\"Cache directory: {CACHE_DIR}\")\n",
    "print(f\"Total size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "print()\n",
    "\n",
    "for f in sorted(cache_files):\n",
    "    if f.is_file():\n",
    "        size_mb = f.stat().st_size / 1024 / 1024\n",
    "        print(f\"  {f.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"TEACHER OUTPUT CACHING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Teacher Model: {teacher_name if 'teacher_name' in dir() else 'N/A (used cache)'}\n",
    "Mode: {'FAST' if config.fast_mode else 'FULL'}\n",
    "\n",
    "Cached Outputs:\n",
    "  KD1 (Logit-based):\n",
    "    - sst2_logits.pt\n",
    "    - squad_logits.pt\n",
    "  \n",
    "  KD2 (Sequence-level):\n",
    "    - squad_teacher_answers.json\n",
    "  \n",
    "  KD3 (Feature-based):\n",
    "    - hidden_states_*.pt (chunked)\n",
    "\n",
    "Next Steps:\n",
    "  1. Run 05_train_baseline_and_kd1.ipynb for baseline and logit KD\n",
    "  2. Run 06_train_kd2_and_kd3.ipynb for sequence and feature KD\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
