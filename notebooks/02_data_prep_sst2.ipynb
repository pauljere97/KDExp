{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e264daf",
   "metadata": {},
   "source": [
    "# 02 - Data Preparation: SST-2\n",
    "\n",
    "**Thesis Section Reference:** Chapter 3.6 - Tasks and Datasets\n",
    "\n",
    "This notebook prepares the SST-2 sentiment classification dataset:\n",
    "1. Load SST-2 from GLUE benchmark\n",
    "2. Create subsets for FAST MODE\n",
    "3. Tokenize for causal LM training\n",
    "4. Save processed datasets\n",
    "\n",
    "## Task Description\n",
    "- **Dataset:** GLUE SST-2 (Stanford Sentiment Treebank)\n",
    "- **Task:** Binary sentiment classification (positive/negative)\n",
    "- **Metrics:** Accuracy, F1\n",
    "- **Splits:** Train (67,349), Validation (872)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adf6529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard setup - load environment and config\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(ROOT_DIR / \"src\"))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(ROOT_DIR / \".env\")\n",
    "\n",
    "from config import load_config\n",
    "from utils_seed import set_seed\n",
    "\n",
    "config = load_config(str(ROOT_DIR / \"configs\" / \"experiment.yaml\"))\n",
    "config.ensure_dirs()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = config.get_seeds()[0]\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"Mode: {'FAST' if config.fast_mode else 'FULL'}\")\n",
    "print(f\"Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data already exists (idempotent)\n",
    "DATA_DIR = ROOT_DIR / \"results\" / \"processed_data\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sst2_train_path = DATA_DIR / \"sst2_train.arrow\"\n",
    "sst2_val_path = DATA_DIR / \"sst2_validation.arrow\"\n",
    "\n",
    "if sst2_train_path.exists() and sst2_val_path.exists():\n",
    "    print(\"✓ SST-2 data already exists, loading from cache...\")\n",
    "    SKIP_PROCESSING = True\n",
    "else:\n",
    "    print(\"SST-2 data not found, will process...\")\n",
    "    SKIP_PROCESSING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SST-2 dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Loading SST-2 from GLUE...\")\n",
    "    \n",
    "    raw_dataset = load_dataset(\n",
    "        \"glue\", \n",
    "        \"sst2\",\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDataset structure:\")\n",
    "    print(raw_dataset)\n",
    "    \n",
    "    print(f\"\\nSample examples:\")\n",
    "    for i in range(3):\n",
    "        ex = raw_dataset[\"train\"][i]\n",
    "        label = \"positive\" if ex[\"label\"] == 1 else \"negative\"\n",
    "        print(f\"  [{label}] {ex['sentence'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ddc0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subsets based on mode\n",
    "if not SKIP_PROCESSING:\n",
    "    train_size = config.get_subset_size(\"sst2\", \"train\")\n",
    "    val_size = config.get_subset_size(\"sst2\", \"validation\")\n",
    "    \n",
    "    if train_size is not None:\n",
    "        print(f\"FAST MODE: Subsetting to {train_size} train, {val_size} validation examples\")\n",
    "        \n",
    "        train_dataset = raw_dataset[\"train\"].shuffle(seed=SEED).select(range(train_size))\n",
    "        val_dataset = raw_dataset[\"validation\"].shuffle(seed=SEED).select(range(min(val_size, len(raw_dataset[\"validation\"]))))\n",
    "    else:\n",
    "        print(\"FULL MODE: Using complete dataset\")\n",
    "        train_dataset = raw_dataset[\"train\"]\n",
    "        val_dataset = raw_dataset[\"validation\"]\n",
    "    \n",
    "    print(f\"\\nFinal sizes:\")\n",
    "    print(f\"  Train: {len(train_dataset)}\")\n",
    "    print(f\"  Validation: {len(val_dataset)}\")\n",
    "    \n",
    "    # Check label distribution\n",
    "    train_labels = train_dataset[\"label\"]\n",
    "    pos_ratio = sum(train_labels) / len(train_labels)\n",
    "    print(f\"\\nLabel distribution (train):\")\n",
    "    print(f\"  Positive: {pos_ratio:.1%}\")\n",
    "    print(f\"  Negative: {1-pos_ratio:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb56892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    # Use student tokenizer (will be used for all models)\n",
    "    tokenizer_name = os.getenv(\"STUDENT_S1\", config.student_s1.name)\n",
    "    \n",
    "    print(f\"Loading tokenizer: {tokenizer_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "    )\n",
    "    \n",
    "    # Ensure pad token exists\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "    print(f\"  Pad token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")\n",
    "    print(f\"  EOS token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dae1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template and tokenization\n",
    "from data_sst2 import create_sst2_prompt, get_sst2_label_tokens\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    max_length = config.get_max_length(\"sst2\")\n",
    "    print(f\"Max sequence length: {max_length}\")\n",
    "    \n",
    "    # Get label token IDs for classification\n",
    "    label_tokens = get_sst2_label_tokens(tokenizer)\n",
    "    print(f\"\\nLabel tokens:\")\n",
    "    print(f\"  positive: token ID {label_tokens['positive']}\")\n",
    "    print(f\"  negative: token ID {label_tokens['negative']}\")\n",
    "    \n",
    "    # Show example prompt\n",
    "    example_sentence = train_dataset[0][\"sentence\"]\n",
    "    example_prompt = create_sst2_prompt(example_sentence, include_label=False)\n",
    "    print(f\"\\nExample prompt:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(example_prompt)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ed4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "from data_sst2 import tokenize_sst2_for_lm\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Tokenizing datasets...\")\n",
    "    \n",
    "    def tokenize_fn(examples):\n",
    "        return tokenize_sst2_for_lm(\n",
    "            examples, \n",
    "            tokenizer, \n",
    "            max_length=max_length,\n",
    "            include_labels=True\n",
    "        )\n",
    "    \n",
    "    # Tokenize train\n",
    "    print(\"  Tokenizing train split...\")\n",
    "    tokenized_train = train_dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "        desc=\"Tokenizing train\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize validation\n",
    "    print(\"  Tokenizing validation split...\")\n",
    "    tokenized_val = val_dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=val_dataset.column_names,\n",
    "        desc=\"Tokenizing validation\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTokenized dataset features:\")\n",
    "    print(f\"  {tokenized_train.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32234b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify tokenization\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Verifying tokenization...\")\n",
    "    \n",
    "    sample = tokenized_train[0]\n",
    "    \n",
    "    # Decode input\n",
    "    decoded = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=False)\n",
    "    print(f\"\\nSample decoded input:\")\n",
    "    print(decoded[:300])\n",
    "    \n",
    "    # Check labels\n",
    "    labels = sample[\"labels\"]\n",
    "    non_masked = [l for l in labels if l != -100]\n",
    "    if non_masked:\n",
    "        print(f\"\\nNon-masked label tokens: {non_masked}\")\n",
    "        print(f\"Decoded: {tokenizer.decode(non_masked)}\")\n",
    "    \n",
    "    print(f\"\\nOriginal label: {sample['original_labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08839e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Saving processed datasets...\")\n",
    "    \n",
    "    tokenized_train.save_to_disk(str(sst2_train_path.with_suffix(\"\")))\n",
    "    tokenized_val.save_to_disk(str(sst2_val_path.with_suffix(\"\")))\n",
    "    \n",
    "    # Save tokenizer for later use\n",
    "    tokenizer_path = DATA_DIR / \"sst2_tokenizer\"\n",
    "    tokenizer.save_pretrained(str(tokenizer_path))\n",
    "    \n",
    "    # Save metadata\n",
    "    import json\n",
    "    metadata = {\n",
    "        \"task\": \"sst2\",\n",
    "        \"train_size\": len(tokenized_train),\n",
    "        \"val_size\": len(tokenized_val),\n",
    "        \"max_length\": max_length,\n",
    "        \"tokenizer\": tokenizer_name,\n",
    "        \"fast_mode\": config.fast_mode,\n",
    "        \"seed\": SEED,\n",
    "        \"label_tokens\": label_tokens\n",
    "    }\n",
    "    \n",
    "    with open(DATA_DIR / \"sst2_metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Saved to {DATA_DIR}\")\n",
    "    print(f\"  - sst2_train/\")\n",
    "    print(f\"  - sst2_validation/\")\n",
    "    print(f\"  - sst2_tokenizer/\")\n",
    "    print(f\"  - sst2_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cached data (for verification or if skipped)\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "\n",
    "if SKIP_PROCESSING:\n",
    "    print(\"Loading cached SST-2 data...\")\n",
    "    tokenized_train = load_from_disk(str(sst2_train_path.with_suffix(\"\")))\n",
    "    tokenized_val = load_from_disk(str(sst2_val_path.with_suffix(\"\")))\n",
    "    \n",
    "    with open(DATA_DIR / \"sst2_metadata.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"\\nLoaded from cache:\")\n",
    "    print(f\"  Train: {len(tokenized_train)} examples\")\n",
    "    print(f\"  Validation: {len(tokenized_val)} examples\")\n",
    "    print(f\"  Max length: {metadata['max_length']}\")\n",
    "    print(f\"  Tokenizer: {metadata['tokenizer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56282be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"SST-2 DATA PREPARATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Dataset: GLUE SST-2 (Sentiment Classification)\n",
    "Mode: {'FAST' if config.fast_mode else 'FULL'}\n",
    "\n",
    "Sizes:\n",
    "  Train: {len(tokenized_train)} examples\n",
    "  Validation: {len(tokenized_val)} examples\n",
    "\n",
    "Files saved to: {DATA_DIR}\n",
    "\n",
    "Next Steps:\n",
    "  1. Run 03_data_prep_squad.ipynb to prepare SQuAD data\n",
    "  2. Run 04_teacher_cache_outputs.ipynb to cache teacher outputs\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
