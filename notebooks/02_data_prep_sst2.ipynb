{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e264daf",
   "metadata": {},
   "source": [
    "# 02 - Data Preparation: SST-2\n",
    "\n",
    "**Thesis Section Reference:** Chapter 3.6 - Tasks and Datasets\n",
    "\n",
    "This notebook prepares the SST-2 sentiment classification dataset:\n",
    "1. Load SST-2 from GLUE benchmark\n",
    "2. Create subsets for FAST MODE\n",
    "3. Tokenize for causal LM training\n",
    "4. Save processed datasets\n",
    "\n",
    "## Task Description\n",
    "- **Dataset:** GLUE SST-2 (Stanford Sentiment Treebank)\n",
    "- **Task:** Binary sentiment classification (positive/negative)\n",
    "- **Metrics:** Accuracy, F1\n",
    "- **Splits:** Train (67,349), Validation (872)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6adf6529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: FAST\n",
      "Seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Standard setup - load environment and config\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(ROOT_DIR / \"src\"))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(ROOT_DIR / \".env\")\n",
    "\n",
    "from config import load_config\n",
    "from utils_seed import set_seed\n",
    "\n",
    "config = load_config(str(ROOT_DIR / \"configs\" / \"experiment.yaml\"))\n",
    "config.ensure_dirs()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = config.get_seeds()[0]\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"Mode: {'FAST' if config.fast_mode else 'FULL'}\")\n",
    "print(f\"Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80aa224b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST-2 data not found, will process...\n"
     ]
    }
   ],
   "source": [
    "# Check if data already exists (idempotent)\n",
    "DATA_DIR = ROOT_DIR / \"results\" / \"processed_data\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sst2_train_path = DATA_DIR / \"sst2_train.arrow\"\n",
    "sst2_val_path = DATA_DIR / \"sst2_validation.arrow\"\n",
    "\n",
    "if sst2_train_path.exists() and sst2_val_path.exists():\n",
    "    print(\"✓ SST-2 data already exists, loading from cache...\")\n",
    "    SKIP_PROCESSING = True\n",
    "else:\n",
    "    print(\"SST-2 data not found, will process...\")\n",
    "    SKIP_PROCESSING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4290f1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST-2 from GLUE...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af21ffd7fadc4ecf8b2f0e07bce1d027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87ec0949a49493a9195d945ff98114d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sst2/train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac022394b564e5ab23dde149df87e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sst2/validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce5439638b242c1859aae523cacb589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sst2/test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8807e220f0884542b30c86800a2a7019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be085ed3915438ca73a5cdee74cead2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e46de3e447547f2bf8ecfa4924d678e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 67349\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 872\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1821\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample examples:\n",
      "  [negative] hide new secretions from the parental units ...\n",
      "  [negative] contains no wit , only labored gags ...\n",
      "  [positive] that loves its characters and communicates something rather beautiful about huma...\n"
     ]
    }
   ],
   "source": [
    "# Load SST-2 dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Loading SST-2 from GLUE...\")\n",
    "    \n",
    "    raw_dataset = load_dataset(\n",
    "        \"glue\", \n",
    "        \"sst2\",\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDataset structure:\")\n",
    "    print(raw_dataset)\n",
    "    \n",
    "    print(f\"\\nSample examples:\")\n",
    "    for i in range(3):\n",
    "        ex = raw_dataset[\"train\"][i]\n",
    "        label = \"positive\" if ex[\"label\"] == 1 else \"negative\"\n",
    "        print(f\"  [{label}] {ex['sentence'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ddc0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAST MODE: Subsetting to 2000 train, 500 validation examples\n",
      "\n",
      "Final sizes:\n",
      "  Train: 2000\n",
      "  Validation: 500\n",
      "\n",
      "Label distribution (train):\n",
      "  Positive: 56.0%\n",
      "  Negative: 44.0%\n"
     ]
    }
   ],
   "source": [
    "# Create subsets based on mode\n",
    "if not SKIP_PROCESSING:\n",
    "    train_size = config.get_subset_size(\"sst2\", \"train\")\n",
    "    val_size = config.get_subset_size(\"sst2\", \"validation\")\n",
    "    \n",
    "    if train_size is not None:\n",
    "        print(f\"FAST MODE: Subsetting to {train_size} train, {val_size} validation examples\")\n",
    "        \n",
    "        train_dataset = raw_dataset[\"train\"].shuffle(seed=SEED).select(range(train_size))\n",
    "        val_dataset = raw_dataset[\"validation\"].shuffle(seed=SEED).select(range(min(val_size, len(raw_dataset[\"validation\"]))))\n",
    "    else:\n",
    "        print(\"FULL MODE: Using complete dataset\")\n",
    "        train_dataset = raw_dataset[\"train\"]\n",
    "        val_dataset = raw_dataset[\"validation\"]\n",
    "    \n",
    "    print(f\"\\nFinal sizes:\")\n",
    "    print(f\"  Train: {len(train_dataset)}\")\n",
    "    print(f\"  Validation: {len(val_dataset)}\")\n",
    "    \n",
    "    # Check label distribution\n",
    "    train_labels = train_dataset[\"label\"]\n",
    "    pos_ratio = sum(train_labels) / len(train_labels)\n",
    "    print(f\"\\nLabel distribution (train):\")\n",
    "    print(f\"  Positive: {pos_ratio:.1%}\")\n",
    "    print(f\"  Negative: {1-pos_ratio:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb56892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  Vocab size: 32000\n",
      "  Pad token: </s> (id: 2)\n",
      "  EOS token: </s> (id: 2)\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    # Use student tokenizer (will be used for all models)\n",
    "    tokenizer_name = os.getenv(\"STUDENT_S1\", config.student_s1.name)\n",
    "    \n",
    "    print(f\"Loading tokenizer: {tokenizer_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "    )\n",
    "    \n",
    "    # Ensure pad token exists\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    print(f\"  Vocab size: {tokenizer.vocab_size}\")\n",
    "    print(f\"  Pad token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")\n",
    "    print(f\"  EOS token: {tokenizer.eos_token} (id: {tokenizer.eos_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52dae1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 256\n",
      "\n",
      "Label tokens:\n",
      "  positive: token ID 29871\n",
      "  negative: token ID 29871\n",
      "\n",
      "Example prompt:\n",
      "----------------------------------------\n",
      "Classify the sentiment of the following sentence as positive or negative.\n",
      "\n",
      "Sentence: klein , charming in comedies like american pie and dead-on in election , \n",
      "\n",
      "Sentiment:\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define prompt template and tokenization\n",
    "from data_sst2 import create_sst2_prompt, get_sst2_label_tokens\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    max_length = config.get_max_length(\"sst2\")\n",
    "    print(f\"Max sequence length: {max_length}\")\n",
    "    \n",
    "    # Get label token IDs for classification\n",
    "    label_tokens = get_sst2_label_tokens(tokenizer)\n",
    "    print(f\"\\nLabel tokens:\")\n",
    "    print(f\"  positive: token ID {label_tokens['positive']}\")\n",
    "    print(f\"  negative: token ID {label_tokens['negative']}\")\n",
    "    \n",
    "    # Show example prompt\n",
    "    example_sentence = train_dataset[0][\"sentence\"]\n",
    "    example_prompt = create_sst2_prompt(example_sentence, include_label=False)\n",
    "    print(f\"\\nExample prompt:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(example_prompt)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e47ed4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n",
      "  Tokenizing train split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af720622744941e8b5758455787760f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing validation split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076437950bad432b82bd8ffea07dfae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized dataset features:\n",
      "  {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64')), 'original_labels': Value('int64')}\n"
     ]
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "from data_sst2 import tokenize_sst2_for_lm\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Tokenizing datasets...\")\n",
    "    \n",
    "    def tokenize_fn(examples):\n",
    "        return tokenize_sst2_for_lm(\n",
    "            examples, \n",
    "            tokenizer, \n",
    "            max_length=max_length,\n",
    "            include_labels=True\n",
    "        )\n",
    "    \n",
    "    # Tokenize train\n",
    "    print(\"  Tokenizing train split...\")\n",
    "    tokenized_train = train_dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "        desc=\"Tokenizing train\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize validation\n",
    "    print(\"  Tokenizing validation split...\")\n",
    "    tokenized_val = val_dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=val_dataset.column_names,\n",
    "        desc=\"Tokenizing validation\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTokenized dataset features:\")\n",
    "    print(f\"  {tokenized_train.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a32234b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying tokenization...\n",
      "\n",
      "Sample decoded input:\n",
      "<s> Classify the sentiment of the following sentence as positive or negative.\n",
      "\n",
      "Sentence: klein , charming in comedies like american pie and dead-on in election , \n",
      "\n",
      "Sentiment: positive</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><\n",
      "\n",
      "Original label: 1\n"
     ]
    }
   ],
   "source": [
    "# Verify tokenization\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Verifying tokenization...\")\n",
    "    \n",
    "    sample = tokenized_train[0]\n",
    "    \n",
    "    # Decode input\n",
    "    decoded = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=False)\n",
    "    print(f\"\\nSample decoded input:\")\n",
    "    print(decoded[:300])\n",
    "    \n",
    "    # Check labels\n",
    "    labels = sample[\"labels\"]\n",
    "    non_masked = [l for l in labels if l != -100]\n",
    "    if non_masked:\n",
    "        print(f\"\\nNon-masked label tokens: {non_masked}\")\n",
    "        print(f\"Decoded: {tokenizer.decode(non_masked)}\")\n",
    "    \n",
    "    print(f\"\\nOriginal label: {sample['original_labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08839e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d4eefdfbba4b3787359626b01dfd62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c202cf55754de792fa23827891ae27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved to /Users/pjere/Workshop/thesis-exp/results/processed_data\n",
      "  - sst2_train/\n",
      "  - sst2_validation/\n",
      "  - sst2_tokenizer/\n",
      "  - sst2_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Saving processed datasets...\")\n",
    "    \n",
    "    tokenized_train.save_to_disk(str(sst2_train_path.with_suffix(\"\")))\n",
    "    tokenized_val.save_to_disk(str(sst2_val_path.with_suffix(\"\")))\n",
    "    \n",
    "    # Save tokenizer for later use\n",
    "    tokenizer_path = DATA_DIR / \"sst2_tokenizer\"\n",
    "    tokenizer.save_pretrained(str(tokenizer_path))\n",
    "    \n",
    "    # Save metadata\n",
    "    import json\n",
    "    metadata = {\n",
    "        \"task\": \"sst2\",\n",
    "        \"train_size\": len(tokenized_train),\n",
    "        \"val_size\": len(tokenized_val),\n",
    "        \"max_length\": max_length,\n",
    "        \"tokenizer\": tokenizer_name,\n",
    "        \"fast_mode\": config.fast_mode,\n",
    "        \"seed\": SEED,\n",
    "        \"label_tokens\": label_tokens\n",
    "    }\n",
    "    \n",
    "    with open(DATA_DIR / \"sst2_metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Saved to {DATA_DIR}\")\n",
    "    print(f\"  - sst2_train/\")\n",
    "    print(f\"  - sst2_validation/\")\n",
    "    print(f\"  - sst2_tokenizer/\")\n",
    "    print(f\"  - sst2_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3172003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cached data (for verification or if skipped)\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "\n",
    "if SKIP_PROCESSING:\n",
    "    print(\"Loading cached SST-2 data...\")\n",
    "    tokenized_train = load_from_disk(str(sst2_train_path.with_suffix(\"\")))\n",
    "    tokenized_val = load_from_disk(str(sst2_val_path.with_suffix(\"\")))\n",
    "    \n",
    "    with open(DATA_DIR / \"sst2_metadata.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"\\nLoaded from cache:\")\n",
    "    print(f\"  Train: {len(tokenized_train)} examples\")\n",
    "    print(f\"  Validation: {len(tokenized_val)} examples\")\n",
    "    print(f\"  Max length: {metadata['max_length']}\")\n",
    "    print(f\"  Tokenizer: {metadata['tokenizer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56282be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SST-2 DATA PREPARATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Dataset: GLUE SST-2 (Sentiment Classification)\n",
      "Mode: FAST\n",
      "\n",
      "Sizes:\n",
      "  Train: 2000 examples\n",
      "  Validation: 500 examples\n",
      "\n",
      "Files saved to: /Users/pjere/Workshop/thesis-exp/results/processed_data\n",
      "\n",
      "Next Steps:\n",
      "  1. Run 03_data_prep_squad.ipynb to prepare SQuAD data\n",
      "  2. Run 04_teacher_cache_outputs.ipynb to cache teacher outputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"SST-2 DATA PREPARATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Dataset: GLUE SST-2 (Sentiment Classification)\n",
    "Mode: {'FAST' if config.fast_mode else 'FULL'}\n",
    "\n",
    "Sizes:\n",
    "  Train: {len(tokenized_train)} examples\n",
    "  Validation: {len(tokenized_val)} examples\n",
    "\n",
    "Files saved to: {DATA_DIR}\n",
    "\n",
    "Next Steps:\n",
    "  1. Run 03_data_prep_squad.ipynb to prepare SQuAD data\n",
    "  2. Run 04_teacher_cache_outputs.ipynb to cache teacher outputs\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
