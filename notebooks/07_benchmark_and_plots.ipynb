{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da5cb28",
   "metadata": {},
   "source": [
    "# 07 - Benchmarking and Thesis Plots\n",
    "\n",
    "**Thesis Section Reference:** Chapter 4.5-4.7 - Efficiency Analysis and Results\n",
    "\n",
    "This notebook:\n",
    "1. Benchmarks all trained models (latency, throughput, memory)\n",
    "2. Generates thesis figures (Fig 4.1-4.6)\n",
    "3. Creates summary tables (Table 4.1-4.4)\n",
    "4. Performs statistical significance testing\n",
    "5. Optionally applies quantization for additional compression\n",
    "\n",
    "## Outputs\n",
    "- **Figures:** PNG files for thesis Chapter 4\n",
    "- **Tables:** CSV files with main results and ablations\n",
    "- **Statistical tests:** Paired t-test results, Cohen's d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard setup\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ROOT_DIR = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(ROOT_DIR / \"src\"))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(ROOT_DIR / \".env\")\n",
    "\n",
    "from config import load_config\n",
    "\n",
    "config = load_config(str(ROOT_DIR / \"configs\" / \"experiment.yaml\"))\n",
    "config.ensure_dirs()\n",
    "\n",
    "# Device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Mode: {'FAST' if config.fast_mode else 'FULL'}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "MODELS_DIR = ROOT_DIR / \"results\" / \"models\"\n",
    "RUNS_DIR = ROOT_DIR / \"results\" / \"raw_runs\"\n",
    "FIGURES_DIR = ROOT_DIR / \"results\" / \"figures\"\n",
    "SUMMARY_DIR = ROOT_DIR / \"results\" / \"summary\"\n",
    "\n",
    "for d in [FIGURES_DIR, SUMMARY_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Figures will be saved to: {FIGURES_DIR}\")\n",
    "print(f\"Tables will be saved to: {SUMMARY_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95628b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all training results\n",
    "print(\"Loading training results...\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Load from notebook 05\n",
    "if (RUNS_DIR / \"nb05_results.csv\").exists():\n",
    "    df_05 = pd.read_csv(RUNS_DIR / \"nb05_results.csv\")\n",
    "    all_results.append(df_05)\n",
    "    print(f\"  nb05: {len(df_05)} runs\")\n",
    "\n",
    "# Load from notebook 06\n",
    "if (RUNS_DIR / \"nb06_results.csv\").exists():\n",
    "    df_06 = pd.read_csv(RUNS_DIR / \"nb06_results.csv\")\n",
    "    all_results.append(df_06)\n",
    "    print(f\"  nb06: {len(df_06)} runs\")\n",
    "\n",
    "# Combine\n",
    "if all_results:\n",
    "    df_all = pd.concat(all_results, ignore_index=True)\n",
    "    print(f\"\\nTotal: {len(df_all)} runs\")\n",
    "else:\n",
    "    print(\"No results found! Run notebooks 05 and 06 first.\")\n",
    "    df_all = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad90165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List trained models\n",
    "print(\"Trained models:\")\n",
    "\n",
    "model_dirs = [d for d in MODELS_DIR.iterdir() if d.is_dir()]\n",
    "\n",
    "for model_dir in sorted(model_dirs):\n",
    "    final_dir = model_dir / \"final\"\n",
    "    if final_dir.exists():\n",
    "        print(f\"  âœ“ {model_dir.name}\")\n",
    "    else:\n",
    "        print(f\"  ? {model_dir.name} (no final checkpoint)\")\n",
    "\n",
    "print(f\"\\nTotal: {len(model_dirs)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95604e82",
   "metadata": {},
   "source": [
    "## Section 1: Efficiency Benchmarking\n",
    "\n",
    "Measure latency, throughput, and memory for each trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark all models\n",
    "from bench import run_full_benchmark\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Check if benchmarks already exist\n",
    "benchmark_file = SUMMARY_DIR / \"benchmarks.csv\"\n",
    "\n",
    "if benchmark_file.exists():\n",
    "    print(\"Loading existing benchmarks...\")\n",
    "    df_bench = pd.read_csv(benchmark_file)\n",
    "    print(f\"  Loaded {len(df_bench)} benchmark results\")\n",
    "else:\n",
    "    print(\"Running benchmarks...\")\n",
    "    print(\"(This may take a while)\")\n",
    "    \n",
    "    benchmark_results = []\n",
    "    \n",
    "    # Get student base model name\n",
    "    student_name = os.getenv(\"STUDENT_S1\", config.student_s1.name)\n",
    "    \n",
    "    for model_dir in sorted(model_dirs):\n",
    "        final_dir = model_dir / \"final\"\n",
    "        if not final_dir.exists():\n",
    "            continue\n",
    "        \n",
    "        run_id = model_dir.name\n",
    "        print(f\"\\nBenchmarking: {run_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Load base model\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\n",
    "                student_name,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "            )\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                student_name,\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float32,\n",
    "                cache_dir=str(ROOT_DIR / \"hf_cache\"),\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            # Load LoRA weights\n",
    "            model = PeftModel.from_pretrained(base_model, str(final_dir))\n",
    "            model = model.merge_and_unload()  # Merge for inference\n",
    "            model = model.to(DEVICE)\n",
    "            model.eval()\n",
    "            \n",
    "            # Run benchmark\n",
    "            bench_result = run_full_benchmark(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device=DEVICE,\n",
    "                num_warmup=3,\n",
    "                num_runs=10,\n",
    "                sequence_lengths=[32, 128],\n",
    "                batch_sizes=[1]\n",
    "            )\n",
    "            \n",
    "            bench_result[\"run_id\"] = run_id\n",
    "            benchmark_results.append(bench_result)\n",
    "            \n",
    "            print(f\"  Latency (32 tok): {bench_result['latency_32']:.2f}ms\")\n",
    "            print(f\"  Latency (128 tok): {bench_result['latency_128']:.2f}ms\")\n",
    "            \n",
    "            # Cleanup\n",
    "            del model, base_model\n",
    "            if DEVICE.type == \"mps\":\n",
    "                torch.mps.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Save benchmarks\n",
    "    df_bench = pd.DataFrame(benchmark_results)\n",
    "    df_bench.to_csv(benchmark_file, index=False)\n",
    "    print(f\"\\nSaved benchmarks to {benchmark_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e4c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add teacher model benchmark (if not already done)\n",
    "if 'teacher' not in df_bench.get('run_id', pd.Series()).values:\n",
    "    print(\"Benchmarking teacher model for comparison...\")\n",
    "    \n",
    "    try:\n",
    "        teacher_name = os.getenv(\"TEACHER_FALLBACK\", config.teacher.fallback)\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            teacher_name,\n",
    "            trust_remote_code=True,\n",
    "            cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "        )\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            teacher_name,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float32,\n",
    "            cache_dir=str(ROOT_DIR / \"hf_cache\"),\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        model = model.to(DEVICE)\n",
    "        model.eval()\n",
    "        \n",
    "        bench_result = run_full_benchmark(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=DEVICE,\n",
    "            num_warmup=3,\n",
    "            num_runs=10,\n",
    "            sequence_lengths=[32, 128],\n",
    "            batch_sizes=[1]\n",
    "        )\n",
    "        bench_result[\"run_id\"] = \"teacher\"\n",
    "        \n",
    "        # Add to dataframe\n",
    "        df_bench = pd.concat([df_bench, pd.DataFrame([bench_result])], ignore_index=True)\n",
    "        df_bench.to_csv(benchmark_file, index=False)\n",
    "        \n",
    "        print(f\"  Teacher latency (32 tok): {bench_result['latency_32']:.2f}ms\")\n",
    "        \n",
    "        del model\n",
    "        if DEVICE.type == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Could not benchmark teacher: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b54548",
   "metadata": {},
   "source": [
    "## Section 2: Create Summary Tables\n",
    "\n",
    "Generate tables for thesis Chapter 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15809701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 4.1: Main Results\n",
    "print(\"Creating Table 4.1: Main Results\")\n",
    "\n",
    "if not df_all.empty:\n",
    "    # Group by method and task, compute mean/std across seeds\n",
    "    summary = df_all.groupby(['method', 'task']).agg({\n",
    "        'eval_loss': ['mean', 'std'],\n",
    "    }).round(4)\n",
    "    \n",
    "    summary.columns = ['loss_mean', 'loss_std']\n",
    "    summary = summary.reset_index()\n",
    "    \n",
    "    # Merge with benchmarks\n",
    "    if not df_bench.empty:\n",
    "        # Extract method from run_id\n",
    "        df_bench_copy = df_bench.copy()\n",
    "        df_bench_copy['method'] = df_bench_copy['run_id'].apply(\n",
    "            lambda x: x.split('_')[0] if x != 'teacher' else 'Teacher'\n",
    "        )\n",
    "        \n",
    "        bench_summary = df_bench_copy.groupby('method').agg({\n",
    "            'latency_32': 'mean',\n",
    "            'latency_128': 'mean',\n",
    "            'memory_mb': 'mean'\n",
    "        }).round(2)\n",
    "        bench_summary = bench_summary.reset_index()\n",
    "        \n",
    "        summary = summary.merge(bench_summary, on='method', how='left')\n",
    "    \n",
    "    # Save\n",
    "    summary.to_csv(SUMMARY_DIR / \"table_4_1_main_results.csv\", index=False)\n",
    "    \n",
    "    print(summary.to_string())\n",
    "else:\n",
    "    print(\"No results to summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 4.2: KD1 Ablation (Temperature vs Alpha)\n",
    "print(\"\\nCreating Table 4.2: KD1 Ablation\")\n",
    "\n",
    "if not df_all.empty:\n",
    "    df_kd1 = df_all[df_all['method'] == 'KD1']\n",
    "    \n",
    "    if not df_kd1.empty and 'temperature' in df_kd1.columns:\n",
    "        # Pivot table\n",
    "        ablation = df_kd1.pivot_table(\n",
    "            values='eval_loss',\n",
    "            index='temperature',\n",
    "            columns='alpha',\n",
    "            aggfunc='mean'\n",
    "        ).round(4)\n",
    "        \n",
    "        ablation.to_csv(SUMMARY_DIR / \"table_4_2_kd1_ablation.csv\")\n",
    "        print(ablation.to_string())\n",
    "    else:\n",
    "        print(\"No KD1 ablation data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6dec81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 4.3: Statistical Significance\n",
    "from stats import paired_t_test, cohens_d, create_significance_table\n",
    "\n",
    "print(\"\\nCreating Table 4.3: Statistical Significance\")\n",
    "\n",
    "if not df_all.empty:\n",
    "    sig_results = []\n",
    "    \n",
    "    # Compare each KD method against baseline\n",
    "    for task in df_all['task'].unique():\n",
    "        df_task = df_all[df_all['task'] == task]\n",
    "        \n",
    "        # Get baseline results\n",
    "        baseline = df_task[df_task['method'] == 'B0']['eval_loss'].values\n",
    "        \n",
    "        if len(baseline) == 0:\n",
    "            continue\n",
    "        \n",
    "        for method in ['KD1', 'KD2', 'KD3']:\n",
    "            method_results = df_task[df_task['method'] == method]['eval_loss'].values\n",
    "            \n",
    "            if len(method_results) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Perform t-test (if enough samples)\n",
    "            if len(baseline) >= 2 and len(method_results) >= 2:\n",
    "                # Align lengths\n",
    "                min_len = min(len(baseline), len(method_results))\n",
    "                t_stat, p_value = paired_t_test(\n",
    "                    baseline[:min_len],\n",
    "                    method_results[:min_len]\n",
    "                )\n",
    "                d = cohens_d(baseline[:min_len], method_results[:min_len])\n",
    "            else:\n",
    "                t_stat, p_value, d = np.nan, np.nan, np.nan\n",
    "            \n",
    "            sig_results.append({\n",
    "                'task': task,\n",
    "                'comparison': f'B0 vs {method}',\n",
    "                'baseline_mean': np.mean(baseline),\n",
    "                'method_mean': np.mean(method_results),\n",
    "                'improvement': np.mean(baseline) - np.mean(method_results),\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'cohens_d': d,\n",
    "                'significant': p_value < 0.05 if not np.isnan(p_value) else False\n",
    "            })\n",
    "    \n",
    "    df_sig = pd.DataFrame(sig_results)\n",
    "    df_sig.to_csv(SUMMARY_DIR / \"table_4_3_significance.csv\", index=False)\n",
    "    \n",
    "    print(df_sig.to_string())\n",
    "else:\n",
    "    print(\"No data for significance testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4037e6f8",
   "metadata": {},
   "source": [
    "## Section 3: Generate Thesis Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaae894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4.1: Performance vs Model Size\n",
    "from plots import plot_performance_vs_size\n",
    "\n",
    "print(\"Creating Figure 4.1: Performance vs Model Size\")\n",
    "\n",
    "if not df_all.empty:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Prepare data\n",
    "    plot_data = df_all.groupby(['method', 'task']).agg({\n",
    "        'eval_loss': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Add model sizes (approximate)\n",
    "    size_map = {\n",
    "        'B0': 1.1,  # TinyLlama 1.1B\n",
    "        'KD1': 1.1,\n",
    "        'KD2': 1.1,\n",
    "        'KD3': 1.1,\n",
    "        'Teacher': 3.0,  # Fallback teacher\n",
    "    }\n",
    "    plot_data['size_b'] = plot_data['method'].map(size_map).fillna(1.0)\n",
    "    \n",
    "    # Plot\n",
    "    for task in plot_data['task'].unique():\n",
    "        task_data = plot_data[plot_data['task'] == task]\n",
    "        ax.scatter(\n",
    "            task_data['size_b'],\n",
    "            task_data['eval_loss'],\n",
    "            s=100,\n",
    "            label=task,\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "        for _, row in task_data.iterrows():\n",
    "            ax.annotate(\n",
    "                row['method'],\n",
    "                (row['size_b'], row['eval_loss']),\n",
    "                textcoords=\"offset points\",\n",
    "                xytext=(5, 5),\n",
    "                fontsize=8\n",
    "            )\n",
    "    \n",
    "    ax.set_xlabel('Model Size (Billions of Parameters)', fontsize=12)\n",
    "    ax.set_ylabel('Eval Loss', fontsize=12)\n",
    "    ax.set_title('Figure 4.1: Performance vs Model Size', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / \"fig_4_1_performance_vs_size.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved to {FIGURES_DIR / 'fig_4_1_performance_vs_size.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c48398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4.2: Latency Comparison\n",
    "print(\"\\nCreating Figure 4.2: Latency Comparison\")\n",
    "\n",
    "if not df_bench.empty:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Extract method from run_id\n",
    "    df_bench_plot = df_bench.copy()\n",
    "    df_bench_plot['method'] = df_bench_plot['run_id'].apply(\n",
    "        lambda x: x.split('_')[0] if x != 'teacher' else 'Teacher'\n",
    "    )\n",
    "    \n",
    "    # Group by method\n",
    "    latency_summary = df_bench_plot.groupby('method').agg({\n",
    "        'latency_32': ['mean', 'std'],\n",
    "        'latency_128': ['mean', 'std']\n",
    "    })\n",
    "    latency_summary.columns = ['lat_32_mean', 'lat_32_std', 'lat_128_mean', 'lat_128_std']\n",
    "    latency_summary = latency_summary.reset_index()\n",
    "    \n",
    "    # Sort by latency\n",
    "    latency_summary = latency_summary.sort_values('lat_32_mean')\n",
    "    \n",
    "    # Plot 32 tokens\n",
    "    axes[0].barh(\n",
    "        latency_summary['method'],\n",
    "        latency_summary['lat_32_mean'],\n",
    "        xerr=latency_summary['lat_32_std'],\n",
    "        color=sns.color_palette(\"husl\", len(latency_summary)),\n",
    "        alpha=0.8\n",
    "    )\n",
    "    axes[0].set_xlabel('Latency (ms)', fontsize=12)\n",
    "    axes[0].set_title('32 Token Input', fontsize=12)\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Plot 128 tokens\n",
    "    axes[1].barh(\n",
    "        latency_summary['method'],\n",
    "        latency_summary['lat_128_mean'],\n",
    "        xerr=latency_summary['lat_128_std'],\n",
    "        color=sns.color_palette(\"husl\", len(latency_summary)),\n",
    "        alpha=0.8\n",
    "    )\n",
    "    axes[1].set_xlabel('Latency (ms)', fontsize=12)\n",
    "    axes[1].set_title('128 Token Input', fontsize=12)\n",
    "    axes[1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    fig.suptitle('Figure 4.2: Inference Latency Comparison', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / \"fig_4_2_latency.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved to {FIGURES_DIR / 'fig_4_2_latency.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd8b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4.3: KD Method Comparison\n",
    "print(\"\\nCreating Figure 4.3: KD Method Comparison\")\n",
    "\n",
    "if not df_all.empty:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Prepare data\n",
    "    method_order = ['B0', 'KD1', 'KD2', 'KD3']\n",
    "    df_plot = df_all[df_all['method'].isin(method_order)]\n",
    "    \n",
    "    # Box plot\n",
    "    sns.boxplot(\n",
    "        data=df_plot,\n",
    "        x='method',\n",
    "        y='eval_loss',\n",
    "        hue='task',\n",
    "        order=method_order,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_xlabel('Method', fontsize=12)\n",
    "    ax.set_ylabel('Eval Loss', fontsize=12)\n",
    "    ax.set_title('Figure 4.3: KD Method Comparison', fontsize=14)\n",
    "    ax.legend(title='Task')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / \"fig_4_3_kd_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved to {FIGURES_DIR / 'fig_4_3_kd_comparison.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32abaf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4.4: Pareto Frontier (Performance vs Efficiency)\n",
    "print(\"\\nCreating Figure 4.4: Pareto Frontier\")\n",
    "\n",
    "if not df_all.empty and not df_bench.empty:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Merge results with benchmarks\n",
    "    df_merged = df_all.merge(\n",
    "        df_bench[['run_id', 'latency_128', 'memory_mb']],\n",
    "        on='run_id',\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    if not df_merged.empty:\n",
    "        # Color by method\n",
    "        colors = {'B0': 'blue', 'KD1': 'green', 'KD2': 'orange', 'KD3': 'red'}\n",
    "        \n",
    "        for method in df_merged['method'].unique():\n",
    "            method_data = df_merged[df_merged['method'] == method]\n",
    "            ax.scatter(\n",
    "                method_data['latency_128'],\n",
    "                method_data['eval_loss'],\n",
    "                c=colors.get(method, 'gray'),\n",
    "                s=100,\n",
    "                label=method,\n",
    "                alpha=0.7\n",
    "            )\n",
    "        \n",
    "        # Draw Pareto frontier\n",
    "        sorted_data = df_merged.sort_values('latency_128')\n",
    "        pareto_front = []\n",
    "        min_loss = float('inf')\n",
    "        \n",
    "        for _, row in sorted_data.iterrows():\n",
    "            if row['eval_loss'] < min_loss:\n",
    "                pareto_front.append((row['latency_128'], row['eval_loss']))\n",
    "                min_loss = row['eval_loss']\n",
    "        \n",
    "        if pareto_front:\n",
    "            pareto_x, pareto_y = zip(*pareto_front)\n",
    "            ax.plot(pareto_x, pareto_y, 'k--', alpha=0.5, label='Pareto Frontier')\n",
    "        \n",
    "        ax.set_xlabel('Latency (ms)', fontsize=12)\n",
    "        ax.set_ylabel('Eval Loss', fontsize=12)\n",
    "        ax.set_title('Figure 4.4: Pareto Frontier (Performance vs Efficiency)', fontsize=14)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / \"fig_4_4_pareto.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Saved to {FIGURES_DIR / 'fig_4_4_pareto.png'}\")\n",
    "    else:\n",
    "        print(\"Could not merge results with benchmarks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4563f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4.5: KD Improvement Over Baseline\n",
    "print(\"\\nCreating Figure 4.5: KD Improvement Over Baseline\")\n",
    "\n",
    "if not df_all.empty:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    improvements = []\n",
    "    \n",
    "    for task in df_all['task'].unique():\n",
    "        df_task = df_all[df_all['task'] == task]\n",
    "        baseline_loss = df_task[df_task['method'] == 'B0']['eval_loss'].mean()\n",
    "        \n",
    "        for method in ['KD1', 'KD2', 'KD3']:\n",
    "            method_loss = df_task[df_task['method'] == method]['eval_loss'].mean()\n",
    "            if not np.isnan(method_loss):\n",
    "                improvement = ((baseline_loss - method_loss) / baseline_loss) * 100\n",
    "                improvements.append({\n",
    "                    'task': task,\n",
    "                    'method': method,\n",
    "                    'improvement': improvement\n",
    "                })\n",
    "    \n",
    "    df_imp = pd.DataFrame(improvements)\n",
    "    \n",
    "    if not df_imp.empty:\n",
    "        # Bar plot\n",
    "        x = np.arange(len(df_imp['method'].unique()))\n",
    "        width = 0.35\n",
    "        \n",
    "        tasks = df_imp['task'].unique()\n",
    "        for i, task in enumerate(tasks):\n",
    "            task_data = df_imp[df_imp['task'] == task]\n",
    "            ax.bar(\n",
    "                x + i * width,\n",
    "                task_data['improvement'],\n",
    "                width,\n",
    "                label=task,\n",
    "                alpha=0.8\n",
    "            )\n",
    "        \n",
    "        ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        ax.set_xlabel('KD Method', fontsize=12)\n",
    "        ax.set_ylabel('Improvement over Baseline (%)', fontsize=12)\n",
    "        ax.set_title('Figure 4.5: KD Improvement Over Baseline', fontsize=14)\n",
    "        ax.set_xticks(x + width / 2)\n",
    "        ax.set_xticklabels(['KD1', 'KD2', 'KD3'])\n",
    "        ax.legend(title='Task')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / \"fig_4_5_kd_gain.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Saved to {FIGURES_DIR / 'fig_4_5_kd_gain.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffaf23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4.6: Memory Usage\n",
    "print(\"\\nCreating Figure 4.6: Memory Usage\")\n",
    "\n",
    "if not df_bench.empty and 'memory_mb' in df_bench.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Extract method\n",
    "    df_mem = df_bench.copy()\n",
    "    df_mem['method'] = df_mem['run_id'].apply(\n",
    "        lambda x: x.split('_')[0] if x != 'teacher' else 'Teacher'\n",
    "    )\n",
    "    \n",
    "    # Group by method\n",
    "    mem_summary = df_mem.groupby('method')['memory_mb'].agg(['mean', 'std']).reset_index()\n",
    "    mem_summary = mem_summary.sort_values('mean')\n",
    "    \n",
    "    # Bar chart\n",
    "    bars = ax.bar(\n",
    "        mem_summary['method'],\n",
    "        mem_summary['mean'],\n",
    "        yerr=mem_summary['std'],\n",
    "        color=sns.color_palette(\"husl\", len(mem_summary)),\n",
    "        alpha=0.8,\n",
    "        capsize=5\n",
    "    )\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, mem_summary['mean']):\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height() + 50,\n",
    "            f'{val:.0f}',\n",
    "            ha='center',\n",
    "            fontsize=10\n",
    "        )\n",
    "    \n",
    "    ax.set_xlabel('Method', fontsize=12)\n",
    "    ax.set_ylabel('Memory Usage (MB)', fontsize=12)\n",
    "    ax.set_title('Figure 4.6: Memory Usage Comparison', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / \"fig_4_6_memory.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved to {FIGURES_DIR / 'fig_4_6_memory.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd493432",
   "metadata": {},
   "source": [
    "## Section 4: Quantization (Optional)\n",
    "\n",
    "Apply post-training quantization for additional compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply quantization to best model\n",
    "from bench import apply_dynamic_quantization\n",
    "\n",
    "APPLY_QUANTIZATION = False  # Set to True to run\n",
    "\n",
    "if APPLY_QUANTIZATION and not df_all.empty:\n",
    "    print(\"Applying quantization to best model...\")\n",
    "    \n",
    "    # Find best model\n",
    "    best_run = df_all.loc[df_all['eval_loss'].idxmin()]\n",
    "    print(f\"Best model: {best_run['run_id']}\")\n",
    "    \n",
    "    # Load and quantize\n",
    "    model_dir = MODELS_DIR / best_run['run_id'] / \"final\"\n",
    "    \n",
    "    if model_dir.exists():\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            student_name,\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float32,\n",
    "            cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(base_model, str(model_dir))\n",
    "        model = model.merge_and_unload()\n",
    "        \n",
    "        # Apply quantization\n",
    "        quantized_model = apply_dynamic_quantization(model)\n",
    "        \n",
    "        # Benchmark quantized\n",
    "        quantized_model.eval()\n",
    "        quantized_bench = run_full_benchmark(\n",
    "            model=quantized_model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=torch.device(\"cpu\"),  # Quantized runs on CPU\n",
    "            num_warmup=3,\n",
    "            num_runs=10,\n",
    "            sequence_lengths=[32, 128],\n",
    "            batch_sizes=[1]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nQuantized model benchmarks:\")\n",
    "        print(f\"  Latency (32 tok): {quantized_bench['latency_32']:.2f}ms\")\n",
    "        print(f\"  Latency (128 tok): {quantized_bench['latency_128']:.2f}ms\")\n",
    "        print(f\"  Memory: {quantized_bench['memory_mb']:.0f}MB\")\n",
    "        \n",
    "        del model, quantized_model, base_model\n",
    "        gc.collect()\n",
    "else:\n",
    "    print(\"Quantization skipped (set APPLY_QUANTIZATION=True to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"BENCHMARKING AND PLOTTING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Mode: {'FAST' if config.fast_mode else 'FULL'}\n",
    "\n",
    "Generated Outputs:\n",
    "\"\"\")\n",
    "\n",
    "print(\"Tables (CSV):\")\n",
    "for f in sorted(SUMMARY_DIR.glob(\"*.csv\")):\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(\"\\nFigures (PNG):\")\n",
    "for f in sorted(FIGURES_DIR.glob(\"*.png\")):\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "\\nThesis Chapter 4 Artifacts:\n",
    "  Tables: {SUMMARY_DIR}\n",
    "  Figures: {FIGURES_DIR}\n",
    "\n",
    "You can now copy these files to your thesis document.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c9e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export tables to LaTeX (optional)\n",
    "EXPORT_LATEX = True\n",
    "\n",
    "if EXPORT_LATEX:\n",
    "    print(\"Exporting tables to LaTeX...\")\n",
    "    \n",
    "    latex_dir = SUMMARY_DIR / \"latex\"\n",
    "    latex_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Main results\n",
    "    if (SUMMARY_DIR / \"table_4_1_main_results.csv\").exists():\n",
    "        df = pd.read_csv(SUMMARY_DIR / \"table_4_1_main_results.csv\")\n",
    "        latex = df.to_latex(index=False, float_format=\"%.4f\")\n",
    "        with open(latex_dir / \"table_4_1.tex\", \"w\") as f:\n",
    "            f.write(latex)\n",
    "    \n",
    "    # Significance\n",
    "    if (SUMMARY_DIR / \"table_4_3_significance.csv\").exists():\n",
    "        df = pd.read_csv(SUMMARY_DIR / \"table_4_3_significance.csv\")\n",
    "        latex = df.to_latex(index=False, float_format=\"%.4f\")\n",
    "        with open(latex_dir / \"table_4_3.tex\", \"w\") as f:\n",
    "            f.write(latex)\n",
    "    \n",
    "    print(f\"LaTeX tables saved to {latex_dir}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
