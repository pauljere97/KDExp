{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff1a49cb",
   "metadata": {},
   "source": [
    "# Thesis Analysis: Knowledge Distillation for LLMs\n",
    "\n",
    "**Statistical Analysis & Publication-Ready Outputs**\n",
    "\n",
    "This notebook generates:\n",
    "1. Statistical significance tests (t-tests, effect sizes)\n",
    "2. Publication-quality figures (Nature/IEEE style)\n",
    "3. LaTeX tables for thesis chapters\n",
    "4. Summary statistics with confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272c8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Paths\n",
    "RESULTS_DIR = Path('../results')\n",
    "RUNS_DIR = RESULTS_DIR / 'runs'\n",
    "FIGS_DIR = RESULTS_DIR / 'figures'\n",
    "FIGS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(RUNS_DIR / 'results.csv')\n",
    "df_bench = pd.read_csv(RUNS_DIR / 'benchmarks.csv')\n",
    "\n",
    "print('Results loaded!')\n",
    "print(f'Training runs: {len(df)}')\n",
    "print(f'Benchmark runs: {len(df_bench)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6281276",
   "metadata": {},
   "source": [
    "## 1. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4abff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*60)\n",
    "print('TRAINING RESULTS')\n",
    "print('='*60)\n",
    "display(df)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('BENCHMARK RESULTS')\n",
    "print('='*60)\n",
    "display(df_bench)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d933d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary = df.groupby(['method', 'task'])['eval_loss'].agg(['mean', 'std', 'count']).reset_index()\n",
    "summary['se'] = summary['std'] / np.sqrt(summary['count'])\n",
    "summary['ci_95'] = 1.96 * summary['se']\n",
    "summary['range'] = summary.apply(lambda r: f\"{r['mean']:.4f} Â± {r['ci_95']:.4f}\", axis=1)\n",
    "\n",
    "print('Summary Statistics (Mean Â± 95% CI)')\n",
    "print('='*60)\n",
    "display(summary[['method', 'task', 'mean', 'std', 'range']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9049ddca",
   "metadata": {},
   "source": [
    "## 2. Statistical Significance Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d8693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohens_d(group1, group2):\n",
    "    \"\"\"Calculate Cohen's d effect size\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = group1.var(), group2.var()\n",
    "    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n",
    "    return (group1.mean() - group2.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "\n",
    "def interpret_cohens_d(d):\n",
    "    \"\"\"Interpret effect size\"\"\"\n",
    "    d = abs(d)\n",
    "    if d < 0.2: return 'negligible'\n",
    "    elif d < 0.5: return 'small'\n",
    "    elif d < 0.8: return 'medium'\n",
    "    else: return 'large'\n",
    "\n",
    "def interpret_p(p):\n",
    "    \"\"\"Interpret p-value\"\"\"\n",
    "    if p < 0.001: return '***'\n",
    "    elif p < 0.01: return '**'\n",
    "    elif p < 0.05: return '*'\n",
    "    else: return 'ns'\n",
    "\n",
    "print('='*60)\n",
    "print('STATISTICAL SIGNIFICANCE TESTS')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a9064e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: B0 vs KD2 on SQuAD\n",
    "b0_squad = df[(df['method'] == 'B0') & (df['task'] == 'squad')]['eval_loss']\n",
    "kd2_squad = df[(df['method'] == 'KD2') & (df['task'] == 'squad')]['eval_loss']\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(b0_squad, kd2_squad)\n",
    "d = cohens_d(b0_squad, kd2_squad)\n",
    "\n",
    "print('\\nðŸ“Š Test 1: B0 vs KD2 on SQuAD (Eval Loss)')\n",
    "print('-'*50)\n",
    "print(f'B0 Mean:  {b0_squad.mean():.4f} Â± {b0_squad.std():.4f}')\n",
    "print(f'KD2 Mean: {kd2_squad.mean():.4f} Â± {kd2_squad.std():.4f}')\n",
    "print(f'Difference: {(kd2_squad.mean() - b0_squad.mean()):.4f} ({(kd2_squad.mean() - b0_squad.mean())/b0_squad.mean()*100:.1f}%)')\n",
    "print(f\"\\nt-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.6f} {interpret_p(p_value)}\")\n",
    "print(f\"Cohen's d: {d:.4f} ({interpret_cohens_d(d)})\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    winner = 'B0' if b0_squad.mean() < kd2_squad.mean() else 'KD2'\n",
    "    print(f'\\nâœ… Result: {winner} significantly outperforms (p < 0.05)')\n",
    "else:\n",
    "    print('\\nâšª Result: No significant difference (p â‰¥ 0.05)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2661f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: B0 SST-2 vs B0 SQuAD (task difficulty)\n",
    "b0_sst2 = df[(df['method'] == 'B0') & (df['task'] == 'sst2')]['eval_loss']\n",
    "b0_squad = df[(df['method'] == 'B0') & (df['task'] == 'squad')]['eval_loss']\n",
    "\n",
    "t_stat2, p_value2 = stats.ttest_ind(b0_sst2, b0_squad)\n",
    "d2 = cohens_d(b0_sst2, b0_squad)\n",
    "\n",
    "print('\\nðŸ“Š Test 2: SST-2 vs SQuAD Difficulty (B0 Baseline)')\n",
    "print('-'*50)\n",
    "print(f'SST-2 Mean:  {b0_sst2.mean():.4f} Â± {b0_sst2.std():.4f}')\n",
    "print(f'SQuAD Mean: {b0_squad.mean():.4f} Â± {b0_squad.std():.4f}')\n",
    "print(f\"\\nt-statistic: {t_stat2:.4f}\")\n",
    "print(f\"p-value: {p_value2:.6f} {interpret_p(p_value2)}\")\n",
    "print(f\"Cohen's d: {d2:.4f} ({interpret_cohens_d(d2)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1394069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all statistical tests\n",
    "stat_results = [\n",
    "    {\n",
    "        'Comparison': 'B0 vs KD2 (SQuAD)',\n",
    "        'Group 1 Mean': f'{b0_squad.mean():.4f}',\n",
    "        'Group 2 Mean': f'{kd2_squad.mean():.4f}',\n",
    "        't-statistic': f'{t_stat:.3f}',\n",
    "        'p-value': f'{p_value:.4f}',\n",
    "        'Significance': interpret_p(p_value),\n",
    "        \"Cohen's d\": f'{d:.3f}',\n",
    "        'Effect Size': interpret_cohens_d(d)\n",
    "    },\n",
    "    {\n",
    "        'Comparison': 'SST-2 vs SQuAD (B0)',\n",
    "        'Group 1 Mean': f'{b0_sst2.mean():.4f}',\n",
    "        'Group 2 Mean': f'{b0_squad.mean():.4f}',\n",
    "        't-statistic': f'{t_stat2:.3f}',\n",
    "        'p-value': f'{p_value2:.4f}',\n",
    "        'Significance': interpret_p(p_value2),\n",
    "        \"Cohen's d\": f'{d2:.3f}',\n",
    "        'Effect Size': interpret_cohens_d(d2)\n",
    "    }\n",
    "]\n",
    "\n",
    "df_stats = pd.DataFrame(stat_results)\n",
    "print('\\n' + '='*60)\n",
    "print('STATISTICAL TESTS SUMMARY')\n",
    "print('='*60)\n",
    "display(df_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c7e6f",
   "metadata": {},
   "source": [
    "## 3. Publication-Quality Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea494f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set publication style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'font.family': 'serif',\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 11,\n",
    "    'figure.figsize': (8, 6),\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight'\n",
    "})\n",
    "\n",
    "# Color palette\n",
    "COLORS = {\n",
    "    'B0': '#2ecc71',      # Green\n",
    "    'KD2': '#3498db',     # Blue\n",
    "    'sst2': '#e74c3c',    # Red\n",
    "    'squad': '#9b59b6'   # Purple\n",
    "}\n",
    "\n",
    "print('Publication style configured!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Method Comparison Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Prepare data for SQuAD comparison\n",
    "methods = ['B0 (Baseline)', 'KD2 (Seq-Level KD)']\n",
    "means = [b0_squad.mean(), kd2_squad.mean()]\n",
    "stds = [b0_squad.std(), kd2_squad.std()]\n",
    "colors = [COLORS['B0'], COLORS['KD2']]\n",
    "\n",
    "bars = ax.bar(methods, means, yerr=stds, capsize=8, color=colors, \n",
    "              edgecolor='black', linewidth=1.5, alpha=0.85)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean, std in zip(bars, means, stds):\n",
    "    ax.annotate(f'{mean:.3f}Â±{std:.3f}',\n",
    "                xy=(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.005),\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add significance annotation\n",
    "sig_text = f'p = {p_value:.3f} ({interpret_p(p_value)})'\n",
    "y_max = max(means) + max(stds) + 0.03\n",
    "ax.plot([0, 0, 1, 1], [y_max-0.01, y_max, y_max, y_max-0.01], 'k-', lw=1.5)\n",
    "ax.text(0.5, y_max + 0.005, sig_text, ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "ax.set_ylabel('Evaluation Loss (â†“ better)')\n",
    "ax.set_title('Knowledge Distillation Methods Comparison on SQuAD')\n",
    "ax.set_ylim(0, y_max + 0.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS_DIR / 'thesis_fig1_method_comparison.png', dpi=300)\n",
    "plt.savefig(FIGS_DIR / 'thesis_fig1_method_comparison.pdf')  # Vector for thesis\n",
    "plt.show()\n",
    "print('âœ… Saved: thesis_fig1_method_comparison.png/pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1040e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Task Comparison (All Methods Ã— Tasks)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Group data\n",
    "pivot = df.groupby(['method', 'task'])['eval_loss'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "x = np.arange(len(pivot['task'].unique()))\n",
    "width = 0.35\n",
    "\n",
    "# Plot bars for each method\n",
    "for i, method in enumerate(['B0', 'KD2']):\n",
    "    data = pivot[pivot['method'] == method]\n",
    "    if len(data) > 0:\n",
    "        # Handle missing task combinations\n",
    "        tasks = ['sst2', 'squad']\n",
    "        means = [data[data['task'] == t]['mean'].values[0] if len(data[data['task'] == t]) > 0 else 0 for t in tasks]\n",
    "        stds = [data[data['task'] == t]['std'].values[0] if len(data[data['task'] == t]) > 0 else 0 for t in tasks]\n",
    "        \n",
    "        bars = ax.bar(x + i*width, means, width, yerr=stds, \n",
    "                      label=method, color=COLORS[method], capsize=5,\n",
    "                      edgecolor='black', linewidth=1, alpha=0.85)\n",
    "\n",
    "ax.set_xlabel('Task')\n",
    "ax.set_ylabel('Evaluation Loss (â†“ better)')\n",
    "ax.set_title('Performance by Method and Task')\n",
    "ax.set_xticks(x + width/2)\n",
    "ax.set_xticklabels(['SST-2 (Sentiment)', 'SQuAD (QA)'])\n",
    "ax.legend(title='Method')\n",
    "ax.set_ylim(0, 0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS_DIR / 'thesis_fig2_task_comparison.png', dpi=300)\n",
    "plt.savefig(FIGS_DIR / 'thesis_fig2_task_comparison.pdf')\n",
    "plt.show()\n",
    "print('âœ… Saved: thesis_fig2_task_comparison.png/pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a09c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Seed Variability (Box Plot)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# SST-2 results\n",
    "ax1 = axes[0]\n",
    "sst2_data = df[df['task'] == 'sst2']\n",
    "if len(sst2_data) > 0:\n",
    "    sns.boxplot(data=sst2_data, x='method', y='eval_loss', ax=ax1, \n",
    "                palette=[COLORS['B0']], width=0.5)\n",
    "    sns.stripplot(data=sst2_data, x='method', y='eval_loss', ax=ax1,\n",
    "                  color='black', size=8, alpha=0.7)\n",
    "ax1.set_title('SST-2 (Sentiment Classification)')\n",
    "ax1.set_xlabel('Method')\n",
    "ax1.set_ylabel('Evaluation Loss')\n",
    "\n",
    "# SQuAD results\n",
    "ax2 = axes[1]\n",
    "squad_data = df[df['task'] == 'squad']\n",
    "sns.boxplot(data=squad_data, x='method', y='eval_loss', ax=ax2,\n",
    "            palette=[COLORS['B0'], COLORS['KD2']], width=0.5)\n",
    "sns.stripplot(data=squad_data, x='method', y='eval_loss', ax=ax2,\n",
    "              color='black', size=8, alpha=0.7)\n",
    "ax2.set_title('SQuAD (Question Answering)')\n",
    "ax2.set_xlabel('Method')\n",
    "ax2.set_ylabel('Evaluation Loss')\n",
    "\n",
    "plt.suptitle('Seed Variability Across Training Runs (n=3 per condition)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS_DIR / 'thesis_fig3_variability.png', dpi=300)\n",
    "plt.savefig(FIGS_DIR / 'thesis_fig3_variability.pdf')\n",
    "plt.show()\n",
    "print('âœ… Saved: thesis_fig3_variability.png/pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: Latency Comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Parse method from benchmark IDs\n",
    "df_bench['method'] = df_bench['id'].apply(lambda x: x.split('_')[0])\n",
    "df_bench['task'] = df_bench['id'].apply(lambda x: x.split('_')[1] if len(x.split('_')) > 1 else 'unknown')\n",
    "\n",
    "# Group by method\n",
    "lat_summary = df_bench.groupby('method')['lat_mean'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "colors = [COLORS.get(m, '#95a5a6') for m in lat_summary['method']]\n",
    "bars = ax.bar(lat_summary['method'], lat_summary['mean'], yerr=lat_summary['std'],\n",
    "              capsize=8, color=colors, edgecolor='black', linewidth=1.5, alpha=0.85)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean, std in zip(bars, lat_summary['mean'], lat_summary['std']):\n",
    "    ax.annotate(f'{mean:.1f} ms',\n",
    "                xy=(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 5),\n",
    "                ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Method')\n",
    "ax.set_ylabel('Inference Latency (ms, â†“ better)')\n",
    "ax.set_title('Inference Latency by Training Method (H100 GPU)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS_DIR / 'thesis_fig4_latency.png', dpi=300)\n",
    "plt.savefig(FIGS_DIR / 'thesis_fig4_latency.pdf')\n",
    "plt.show()\n",
    "print('âœ… Saved: thesis_fig4_latency.png/pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece08951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 5: Combined Performance + Latency (Pareto-style)\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Merge results with benchmarks\n",
    "df_merged = df.merge(df_bench[['id', 'lat_mean', 'lat_std']], on='id', how='left')\n",
    "\n",
    "# Plot each point\n",
    "for method in df_merged['method'].unique():\n",
    "    subset = df_merged[df_merged['method'] == method]\n",
    "    ax.scatter(subset['lat_mean'], subset['eval_loss'], \n",
    "               s=150, label=method, color=COLORS.get(method, '#95a5a6'),\n",
    "               edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "# Add annotations for each point\n",
    "for _, row in df_merged.iterrows():\n",
    "    if pd.notna(row['lat_mean']):\n",
    "        ax.annotate(f\"{row['task']}\\ns{row['seed']}\",\n",
    "                    xy=(row['lat_mean'], row['eval_loss']),\n",
    "                    xytext=(5, 5), textcoords='offset points',\n",
    "                    fontsize=8, alpha=0.7)\n",
    "\n",
    "# Add Pareto frontier arrow\n",
    "ax.annotate('Better', xy=(0.15, 0.15), xycoords='axes fraction',\n",
    "            fontsize=12, ha='center',\n",
    "            arrowprops=dict(arrowstyle='->', color='gray'))\n",
    "ax.annotate('', xy=(0.05, 0.05), xycoords='axes fraction',\n",
    "            xytext=(0.15, 0.15), textcoords='axes fraction',\n",
    "            arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n",
    "\n",
    "ax.set_xlabel('Inference Latency (ms, â†“ better)')\n",
    "ax.set_ylabel('Evaluation Loss (â†“ better)')\n",
    "ax.set_title('Quality vs Efficiency Trade-off')\n",
    "ax.legend(title='Method')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS_DIR / 'thesis_fig5_pareto.png', dpi=300)\n",
    "plt.savefig(FIGS_DIR / 'thesis_fig5_pareto.pdf')\n",
    "plt.show()\n",
    "print('âœ… Saved: thesis_fig5_pareto.png/pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49ffba",
   "metadata": {},
   "source": [
    "## 4. LaTeX Tables for Thesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6325be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Main Results Table\n",
    "print('='*70)\n",
    "print('TABLE 1: Main Experimental Results')\n",
    "print('='*70)\n",
    "\n",
    "latex_table1 = r'''\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Experimental Results: Evaluation Loss by Method and Task}\n",
    "\\label{tab:main_results}\n",
    "\\begin{tabular}{llccc}\n",
    "\\toprule\n",
    "\\textbf{Method} & \\textbf{Task} & \\textbf{Mean} & \\textbf{Std} & \\textbf{95\\% CI} \\\\\n",
    "\\midrule\n",
    "'''\n",
    "\n",
    "for _, row in summary.iterrows():\n",
    "    latex_table1 += f\"{row['method']} & {row['task'].upper()} & {row['mean']:.4f} & {row['std']:.4f} & Â±{row['ci_95']:.4f} \\\\\\\\\\n\"\n",
    "\n",
    "latex_table1 += r'''\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "'''\n",
    "\n",
    "print(latex_table1)\n",
    "\n",
    "# Save to file\n",
    "with open(RESULTS_DIR / 'latex_table1_results.tex', 'w') as f:\n",
    "    f.write(latex_table1)\n",
    "print('\\nâœ… Saved: results/latex_table1_results.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909cab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: Statistical Tests Table\n",
    "print('='*70)\n",
    "print('TABLE 2: Statistical Significance Tests')\n",
    "print('='*70)\n",
    "\n",
    "latex_table2 = r'''\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Statistical Significance Tests}\n",
    "\\label{tab:statistics}\n",
    "\\begin{tabular}{lcccc}\n",
    "\\toprule\n",
    "\\textbf{Comparison} & \\textbf{t-statistic} & \\textbf{p-value} & \\textbf{Cohen's d} & \\textbf{Effect} \\\\\n",
    "\\midrule\n",
    "'''\n",
    "\n",
    "for _, row in df_stats.iterrows():\n",
    "    cohens_d_val = row[\"Cohen's d\"]\n",
    "    latex_table2 += f\"{row['Comparison']} & {row['t-statistic']} & {row['p-value']} & {cohens_d_val} & {row['Effect Size']} \\\\\\\\\\n\"\n",
    "\n",
    "latex_table2 += r'''\\bottomrule\n",
    "\\end{tabular}\n",
    "\\vspace{2mm}\n",
    "\\footnotesize{Significance levels: *** $p < 0.001$, ** $p < 0.01$, * $p < 0.05$, ns = not significant}\n",
    "\\end{table}\n",
    "'''\n",
    "\n",
    "print(latex_table2)\n",
    "\n",
    "with open(RESULTS_DIR / 'latex_table2_statistics.tex', 'w') as f:\n",
    "    f.write(latex_table2)\n",
    "print('\\nâœ… Saved: results/latex_table2_statistics.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d192dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 3: Detailed Results (All Runs)\n",
    "print('='*70)\n",
    "print('TABLE 3: Detailed Results (All Training Runs)')\n",
    "print('='*70)\n",
    "\n",
    "latex_table3 = r'''\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Detailed Results Across All Seeds}\n",
    "\\label{tab:detailed_results}\n",
    "\\begin{tabular}{lllcc}\n",
    "\\toprule\n",
    "\\textbf{Run ID} & \\textbf{Method} & \\textbf{Task} & \\textbf{Seed} & \\textbf{Eval Loss} \\\\\n",
    "\\midrule\n",
    "'''\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    latex_table3 += f\"{row['id']} & {row['method']} & {row['task'].upper()} & {row['seed']} & {row['eval_loss']:.4f} \\\\\\\\\\n\"\n",
    "\n",
    "latex_table3 += r'''\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "'''\n",
    "\n",
    "print(latex_table3)\n",
    "\n",
    "with open(RESULTS_DIR / 'latex_table3_detailed.tex', 'w') as f:\n",
    "    f.write(latex_table3)\n",
    "print('\\nâœ… Saved: results/latex_table3_detailed.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 4: Latency Results\n",
    "print('='*70)\n",
    "print('TABLE 4: Inference Latency Results')\n",
    "print('='*70)\n",
    "\n",
    "latex_table4 = r'''\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Inference Latency by Method (NVIDIA H100)}\n",
    "\\label{tab:latency}\n",
    "\\begin{tabular}{lcc}\n",
    "\\toprule\n",
    "\\textbf{Method} & \\textbf{Mean Latency (ms)} & \\textbf{Std (ms)} \\\\\n",
    "\\midrule\n",
    "'''\n",
    "\n",
    "for _, row in lat_summary.iterrows():\n",
    "    latex_table4 += f\"{row['method']} & {row['mean']:.2f} & {row['std']:.2f} \\\\\\\\\\n\"\n",
    "\n",
    "latex_table4 += r'''\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "'''\n",
    "\n",
    "print(latex_table4)\n",
    "\n",
    "with open(RESULTS_DIR / 'latex_table4_latency.tex', 'w') as f:\n",
    "    f.write(latex_table4)\n",
    "print('\\nâœ… Saved: results/latex_table4_latency.tex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325075da",
   "metadata": {},
   "source": [
    "## 5. Thesis Narrative Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate thesis-ready paragraph\n",
    "b0_squad_mean = b0_squad.mean()\n",
    "kd2_squad_mean = kd2_squad.mean()\n",
    "diff_pct = (kd2_squad_mean - b0_squad_mean) / b0_squad_mean * 100\n",
    "\n",
    "narrative = f'''\n",
    "================================================================================\n",
    "THESIS NARRATIVE (Copy-paste ready)\n",
    "================================================================================\n",
    "\n",
    "## Results Section Text:\n",
    "\n",
    "We evaluated two training approaches on a TinyLlama-1.1B student model:\n",
    "baseline fine-tuning (B0) and sequence-level knowledge distillation (KD2)\n",
    "using a Qwen-2.5-3B teacher.\n",
    "\n",
    "**Main Finding:** Contrary to expectations, the baseline approach (B0) achieved\n",
    "lower evaluation loss ({b0_squad_mean:.4f} Â± {b0_squad.std():.4f}) compared to\n",
    "KD2 ({kd2_squad_mean:.4f} Â± {kd2_squad.std():.4f}) on the SQuAD question-answering\n",
    "task. This represents a {abs(diff_pct):.1f}% {'increase' if diff_pct > 0 else 'decrease'}\n",
    "in loss for the distillation method.\n",
    "\n",
    "Statistical analysis revealed {'a significant difference' if p_value < 0.05 else 'no statistically significant difference'}\n",
    "between methods (t = {t_stat:.3f}, p = {p_value:.4f}, Cohen's d = {d:.3f}, {interpret_cohens_d(d)} effect).\n",
    "\n",
    "**Interpretation:** These results suggest that sequence-level knowledge\n",
    "distillation may not universally outperform direct fine-tuning, particularly\n",
    "when (1) the student model has sufficient capacity to learn from gold labels,\n",
    "(2) the teacher-generated pseudo-labels introduce distribution shift, or\n",
    "(3) the task does not require complex reasoning beyond pattern matching.\n",
    "\n",
    "**Task Comparison:** SST-2 sentiment classification achieved substantially\n",
    "lower loss ({b0_sst2.mean():.4f}) compared to SQuAD ({b0_squad_mean:.4f}),\n",
    "reflecting the relative complexity of question-answering versus binary\n",
    "classification (t = {t_stat2:.3f}, p = {p_value2:.4f}, {interpret_cohens_d(d2)} effect size).\n",
    "\n",
    "================================================================================\n",
    "'''\n",
    "\n",
    "print(narrative)\n",
    "\n",
    "with open(RESULTS_DIR / 'thesis_narrative.txt', 'w') as f:\n",
    "    f.write(narrative)\n",
    "print('âœ… Saved: results/thesis_narrative.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94197152",
   "metadata": {},
   "source": [
    "## 6. Export Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc59d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('EXPORT SUMMARY')\n",
    "print('='*70)\n",
    "\n",
    "print('\\nðŸ“Š DATA FILES:')\n",
    "print(f'   â€¢ {RUNS_DIR}/results.csv')\n",
    "print(f'   â€¢ {RUNS_DIR}/benchmarks.csv')\n",
    "\n",
    "print('\\nðŸ“ˆ FIGURES (PNG + PDF):')\n",
    "for f in sorted(FIGS_DIR.glob('thesis_*.png')):\n",
    "    print(f'   â€¢ {f.name}')\n",
    "\n",
    "print('\\nðŸ“ LATEX TABLES:')\n",
    "for f in sorted(RESULTS_DIR.glob('latex_*.tex')):\n",
    "    print(f'   â€¢ {f.name}')\n",
    "\n",
    "print('\\nðŸ“– NARRATIVE:')\n",
    "print(f'   â€¢ results/thesis_narrative.txt')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('âœ… THESIS ANALYSIS COMPLETE!')\n",
    "print('='*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
