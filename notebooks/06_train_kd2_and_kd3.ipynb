{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "461b80ca",
   "metadata": {},
   "source": [
    "# 06 - Training: KD2 (Sequence-level) and KD3 (Feature-based)\n",
    "\n",
    "**Thesis Section Reference:** Chapter 4.3-4.4 - Sequence-level and Feature-based KD\n",
    "\n",
    "This notebook trains:\n",
    "1. **KD2 (Sequence-level):** Student learns from teacher-generated sequences\n",
    "2. **KD3 (Feature-based):** Student matches teacher's hidden representations\n",
    "\n",
    "## Grid Search\n",
    "- Lambda λ ∈ {0.1, 0.5, 1.0}\n",
    "\n",
    "## Notes\n",
    "- KD2 is particularly useful for QA (SQuAD)\n",
    "- KD3 uses layer mapping between teacher and student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8024ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard setup\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ROOT_DIR = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(ROOT_DIR / \"src\"))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(ROOT_DIR / \".env\")\n",
    "\n",
    "from config import load_config\n",
    "from utils_seed import set_seed\n",
    "from run_io import RunRegistry\n",
    "\n",
    "config = load_config(str(ROOT_DIR / \"configs\" / \"experiment.yaml\"))\n",
    "config.ensure_dirs()\n",
    "\n",
    "# Device\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Mode: {'FAST' if config.fast_mode else 'FULL'}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a816c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths\n",
    "DATA_DIR = ROOT_DIR / \"results\" / \"processed_data\"\n",
    "CACHE_DIR = ROOT_DIR / \"results\" / \"teacher_cache\"\n",
    "MODELS_DIR = ROOT_DIR / \"results\" / \"models\"\n",
    "RUNS_DIR = ROOT_DIR / \"results\" / \"raw_runs\"\n",
    "\n",
    "registry = RunRegistry(RUNS_DIR / \"run_registry.json\")\n",
    "\n",
    "student_name = os.getenv(\"STUDENT_S1\", config.student_s1.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c91cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed datasets\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "sst2_train = load_from_disk(str(DATA_DIR / \"sst2_train\"))\n",
    "sst2_val = load_from_disk(str(DATA_DIR / \"sst2_validation\"))\n",
    "\n",
    "squad_train = load_from_disk(str(DATA_DIR / \"squad_train\"))\n",
    "squad_val = load_from_disk(str(DATA_DIR / \"squad_validation\"))\n",
    "\n",
    "print(f\"SST-2: {len(sst2_train)} train, {len(sst2_val)} val\")\n",
    "print(f\"SQuAD: {len(squad_train)} train, {len(squad_val)} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98363845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load teacher outputs for KD2 and KD3\n",
    "print(\"Loading cached teacher outputs...\")\n",
    "\n",
    "# KD2: Teacher-generated answers for SQuAD\n",
    "with open(CACHE_DIR / \"squad_teacher_answers.json\", \"r\") as f:\n",
    "    teacher_answers = json.load(f)\n",
    "print(f\"  Loaded {len(teacher_answers)} teacher answers for KD2\")\n",
    "\n",
    "# KD3: Hidden states\n",
    "hidden_state_files = sorted(CACHE_DIR.glob(\"hidden_states_*.pt\"))\n",
    "if hidden_state_files:\n",
    "    with open(CACHE_DIR / \"hidden_states_sst2_meta.json\", \"r\") as f:\n",
    "        hidden_state_meta = json.load(f)\n",
    "    print(f\"  Loaded hidden state metadata: {hidden_state_meta['num_chunks']} chunks\")\n",
    "else:\n",
    "    print(\"  No hidden states found - KD3 will be skipped\")\n",
    "    hidden_state_meta = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3cada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training utilities (same as notebook 05)\n",
    "from transformers import TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "def get_training_args(output_dir, task, run_name):\n",
    "    \"\"\"Get training arguments optimized for MPS.\"\"\"\n",
    "    if task == \"sst2\":\n",
    "        per_device_batch = 4 if DEVICE.type == \"mps\" else 8\n",
    "        grad_accum = 4 if DEVICE.type == \"mps\" else 2\n",
    "    else:\n",
    "        per_device_batch = 2 if DEVICE.type == \"mps\" else 4\n",
    "        grad_accum = 8 if DEVICE.type == \"mps\" else 4\n",
    "    \n",
    "    num_epochs = config.training.epochs_fast if config.fast_mode else config.training.epochs_full\n",
    "    \n",
    "    return TrainingArguments(\n",
    "        output_dir=str(output_dir),\n",
    "        run_name=run_name,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=per_device_batch,\n",
    "        per_device_eval_batch_size=per_device_batch,\n",
    "        gradient_accumulation_steps=grad_accum,\n",
    "        learning_rate=config.training.learning_rate,\n",
    "        weight_decay=config.training.weight_decay,\n",
    "        warmup_ratio=config.training.warmup_ratio,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        dataloader_pin_memory=False if DEVICE.type == \"mps\" else True,\n",
    "        dataloader_num_workers=0 if DEVICE.type == \"mps\" else 4,\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        gradient_checkpointing=True,\n",
    "        seed=config.get_seeds()[0],\n",
    "        data_seed=config.get_seeds()[0],\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "def get_lora_config():\n",
    "    return LoraConfig(\n",
    "        r=config.training.lora_r,\n",
    "        lora_alpha=config.training.lora_alpha,\n",
    "        lora_dropout=config.training.lora_dropout,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "\n",
    "def load_student_model(student_name, use_lora=True):\n",
    "    \"\"\"Load student model with optional LoRA.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        student_name,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        student_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float32,\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\"),\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    if use_lora:\n",
    "        model = get_peft_model(model, get_lora_config())\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    return model, tokenizer\n",
    "\n",
    "print(\"Training utilities ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f5c81",
   "metadata": {},
   "source": [
    "## Section 1: KD2 (Sequence-level KD)\n",
    "\n",
    "Student learns from teacher-generated sequences.\n",
    "The student is trained on (prompt, teacher_answer) pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f51a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare KD2 dataset (using teacher answers as targets)\n",
    "from data_squad import create_squad_prompt\n",
    "\n",
    "def create_kd2_dataset(teacher_answers, tokenizer, max_length=512):\n",
    "    \"\"\"Create dataset with teacher answers as targets.\"\"\"\n",
    "    \n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for item in teacher_answers:\n",
    "        # Full sequence: prompt + teacher_answer\n",
    "        prompt = item[\"prompt\"]\n",
    "        answer = item[\"teacher_answer\"]\n",
    "        full_text = f\"{prompt}\\nAnswer: {answer}\"\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer(\n",
    "            full_text,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded[\"input_ids\"].squeeze()\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze()\n",
    "        \n",
    "        # Labels: mask prompt, only predict answer\n",
    "        prompt_encoded = tokenizer(\n",
    "            f\"{prompt}\\nAnswer: \",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        prompt_len = prompt_encoded[\"input_ids\"].shape[1]\n",
    "        \n",
    "        labels = input_ids.clone()\n",
    "        labels[:prompt_len] = -100  # Mask prompt\n",
    "        \n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        labels_list.append(labels)\n",
    "    \n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_mask_list,\n",
    "        \"labels\": labels_list,\n",
    "    })\n",
    "\n",
    "print(\"KD2 data preparation function ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eea4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KD2 models\n",
    "from trainers import SequenceKDTrainer\n",
    "from kd_losses import SequenceKDLoss\n",
    "from data_squad import compute_squad_metrics\n",
    "\n",
    "# Lambda grid\n",
    "lambdas = config.kd2.lambdas if hasattr(config, 'kd2') else [0.1, 0.5, 1.0]\n",
    "\n",
    "if config.fast_mode:\n",
    "    lambdas = [0.5, 1.0]\n",
    "\n",
    "print(f\"KD2 Grid: λ={lambdas}\")\n",
    "\n",
    "kd2_results = []\n",
    "\n",
    "for lambda_val in lambdas:\n",
    "    seed = config.get_seeds()[0]\n",
    "    run_id = f\"KD2_squad_S1_l{lambda_val}_seed{seed}\"\n",
    "    \n",
    "    if registry.check_run(run_id):\n",
    "        print(f\"✓ {run_id} already completed, skipping...\")\n",
    "        existing = registry.get_run(run_id)\n",
    "        kd2_results.append(existing)\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {run_id}\")\n",
    "    print(f\"λ={lambda_val}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Load model\n",
    "    model, tokenizer = load_student_model(student_name, use_lora=True)\n",
    "    \n",
    "    # Create KD2 dataset\n",
    "    print(\"Preparing KD2 dataset...\")\n",
    "    kd2_train = create_kd2_dataset(\n",
    "        teacher_answers,\n",
    "        tokenizer,\n",
    "        max_length=config.get_max_length(\"squad\")\n",
    "    )\n",
    "    \n",
    "    # Create loss\n",
    "    kd_loss_fn = SequenceKDLoss(lambda_=lambda_val)\n",
    "    \n",
    "    # Training args\n",
    "    output_dir = MODELS_DIR / run_id\n",
    "    training_args = get_training_args(output_dir, \"squad\", run_id)\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = SequenceKDTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=kd2_train,\n",
    "        eval_dataset=squad_val,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_squad_metrics,\n",
    "        kd_loss_fn=kd_loss_fn\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_result = trainer.evaluate()\n",
    "    \n",
    "    # Save results\n",
    "    result = {\n",
    "        \"run_id\": run_id,\n",
    "        \"method\": \"KD2\",\n",
    "        \"task\": \"squad\",\n",
    "        \"student\": \"S1\",\n",
    "        \"seed\": seed,\n",
    "        \"lambda\": lambda_val,\n",
    "        \"train_loss\": train_result.training_loss,\n",
    "        \"eval_loss\": eval_result[\"eval_loss\"],\n",
    "        **{k: v for k, v in eval_result.items() if k != \"eval_loss\"}\n",
    "    }\n",
    "    \n",
    "    trainer.save_model(str(output_dir / \"final\"))\n",
    "    registry.register_run(run_id, result)\n",
    "    kd2_results.append(result)\n",
    "    \n",
    "    print(f\"\\n✓ {run_id} complete\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, trainer, kd_loss_fn, kd2_train\n",
    "    if DEVICE.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\n✓ KD2 training complete: {len(kd2_results)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5942439",
   "metadata": {},
   "source": [
    "## Section 2: KD3 (Feature-based KD)\n",
    "\n",
    "Student learns to match teacher's hidden representations.\n",
    "Uses layer mapping to align teacher and student layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e914981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hidden states for KD3\n",
    "if hidden_state_meta is not None:\n",
    "    print(\"Loading hidden states...\")\n",
    "    \n",
    "    hidden_states_list = []\n",
    "    for i in range(hidden_state_meta[\"num_chunks\"]):\n",
    "        chunk = torch.load(CACHE_DIR / f\"hidden_states_sst2_{i}.pt\")\n",
    "        hidden_states_list.append(chunk)\n",
    "    \n",
    "    # Concatenate all chunks\n",
    "    teacher_hidden_states = torch.cat(hidden_states_list, dim=0)\n",
    "    print(f\"  Total hidden states: {teacher_hidden_states.shape}\")\n",
    "    \n",
    "    # Layer mapping info\n",
    "    print(f\"  Teacher layers cached: {hidden_state_meta['selected_layers']}\")\n",
    "    print(f\"  Hidden size: {hidden_state_meta['hidden_size']}\")\n",
    "else:\n",
    "    print(\"No hidden states available - skipping KD3\")\n",
    "    teacher_hidden_states = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983682a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KD3 models\n",
    "from trainers import FeatureKDTrainer\n",
    "from kd_losses import FeatureMatchingLoss\n",
    "from data_sst2 import compute_sst2_metrics\n",
    "\n",
    "kd3_results = []\n",
    "\n",
    "if teacher_hidden_states is not None:\n",
    "    # Lambda grid\n",
    "    lambdas = config.kd3.lambdas if hasattr(config, 'kd3') else [0.1, 0.5, 1.0]\n",
    "    \n",
    "    if config.fast_mode:\n",
    "        lambdas = [0.5, 1.0]\n",
    "    \n",
    "    print(f\"KD3 Grid: λ={lambdas}\")\n",
    "    \n",
    "    for lambda_val in lambdas:\n",
    "        seed = config.get_seeds()[0]\n",
    "        run_id = f\"KD3_sst2_S1_l{lambda_val}_seed{seed}\"\n",
    "        \n",
    "        if registry.check_run(run_id):\n",
    "            print(f\"✓ {run_id} already completed, skipping...\")\n",
    "            existing = registry.get_run(run_id)\n",
    "            kd3_results.append(existing)\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training: {run_id}\")\n",
    "        print(f\"λ={lambda_val}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        set_seed(seed)\n",
    "        \n",
    "        # Load model\n",
    "        model, tokenizer = load_student_model(student_name, use_lora=True)\n",
    "        \n",
    "        # Get student layer count for mapping\n",
    "        student_layers = model.config.num_hidden_layers if hasattr(model, 'config') else 22\n",
    "        teacher_cached_layers = hidden_state_meta['selected_layers']\n",
    "        \n",
    "        # Create layer mapping: evenly distribute\n",
    "        layer_mapping = {}\n",
    "        student_interval = student_layers // len(teacher_cached_layers)\n",
    "        for i, teacher_layer in enumerate(teacher_cached_layers):\n",
    "            student_layer = min(i * student_interval, student_layers - 1)\n",
    "            layer_mapping[teacher_layer] = student_layer\n",
    "        \n",
    "        print(f\"  Layer mapping: {layer_mapping}\")\n",
    "        \n",
    "        # Create loss\n",
    "        kd_loss_fn = FeatureMatchingLoss(\n",
    "            lambda_=lambda_val,\n",
    "            teacher_hidden_size=hidden_state_meta['hidden_size'],\n",
    "            student_hidden_size=model.config.hidden_size if hasattr(model.config, 'hidden_size') else 2048\n",
    "        )\n",
    "        \n",
    "        # Training args\n",
    "        output_dir = MODELS_DIR / run_id\n",
    "        training_args = get_training_args(output_dir, \"sst2\", run_id)\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = FeatureKDTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=sst2_train,\n",
    "            eval_dataset=sst2_val,\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_sst2_metrics,\n",
    "            kd_loss_fn=kd_loss_fn,\n",
    "            teacher_hidden_states=teacher_hidden_states,\n",
    "            layer_mapping=layer_mapping\n",
    "        )\n",
    "        \n",
    "        # Train\n",
    "        train_result = trainer.train()\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = trainer.evaluate()\n",
    "        \n",
    "        # Save results\n",
    "        result = {\n",
    "            \"run_id\": run_id,\n",
    "            \"method\": \"KD3\",\n",
    "            \"task\": \"sst2\",\n",
    "            \"student\": \"S1\",\n",
    "            \"seed\": seed,\n",
    "            \"lambda\": lambda_val,\n",
    "            \"train_loss\": train_result.training_loss,\n",
    "            \"eval_loss\": eval_result[\"eval_loss\"],\n",
    "            **{k: v for k, v in eval_result.items() if k != \"eval_loss\"}\n",
    "        }\n",
    "        \n",
    "        trainer.save_model(str(output_dir / \"final\"))\n",
    "        registry.register_run(run_id, result)\n",
    "        kd3_results.append(result)\n",
    "        \n",
    "        print(f\"\\n✓ {run_id} complete\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del model, trainer, kd_loss_fn\n",
    "        if DEVICE.type == \"mps\":\n",
    "            torch.mps.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"\\n✓ KD3 training complete: {len(kd3_results)} runs\")\n",
    "else:\n",
    "    print(\"Skipping KD3 (no hidden states available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best configurations\n",
    "print(\"\\nBest Configurations:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "best_configs = {}\n",
    "\n",
    "# Best KD2\n",
    "if kd2_results:\n",
    "    df_kd2 = pd.DataFrame(kd2_results)\n",
    "    best_kd2_idx = df_kd2[\"eval_loss\"].idxmin()\n",
    "    best_kd2 = df_kd2.loc[best_kd2_idx]\n",
    "    print(f\"KD2 Best: λ={best_kd2['lambda']}, loss={best_kd2['eval_loss']:.4f}\")\n",
    "    best_configs[\"kd2\"] = {\"lambda\": best_kd2[\"lambda\"]}\n",
    "\n",
    "# Best KD3\n",
    "if kd3_results:\n",
    "    df_kd3 = pd.DataFrame(kd3_results)\n",
    "    best_kd3_idx = df_kd3[\"eval_loss\"].idxmin()\n",
    "    best_kd3 = df_kd3.loc[best_kd3_idx]\n",
    "    print(f\"KD3 Best: λ={best_kd3['lambda']}, loss={best_kd3['eval_loss']:.4f}\")\n",
    "    best_configs[\"kd3\"] = {\"lambda\": best_kd3[\"lambda\"]}\n",
    "\n",
    "# Save best configs\n",
    "with open(RUNS_DIR / \"best_kd2_kd3_config.json\", \"w\") as f:\n",
    "    json.dump(best_configs, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best configs across all seeds\n",
    "if len(config.get_seeds()) > 1:\n",
    "    print(\"\\nTraining best configs across all seeds...\")\n",
    "    \n",
    "    for seed in config.get_seeds()[1:]:\n",
    "        # Best KD2\n",
    "        if \"kd2\" in best_configs:\n",
    "            lambda_val = best_configs[\"kd2\"][\"lambda\"]\n",
    "            run_id = f\"KD2_squad_S1_l{lambda_val}_seed{seed}\"\n",
    "            \n",
    "            if not registry.check_run(run_id):\n",
    "                print(f\"Training: {run_id}\")\n",
    "                set_seed(seed)\n",
    "                \n",
    "                model, tokenizer = load_student_model(student_name, use_lora=True)\n",
    "                kd2_train = create_kd2_dataset(\n",
    "                    teacher_answers,\n",
    "                    tokenizer,\n",
    "                    max_length=config.get_max_length(\"squad\")\n",
    "                )\n",
    "                kd_loss_fn = SequenceKDLoss(lambda_=lambda_val)\n",
    "                \n",
    "                output_dir = MODELS_DIR / run_id\n",
    "                training_args = get_training_args(output_dir, \"squad\", run_id)\n",
    "                \n",
    "                trainer = SequenceKDTrainer(\n",
    "                    model=model,\n",
    "                    args=training_args,\n",
    "                    train_dataset=kd2_train,\n",
    "                    eval_dataset=squad_val,\n",
    "                    tokenizer=tokenizer,\n",
    "                    compute_metrics=compute_squad_metrics,\n",
    "                    kd_loss_fn=kd_loss_fn\n",
    "                )\n",
    "                \n",
    "                train_result = trainer.train()\n",
    "                eval_result = trainer.evaluate()\n",
    "                \n",
    "                result = {\n",
    "                    \"run_id\": run_id,\n",
    "                    \"method\": \"KD2\",\n",
    "                    \"task\": \"squad\",\n",
    "                    \"student\": \"S1\",\n",
    "                    \"seed\": seed,\n",
    "                    \"lambda\": lambda_val,\n",
    "                    \"train_loss\": train_result.training_loss,\n",
    "                    \"eval_loss\": eval_result[\"eval_loss\"],\n",
    "                    **{k: v for k, v in eval_result.items() if k != \"eval_loss\"}\n",
    "                }\n",
    "                \n",
    "                trainer.save_model(str(output_dir / \"final\"))\n",
    "                registry.register_run(run_id, result)\n",
    "                kd2_results.append(result)\n",
    "                \n",
    "                del model, trainer, kd_loss_fn, kd2_train\n",
    "                if DEVICE.type == \"mps\":\n",
    "                    torch.mps.empty_cache()\n",
    "                gc.collect()\n",
    "    \n",
    "    print(\"✓ All seeds trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "all_results = kd2_results + kd3_results\n",
    "\n",
    "df_all = pd.DataFrame(all_results)\n",
    "df_all.to_csv(RUNS_DIR / \"nb06_results.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(all_results)} results to {RUNS_DIR / 'nb06_results.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afa4345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"KD2 AND KD3 TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Mode: {'FAST' if config.fast_mode else 'FULL'}\n",
    "Student: {student_name}\n",
    "\n",
    "Runs Completed:\n",
    "  KD2 Sequence-level: {len(kd2_results)} runs\n",
    "  KD3 Feature-based: {len(kd3_results)} runs\n",
    "\n",
    "Results saved to: {RUNS_DIR / 'nb06_results.csv'}\n",
    "Models saved to: {MODELS_DIR}\n",
    "\n",
    "Next Steps:\n",
    "  1. Run 07_benchmark_and_plots.ipynb for efficiency benchmarks\n",
    "  2. Generate thesis figures and tables\n",
    "\"\"\")\n",
    "\n",
    "# Comparison table\n",
    "if all_results:\n",
    "    print(\"\\nResults Summary:\")\n",
    "    df = pd.DataFrame(all_results)\n",
    "    print(df[[\"run_id\", \"method\", \"lambda\", \"eval_loss\"]].to_string())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
