{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8cce9e1",
   "metadata": {},
   "source": [
    "# 03 - Data Preparation: SQuAD v1.1\n",
    "\n",
    "**Thesis Section Reference:** Chapter 3.6 - Tasks and Datasets\n",
    "\n",
    "This notebook prepares the SQuAD v1.1 extractive QA dataset:\n",
    "1. Load SQuAD v1.1 dataset\n",
    "2. Create subsets for FAST MODE\n",
    "3. Tokenize for causal LM training (generative QA)\n",
    "4. Save processed datasets\n",
    "\n",
    "## Task Description\n",
    "- **Dataset:** SQuAD v1.1 (Stanford Question Answering Dataset)\n",
    "- **Task:** Extractive Question Answering\n",
    "- **Metrics:** Exact Match (EM), F1\n",
    "- **Note:** Test set is hidden, so validation is used as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a3327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(ROOT_DIR / \"src\"))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(ROOT_DIR / \".env\")\n",
    "\n",
    "from config import load_config\n",
    "from utils_seed import set_seed\n",
    "\n",
    "config = load_config(str(ROOT_DIR / \"configs\" / \"experiment.yaml\"))\n",
    "config.ensure_dirs()\n",
    "\n",
    "SEED = config.get_seeds()[0]\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"Mode: {'FAST' if config.fast_mode else 'FULL'}\")\n",
    "print(f\"Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa81508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data already exists\n",
    "DATA_DIR = ROOT_DIR / \"results\" / \"processed_data\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "squad_train_path = DATA_DIR / \"squad_train\"\n",
    "squad_val_path = DATA_DIR / \"squad_validation\"\n",
    "\n",
    "if squad_train_path.exists() and squad_val_path.exists():\n",
    "    print(\"✓ SQuAD data already exists, loading from cache...\")\n",
    "    SKIP_PROCESSING = True\n",
    "else:\n",
    "    print(\"SQuAD data not found, will process...\")\n",
    "    SKIP_PROCESSING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e8e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SQuAD dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Loading SQuAD v1.1...\")\n",
    "    \n",
    "    raw_dataset = load_dataset(\n",
    "        \"squad\",\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDataset structure:\")\n",
    "    print(raw_dataset)\n",
    "    \n",
    "    print(f\"\\nSample example:\")\n",
    "    ex = raw_dataset[\"train\"][0]\n",
    "    print(f\"  ID: {ex['id']}\")\n",
    "    print(f\"  Question: {ex['question']}\")\n",
    "    print(f\"  Context: {ex['context'][:200]}...\")\n",
    "    print(f\"  Answers: {ex['answers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b579e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subsets based on mode\n",
    "if not SKIP_PROCESSING:\n",
    "    train_size = config.get_subset_size(\"squad\", \"train\")\n",
    "    val_size = config.get_subset_size(\"squad\", \"validation\")\n",
    "    \n",
    "    if train_size is not None:\n",
    "        print(f\"FAST MODE: Subsetting to {train_size} train, {val_size} validation examples\")\n",
    "        \n",
    "        train_dataset = raw_dataset[\"train\"].shuffle(seed=SEED).select(range(train_size))\n",
    "        val_dataset = raw_dataset[\"validation\"].shuffle(seed=SEED).select(range(min(val_size, len(raw_dataset[\"validation\"]))))\n",
    "    else:\n",
    "        print(\"FULL MODE: Using complete dataset\")\n",
    "        train_dataset = raw_dataset[\"train\"]\n",
    "        val_dataset = raw_dataset[\"validation\"]\n",
    "    \n",
    "    print(f\"\\nFinal sizes:\")\n",
    "    print(f\"  Train: {len(train_dataset)}\")\n",
    "    print(f\"  Validation: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4b4563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    tokenizer_name = os.getenv(\"STUDENT_S1\", config.student_s1.name)\n",
    "    \n",
    "    print(f\"Loading tokenizer: {tokenizer_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    print(f\"  Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e164d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template for generative QA\n",
    "from data_squad import create_squad_prompt\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    max_length = config.get_max_length(\"squad\")\n",
    "    print(f\"Max sequence length: {max_length}\")\n",
    "    \n",
    "    # Show example prompt\n",
    "    example = train_dataset[0]\n",
    "    example_prompt = create_squad_prompt(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        include_answer=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nExample prompt (truncated):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(example_prompt[:500])\n",
    "    print(\"...\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"\\nExpected answer: {example['answers']['text'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfb9cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "from data_squad import tokenize_squad_for_lm\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Tokenizing datasets...\")\n",
    "    \n",
    "    def tokenize_fn(examples):\n",
    "        return tokenize_squad_for_lm(\n",
    "            examples,\n",
    "            tokenizer,\n",
    "            max_length=max_length,\n",
    "            include_labels=True\n",
    "        )\n",
    "    \n",
    "    # Tokenize train\n",
    "    print(\"  Tokenizing train split...\")\n",
    "    tokenized_train = train_dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=[\"title\", \"context\", \"question\", \"answers\"],\n",
    "        desc=\"Tokenizing train\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize validation\n",
    "    print(\"  Tokenizing validation split...\")\n",
    "    tokenized_val = val_dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=[\"title\", \"context\", \"question\", \"answers\"],\n",
    "        desc=\"Tokenizing validation\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTokenized dataset features:\")\n",
    "    print(f\"  {tokenized_train.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58f714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify tokenization\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Verifying tokenization...\")\n",
    "    \n",
    "    sample = tokenized_train[0]\n",
    "    \n",
    "    # Decode input\n",
    "    decoded = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n",
    "    print(f\"\\nSample decoded (truncated):\")\n",
    "    print(decoded[:400])\n",
    "    print(\"...\")\n",
    "    \n",
    "    # Check gold answers are preserved\n",
    "    if \"gold_answers\" in sample:\n",
    "        print(f\"\\nGold answers: {sample['gold_answers']}\")\n",
    "    \n",
    "    # Check sequence length distribution\n",
    "    lengths = [len([t for t in ex[\"input_ids\"] if t != tokenizer.pad_token_id]) \n",
    "               for ex in tokenized_train.select(range(min(100, len(tokenized_train))))]\n",
    "    \n",
    "    print(f\"\\nSequence length stats (first 100):\")\n",
    "    print(f\"  Mean: {sum(lengths)/len(lengths):.1f}\")\n",
    "    print(f\"  Max: {max(lengths)}\")\n",
    "    print(f\"  Min: {min(lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b837e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw examples for KD2 (sequence-level KD needs prompts without answers)\n",
    "import json\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Saving raw examples for KD2 (sequence-level KD)...\")\n",
    "    \n",
    "    # Create prompts without answers for teacher generation\n",
    "    train_prompts = []\n",
    "    for i, ex in enumerate(train_dataset):\n",
    "        prompt = create_squad_prompt(\n",
    "            ex[\"question\"],\n",
    "            ex[\"context\"],\n",
    "            include_answer=False\n",
    "        )\n",
    "        train_prompts.append({\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"prompt\": prompt,\n",
    "            \"gold_answers\": ex[\"answers\"][\"text\"]\n",
    "        })\n",
    "    \n",
    "    val_prompts = []\n",
    "    for i, ex in enumerate(val_dataset):\n",
    "        prompt = create_squad_prompt(\n",
    "            ex[\"question\"],\n",
    "            ex[\"context\"],\n",
    "            include_answer=False\n",
    "        )\n",
    "        val_prompts.append({\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"prompt\": prompt,\n",
    "            \"gold_answers\": ex[\"answers\"][\"text\"]\n",
    "        })\n",
    "    \n",
    "    # Save prompts\n",
    "    with open(DATA_DIR / \"squad_train_prompts.json\", \"w\") as f:\n",
    "        json.dump(train_prompts, f)\n",
    "    \n",
    "    with open(DATA_DIR / \"squad_val_prompts.json\", \"w\") as f:\n",
    "        json.dump(val_prompts, f)\n",
    "    \n",
    "    print(f\"  Saved {len(train_prompts)} train prompts\")\n",
    "    print(f\"  Saved {len(val_prompts)} validation prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa555f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Saving processed datasets...\")\n",
    "    \n",
    "    tokenized_train.save_to_disk(str(squad_train_path))\n",
    "    tokenized_val.save_to_disk(str(squad_val_path))\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer_path = DATA_DIR / \"squad_tokenizer\"\n",
    "    tokenizer.save_pretrained(str(tokenizer_path))\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"task\": \"squad\",\n",
    "        \"train_size\": len(tokenized_train),\n",
    "        \"val_size\": len(tokenized_val),\n",
    "        \"max_length\": max_length,\n",
    "        \"tokenizer\": tokenizer_name,\n",
    "        \"fast_mode\": config.fast_mode,\n",
    "        \"seed\": SEED,\n",
    "        \"use_validation_as_test\": True\n",
    "    }\n",
    "    \n",
    "    with open(DATA_DIR / \"squad_metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Saved to {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e1b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cached data if skipped\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "\n",
    "if SKIP_PROCESSING:\n",
    "    print(\"Loading cached SQuAD data...\")\n",
    "    tokenized_train = load_from_disk(str(squad_train_path))\n",
    "    tokenized_val = load_from_disk(str(squad_val_path))\n",
    "    \n",
    "    with open(DATA_DIR / \"squad_metadata.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"\\nLoaded from cache:\")\n",
    "    print(f\"  Train: {len(tokenized_train)} examples\")\n",
    "    print(f\"  Validation: {len(tokenized_val)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da108f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"SQUAD DATA PREPARATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Dataset: SQuAD v1.1 (Extractive QA)\n",
    "Mode: {'FAST' if config.fast_mode else 'FULL'}\n",
    "\n",
    "Sizes:\n",
    "  Train: {len(tokenized_train)} examples\n",
    "  Validation: {len(tokenized_val)} examples\n",
    "\n",
    "Files saved to: {DATA_DIR}\n",
    "  - squad_train/\n",
    "  - squad_validation/\n",
    "  - squad_train_prompts.json (for KD2)\n",
    "  - squad_val_prompts.json (for KD2)\n",
    "\n",
    "Next Steps:\n",
    "  1. Run 04_teacher_cache_outputs.ipynb to cache teacher outputs\n",
    "  2. Run 05_train_baseline_and_kd1.ipynb for training\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
