{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8cce9e1",
   "metadata": {},
   "source": [
    "# 03 - Data Preparation: SQuAD v1.1\n",
    "\n",
    "**Thesis Section Reference:** Chapter 3.6 - Tasks and Datasets\n",
    "\n",
    "This notebook prepares the SQuAD v1.1 extractive QA dataset:\n",
    "1. Load SQuAD v1.1 dataset\n",
    "2. Create subsets for FAST MODE\n",
    "3. Tokenize for causal LM training (generative QA)\n",
    "4. Save processed datasets\n",
    "\n",
    "## Task Description\n",
    "- **Dataset:** SQuAD v1.1 (Stanford Question Answering Dataset)\n",
    "- **Task:** Extractive Question Answering\n",
    "- **Metrics:** Exact Match (EM), F1\n",
    "- **Note:** Test set is hidden, so validation is used as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "957a3327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: FAST\n",
      "Seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Standard setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(ROOT_DIR / \"src\"))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(ROOT_DIR / \".env\")\n",
    "\n",
    "from config import load_config\n",
    "from utils_seed import set_seed\n",
    "\n",
    "config = load_config(str(ROOT_DIR / \"configs\" / \"experiment.yaml\"))\n",
    "config.ensure_dirs()\n",
    "\n",
    "SEED = config.get_seeds()[0]\n",
    "set_seed(SEED)\n",
    "\n",
    "print(f\"Mode: {'FAST' if config.fast_mode else 'FULL'}\")\n",
    "print(f\"Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa81508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD data not found, will process...\n"
     ]
    }
   ],
   "source": [
    "# Check if data already exists\n",
    "DATA_DIR = ROOT_DIR / \"results\" / \"processed_data\"\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "squad_train_path = DATA_DIR / \"squad_train\"\n",
    "squad_val_path = DATA_DIR / \"squad_validation\"\n",
    "\n",
    "if squad_train_path.exists() and squad_val_path.exists():\n",
    "    print(\"✓ SQuAD data already exists, loading from cache...\")\n",
    "    SKIP_PROCESSING = True\n",
    "else:\n",
    "    print(\"SQuAD data not found, will process...\")\n",
    "    SKIP_PROCESSING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d0e8e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SQuAD v1.1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991fa755edca4017bfe5a5e77b44f21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332b9033175f4ae49b2915016667cd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4188198f5f84d699a4c7fed95a4d8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/validation-00000-of-00001.par(…):   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0849b1b79c1e4bd8b2759e3b9f2a8557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f771f5ff4e42558933d6f42233efcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample example:\n",
      "  ID: 5733be284776f41900661182\n",
      "  Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "  Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper sta...\n",
      "  Answers: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
     ]
    }
   ],
   "source": [
    "# Load SQuAD dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Loading SQuAD v1.1...\")\n",
    "    \n",
    "    raw_dataset = load_dataset(\n",
    "        \"squad\",\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDataset structure:\")\n",
    "    print(raw_dataset)\n",
    "    \n",
    "    print(f\"\\nSample example:\")\n",
    "    ex = raw_dataset[\"train\"][0]\n",
    "    print(f\"  ID: {ex['id']}\")\n",
    "    print(f\"  Question: {ex['question']}\")\n",
    "    print(f\"  Context: {ex['context'][:200]}...\")\n",
    "    print(f\"  Answers: {ex['answers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b579e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAST MODE: Subsetting to 2000 train, 500 validation examples\n",
      "\n",
      "Final sizes:\n",
      "  Train: 2000\n",
      "  Validation: 500\n"
     ]
    }
   ],
   "source": [
    "# Create subsets based on mode\n",
    "if not SKIP_PROCESSING:\n",
    "    train_size = config.get_subset_size(\"squad\", \"train\")\n",
    "    val_size = config.get_subset_size(\"squad\", \"validation\")\n",
    "    \n",
    "    if train_size is not None:\n",
    "        print(f\"FAST MODE: Subsetting to {train_size} train, {val_size} validation examples\")\n",
    "        \n",
    "        train_dataset = raw_dataset[\"train\"].shuffle(seed=SEED).select(range(train_size))\n",
    "        val_dataset = raw_dataset[\"validation\"].shuffle(seed=SEED).select(range(min(val_size, len(raw_dataset[\"validation\"]))))\n",
    "    else:\n",
    "        print(\"FULL MODE: Using complete dataset\")\n",
    "        train_dataset = raw_dataset[\"train\"]\n",
    "        val_dataset = raw_dataset[\"validation\"]\n",
    "    \n",
    "    print(f\"\\nFinal sizes:\")\n",
    "    print(f\"  Train: {len(train_dataset)}\")\n",
    "    print(f\"  Validation: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe4b4563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  Vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    tokenizer_name = os.getenv(\"STUDENT_S1\", config.student_s1.name)\n",
    "    \n",
    "    print(f\"Loading tokenizer: {tokenizer_name}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    print(f\"  Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e164d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 512\n",
      "\n",
      "Example prompt (truncated):\n",
      "----------------------------------------\n",
      "Answer the question based on the context below. Give a short, exact answer from the context.\n",
      "\n",
      "Context: The Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require close monitoring due to the nature and extent of violations of religious freedom engaged in or tolerat\n",
      "...\n",
      "----------------------------------------\n",
      "\n",
      "Expected answer: 84%\n"
     ]
    }
   ],
   "source": [
    "# Define prompt template for generative QA\n",
    "from data_squad import create_squad_prompt\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    max_length = config.get_max_length(\"squad\")\n",
    "    print(f\"Max sequence length: {max_length}\")\n",
    "    \n",
    "    # Show example prompt\n",
    "    example = train_dataset[0]\n",
    "    example_prompt = create_squad_prompt(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        include_answer=False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nExample prompt (truncated):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(example_prompt[:500])\n",
    "    print(\"...\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"\\nExpected answer: {example['answers']['text'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dfb9cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n",
      "  Tokenizing train split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed3ee5f258148c7a9fa6d54c81c2b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tokenizing validation split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62164a0301e847f2a1f0df7214d83fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized dataset features:\n",
      "  ['id', 'context', 'question', 'input_ids', 'attention_mask', 'labels', 'gold_answers', 'example_id']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "from data_squad import tokenize_squad_for_lm\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Tokenizing datasets...\")\n",
    "    \n",
    "    def tokenize_fn(examples):\n",
    "        return tokenize_squad_for_lm(\n",
    "            examples,\n",
    "            tokenizer,\n",
    "            max_length=max_length,\n",
    "            include_labels=True\n",
    "        )\n",
    "    \n",
    "    # Tokenize train\n",
    "    print(\"  Tokenizing train split...\")\n",
    "    tokenized_train = train_dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=[\"title\", \"context\", \"question\", \"answers\"],\n",
    "        desc=\"Tokenizing train\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize validation\n",
    "    print(\"  Tokenizing validation split...\")\n",
    "    tokenized_val = val_dataset.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=[\"title\", \"context\", \"question\", \"answers\"],\n",
    "        desc=\"Tokenizing validation\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTokenized dataset features:\")\n",
    "    print(f\"  {tokenized_train.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b58f714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying tokenization...\n",
      "\n",
      "Sample decoded (truncated):\n",
      "Answer the question based on the context below. Give a short, exact answer from the context.\n",
      "\n",
      "Context: The Pew Forum on Religion & Public Life ranks Egypt as the fifth worst country in the world for religious freedom. The United States Commission on International Religious Freedom, a bipartisan independent agency of the US government, has placed Egypt on its watch list of countries that require cl\n",
      "...\n",
      "\n",
      "Gold answers: ['84%']\n",
      "\n",
      "Sequence length stats (first 100):\n",
      "  Mean: 230.9\n",
      "  Max: 512\n",
      "  Min: 93\n"
     ]
    }
   ],
   "source": [
    "# Verify tokenization\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Verifying tokenization...\")\n",
    "    \n",
    "    sample = tokenized_train[0]\n",
    "    \n",
    "    # Decode input\n",
    "    decoded = tokenizer.decode(sample[\"input_ids\"], skip_special_tokens=True)\n",
    "    print(f\"\\nSample decoded (truncated):\")\n",
    "    print(decoded[:400])\n",
    "    print(\"...\")\n",
    "    \n",
    "    # Check gold answers are preserved\n",
    "    if \"gold_answers\" in sample:\n",
    "        print(f\"\\nGold answers: {sample['gold_answers']}\")\n",
    "    \n",
    "    # Check sequence length distribution\n",
    "    lengths = [len([t for t in ex[\"input_ids\"] if t != tokenizer.pad_token_id]) \n",
    "               for ex in tokenized_train.select(range(min(100, len(tokenized_train))))]\n",
    "    \n",
    "    print(f\"\\nSequence length stats (first 100):\")\n",
    "    print(f\"  Mean: {sum(lengths)/len(lengths):.1f}\")\n",
    "    print(f\"  Max: {max(lengths)}\")\n",
    "    print(f\"  Min: {min(lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13b837e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving raw examples for KD2 (sequence-level KD)...\n",
      "  Saved 2000 train prompts\n",
      "  Saved 500 validation prompts\n"
     ]
    }
   ],
   "source": [
    "# Save raw examples for KD2 (sequence-level KD needs prompts without answers)\n",
    "import json\n",
    "\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Saving raw examples for KD2 (sequence-level KD)...\")\n",
    "    \n",
    "    # Create prompts without answers for teacher generation\n",
    "    train_prompts = []\n",
    "    for i, ex in enumerate(train_dataset):\n",
    "        prompt = create_squad_prompt(\n",
    "            ex[\"question\"],\n",
    "            ex[\"context\"],\n",
    "            include_answer=False\n",
    "        )\n",
    "        train_prompts.append({\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"prompt\": prompt,\n",
    "            \"gold_answers\": ex[\"answers\"][\"text\"]\n",
    "        })\n",
    "    \n",
    "    val_prompts = []\n",
    "    for i, ex in enumerate(val_dataset):\n",
    "        prompt = create_squad_prompt(\n",
    "            ex[\"question\"],\n",
    "            ex[\"context\"],\n",
    "            include_answer=False\n",
    "        )\n",
    "        val_prompts.append({\n",
    "            \"id\": ex[\"id\"],\n",
    "            \"prompt\": prompt,\n",
    "            \"gold_answers\": ex[\"answers\"][\"text\"]\n",
    "        })\n",
    "    \n",
    "    # Save prompts\n",
    "    with open(DATA_DIR / \"squad_train_prompts.json\", \"w\") as f:\n",
    "        json.dump(train_prompts, f)\n",
    "    \n",
    "    with open(DATA_DIR / \"squad_val_prompts.json\", \"w\") as f:\n",
    "        json.dump(val_prompts, f)\n",
    "    \n",
    "    print(f\"  Saved {len(train_prompts)} train prompts\")\n",
    "    print(f\"  Saved {len(val_prompts)} validation prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aa555f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5da7cd6ebc4f84b3f1302c4aa0d796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0327444a21949b3a22dada17b1d154d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved to /Users/pjere/Workshop/thesis-exp/results/processed_data\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets\n",
    "if not SKIP_PROCESSING:\n",
    "    print(\"Saving processed datasets...\")\n",
    "    \n",
    "    tokenized_train.save_to_disk(str(squad_train_path))\n",
    "    tokenized_val.save_to_disk(str(squad_val_path))\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer_path = DATA_DIR / \"squad_tokenizer\"\n",
    "    tokenizer.save_pretrained(str(tokenizer_path))\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        \"task\": \"squad\",\n",
    "        \"train_size\": len(tokenized_train),\n",
    "        \"val_size\": len(tokenized_val),\n",
    "        \"max_length\": max_length,\n",
    "        \"tokenizer\": tokenizer_name,\n",
    "        \"fast_mode\": config.fast_mode,\n",
    "        \"seed\": SEED,\n",
    "        \"use_validation_as_test\": True\n",
    "    }\n",
    "    \n",
    "    with open(DATA_DIR / \"squad_metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Saved to {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e1b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cached data if skipped\n",
    "from datasets import load_from_disk\n",
    "import json\n",
    "\n",
    "if SKIP_PROCESSING:\n",
    "    print(\"Loading cached SQuAD data...\")\n",
    "    tokenized_train = load_from_disk(str(squad_train_path))\n",
    "    tokenized_val = load_from_disk(str(squad_val_path))\n",
    "    \n",
    "    with open(DATA_DIR / \"squad_metadata.json\", \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"\\nLoaded from cache:\")\n",
    "    print(f\"  Train: {len(tokenized_train)} examples\")\n",
    "    print(f\"  Validation: {len(tokenized_val)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6da108f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SQUAD DATA PREPARATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Dataset: SQuAD v1.1 (Extractive QA)\n",
      "Mode: FAST\n",
      "\n",
      "Sizes:\n",
      "  Train: 2000 examples\n",
      "  Validation: 500 examples\n",
      "\n",
      "Files saved to: /Users/pjere/Workshop/thesis-exp/results/processed_data\n",
      "  - squad_train/\n",
      "  - squad_validation/\n",
      "  - squad_train_prompts.json (for KD2)\n",
      "  - squad_val_prompts.json (for KD2)\n",
      "\n",
      "Next Steps:\n",
      "  1. Run 04_teacher_cache_outputs.ipynb to cache teacher outputs\n",
      "  2. Run 05_train_baseline_and_kd1.ipynb for training\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"SQUAD DATA PREPARATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Dataset: SQuAD v1.1 (Extractive QA)\n",
    "Mode: {'FAST' if config.fast_mode else 'FULL'}\n",
    "\n",
    "Sizes:\n",
    "  Train: {len(tokenized_train)} examples\n",
    "  Validation: {len(tokenized_val)} examples\n",
    "\n",
    "Files saved to: {DATA_DIR}\n",
    "  - squad_train/\n",
    "  - squad_validation/\n",
    "  - squad_train_prompts.json (for KD2)\n",
    "  - squad_val_prompts.json (for KD2)\n",
    "\n",
    "Next Steps:\n",
    "  1. Run 04_teacher_cache_outputs.ipynb to cache teacher outputs\n",
    "  2. Run 05_train_baseline_and_kd1.ipynb for training\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
