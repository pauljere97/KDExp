{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68786abc",
   "metadata": {},
   "source": [
    "# 01 - Environment Setup\n",
    "\n",
    "**Thesis Section Reference:** Chapter 3.4 - Experimental Setup\n",
    "\n",
    "This notebook sets up the environment for knowledge distillation experiments:\n",
    "1. Load environment variables from `.env`\n",
    "2. Verify Python packages and versions\n",
    "3. Check device availability (MPS/CPU)\n",
    "4. Create directory structure\n",
    "5. Test basic model loading\n",
    "\n",
    "## Important Notes for MPS (Apple Silicon)\n",
    "- Use `device=\"mps\"` when available, fallback to CPU\n",
    "- Avoid bf16 (not supported on MPS)\n",
    "- Use fp32 for stability (fp16 can cause issues)\n",
    "- Clear MPS cache periodically with `torch.mps.empty_cache()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429d1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables FIRST\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "ROOT_DIR = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(ROOT_DIR / \"src\"))\n",
    "\n",
    "# Load .env file\n",
    "from dotenv import load_dotenv\n",
    "env_path = ROOT_DIR / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"✓ Loaded environment from {env_path}\")\n",
    "else:\n",
    "    print(f\"⚠ Warning: .env file not found at {env_path}\")\n",
    "    print(\"  Please copy .env.example to .env and fill in your values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce3c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify required packages\n",
    "import importlib\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    (\"torch\", \"2.0.0\"),\n",
    "    (\"transformers\", \"4.30.0\"),\n",
    "    (\"datasets\", \"2.0.0\"),\n",
    "    (\"evaluate\", \"0.4.0\"),\n",
    "    (\"accelerate\", \"0.20.0\"),\n",
    "    (\"peft\", \"0.4.0\"),\n",
    "    (\"numpy\", \"1.20.0\"),\n",
    "    (\"pandas\", \"1.3.0\"),\n",
    "    (\"matplotlib\", \"3.4.0\"),\n",
    "    (\"scipy\", \"1.7.0\"),\n",
    "    (\"psutil\", \"5.8.0\"),\n",
    "    (\"pyyaml\", \"5.4.0\"),\n",
    "    (\"python-dotenv\", \"0.19.0\"),\n",
    "    (\"tqdm\", \"4.60.0\"),\n",
    "]\n",
    "\n",
    "print(\"Checking required packages...\\n\")\n",
    "all_ok = True\n",
    "\n",
    "for package, min_version in REQUIRED_PACKAGES:\n",
    "    try:\n",
    "        # Handle package name differences\n",
    "        import_name = package.replace(\"-\", \"_\")\n",
    "        if import_name == \"python_dotenv\":\n",
    "            import_name = \"dotenv\"\n",
    "        if import_name == \"pyyaml\":\n",
    "            import_name = \"yaml\"\n",
    "            \n",
    "        mod = importlib.import_module(import_name)\n",
    "        version = getattr(mod, \"__version__\", \"unknown\")\n",
    "        print(f\"  ✓ {package}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"  ✗ {package}: NOT INSTALLED\")\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\n✓ All required packages are installed\")\n",
    "else:\n",
    "    print(\"\\n⚠ Some packages are missing. Install with:\")\n",
    "    print(\"  pip install torch transformers datasets evaluate accelerate peft\")\n",
    "    print(\"  pip install numpy pandas matplotlib scipy psutil pyyaml python-dotenv tqdm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a021d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device availability\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Device Availability Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# MPS (Apple Silicon)\n",
    "mps_available = torch.backends.mps.is_available()\n",
    "mps_built = torch.backends.mps.is_built()\n",
    "print(f\"\\nMPS (Apple Silicon):\")\n",
    "print(f\"  Built: {mps_built}\")\n",
    "print(f\"  Available: {mps_available}\")\n",
    "\n",
    "# CUDA\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\nCUDA:\")\n",
    "print(f\"  Available: {cuda_available}\")\n",
    "if cuda_available:\n",
    "    print(f\"  Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"  Current device: {torch.cuda.current_device()}\")\n",
    "\n",
    "# Determine best device\n",
    "if mps_available:\n",
    "    DEVICE = \"mps\"\n",
    "elif cuda_available:\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "# Override from environment\n",
    "env_device = os.getenv(\"DEVICE\", \"\").lower()\n",
    "if env_device in [\"mps\", \"cuda\", \"cpu\"]:\n",
    "    if env_device == \"mps\" and not mps_available:\n",
    "        print(f\"\\n⚠ Requested MPS but not available, using {DEVICE}\")\n",
    "    elif env_device == \"cuda\" and not cuda_available:\n",
    "        print(f\"\\n⚠ Requested CUDA but not available, using {DEVICE}\")\n",
    "    else:\n",
    "        DEVICE = env_device\n",
    "\n",
    "print(f\"\\n→ Selected device: {DEVICE}\")\n",
    "\n",
    "# Test device\n",
    "try:\n",
    "    x = torch.randn(10, 10).to(DEVICE)\n",
    "    y = x @ x.T\n",
    "    print(f\"  ✓ Device test passed\")\n",
    "    del x, y\n",
    "    if DEVICE == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Device test failed: {e}\")\n",
    "    DEVICE = \"cpu\"\n",
    "    print(f\"  → Falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16aa473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system memory\n",
    "import psutil\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"System Memory\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"\\nTotal RAM: {mem.total / (1024**3):.1f} GB\")\n",
    "print(f\"Available RAM: {mem.available / (1024**3):.1f} GB\")\n",
    "print(f\"Used RAM: {mem.used / (1024**3):.1f} GB ({mem.percent}%)\")\n",
    "\n",
    "# Recommendations based on memory\n",
    "available_gb = mem.available / (1024**3)\n",
    "print(f\"\\nRecommendations based on {available_gb:.1f} GB available:\")\n",
    "\n",
    "if available_gb < 8:\n",
    "    print(\"  ⚠ Low memory - use FAST_MODE=true and smaller models\")\n",
    "    print(\"  ⚠ Consider using teacher_model_local_fallback\")\n",
    "elif available_gb < 16:\n",
    "    print(\"  ✓ Moderate memory - FAST_MODE recommended\")\n",
    "    print(\"  ✓ Can run 3B teacher models\")\n",
    "elif available_gb < 32:\n",
    "    print(\"  ✓ Good memory - Can run most experiments\")\n",
    "    print(\"  ✓ May need gradient checkpointing for 7B models\")\n",
    "else:\n",
    "    print(\"  ✓ Excellent memory - Full experiments possible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8392b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment configuration\n",
    "from config import load_config\n",
    "\n",
    "try:\n",
    "    config = load_config(str(ROOT_DIR / \"configs\" / \"experiment.yaml\"))\n",
    "    print(\"✓ Configuration loaded successfully\")\n",
    "    print(f\"\\nExperiment: {config.experiment_name}\")\n",
    "    print(f\"Fast mode: {config.fast_mode}\")\n",
    "    print(f\"Device: {config.get_device()}\")\n",
    "    print(f\"Precision: {config.get_precision()}\")\n",
    "    print(f\"\\nModels:\")\n",
    "    print(f\"  Teacher (primary): {config.teacher.primary}\")\n",
    "    print(f\"  Teacher (fallback): {config.teacher.local_fallback}\")\n",
    "    print(f\"  Student S1: {config.student_s1.name}\")\n",
    "    print(f\"  Student S2: {config.student_s2.name}\")\n",
    "    print(f\"\\nTraining:\")\n",
    "    print(f\"  Epochs: {config.get_epochs()}\")\n",
    "    print(f\"  Batch size: {config.get_batch_size()}\")\n",
    "    print(f\"  Seeds: {config.get_seeds()}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to load configuration: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "directories = [\n",
    "    ROOT_DIR / \"results\" / \"raw_runs\",\n",
    "    ROOT_DIR / \"results\" / \"summary\",\n",
    "    ROOT_DIR / \"results\" / \"figures\",\n",
    "    ROOT_DIR / \"results\" / \"models\",\n",
    "    ROOT_DIR / \"results\" / \"teacher_cache\",\n",
    "    ROOT_DIR / \"hf_cache\",\n",
    "]\n",
    "\n",
    "print(\"Creating directory structure...\")\n",
    "for dir_path in directories:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"  ✓ {dir_path.relative_to(ROOT_DIR)}\")\n",
    "\n",
    "# Create .gitkeep files\n",
    "for dir_path in directories:\n",
    "    gitkeep = dir_path / \".gitkeep\"\n",
    "    gitkeep.touch(exist_ok=True)\n",
    "\n",
    "print(\"\\n✓ Directory structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1655305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Hugging Face authentication\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\", \"\")\n",
    "\n",
    "if hf_token and hf_token != \"your_huggingface_token_here\":\n",
    "    try:\n",
    "        # Try to login\n",
    "        login(token=hf_token, add_to_git_credential=False)\n",
    "        api = HfApi()\n",
    "        user_info = api.whoami()\n",
    "        print(f\"✓ Logged in to Hugging Face as: {user_info.get('name', 'Unknown')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ HF authentication warning: {e}\")\n",
    "        print(\"  You may not be able to access gated models\")\n",
    "else:\n",
    "    print(\"⚠ No HF_TOKEN found in .env\")\n",
    "    print(\"  Some models may require authentication\")\n",
    "    print(\"  Get your token from: https://huggingface.co/settings/tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452bbbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model loading (with fallback)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gc\n",
    "\n",
    "print(\"Testing model loading...\\n\")\n",
    "\n",
    "# Try student model first (smaller)\n",
    "student_model_name = os.getenv(\"STUDENT_S1\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "try:\n",
    "    print(f\"Loading tokenizer: {student_model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        student_model_name,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "    )\n",
    "    \n",
    "    # Ensure pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"  ✓ Tokenizer loaded (vocab size: {tokenizer.vocab_size})\")\n",
    "    \n",
    "    print(f\"\\nLoading model: {student_model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        student_model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float32,  # Use fp32 for MPS stability\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  ✓ Model loaded ({total_params / 1e9:.2f}B parameters)\")\n",
    "    \n",
    "    # Test inference\n",
    "    print(f\"\\nTesting inference on {DEVICE}...\")\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"  ✓ Generation test passed\")\n",
    "    print(f\"  Response: {response[:100]}...\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, tokenizer, inputs, outputs\n",
    "    gc.collect()\n",
    "    if DEVICE == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    print(\"\\n✓ Model loading test passed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Model loading failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Check your internet connection\")\n",
    "    print(\"  2. Verify HF_TOKEN if using gated models\")\n",
    "    print(\"  3. Try a different model in .env\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b15610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"ENVIRONMENT SETUP COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Configuration:\n",
    "  Fast Mode: {os.getenv('FAST_MODE', 'true')}\n",
    "  Device: {DEVICE}\n",
    "  Precision: fp32 (recommended for MPS stability)\n",
    "\n",
    "Models (from .env):\n",
    "  Teacher Primary: {os.getenv('TEACHER_MODEL_PRIMARY', 'not set')}\n",
    "  Teacher Fallback: {os.getenv('TEACHER_MODEL_FALLBACK', 'not set')}\n",
    "  Student S1: {os.getenv('STUDENT_S1', 'not set')}\n",
    "  Student S2: {os.getenv('STUDENT_S2', 'not set')}\n",
    "\n",
    "Directories:\n",
    "  Results: {ROOT_DIR / 'results'}\n",
    "  HF Cache: {ROOT_DIR / 'hf_cache'}\n",
    "\n",
    "Next Steps:\n",
    "  1. Run 02_data_prep_sst2.ipynb to prepare SST-2 data\n",
    "  2. Run 03_data_prep_squad.ipynb to prepare SQuAD data\n",
    "  3. Run 04_teacher_cache_outputs.ipynb to cache teacher outputs\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
