{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68786abc",
   "metadata": {},
   "source": [
    "# 01 - Environment Setup\n",
    "\n",
    "**Thesis Section Reference:** Chapter 3.4 - Experimental Setup\n",
    "\n",
    "This notebook sets up the environment for knowledge distillation experiments:\n",
    "1. Load environment variables from `.env`\n",
    "2. Verify Python packages and versions\n",
    "3. Check device availability (MPS/CPU)\n",
    "4. Create directory structure\n",
    "5. Test basic model loading\n",
    "\n",
    "## Important Notes for MPS (Apple Silicon)\n",
    "- Use `device=\"mps\"` when available, fallback to CPU\n",
    "- Avoid bf16 (not supported on MPS)\n",
    "- Use fp32 for stability (fp16 can cause issues)\n",
    "- Clear MPS cache periodically with `torch.mps.empty_cache()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "429d1539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded environment from /Users/pjere/Workshop/thesis-exp/.env\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables FIRST\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "ROOT_DIR = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "sys.path.insert(0, str(ROOT_DIR / \"src\"))\n",
    "\n",
    "# Load .env file\n",
    "from dotenv import load_dotenv\n",
    "env_path = ROOT_DIR / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"✓ Loaded environment from {env_path}\")\n",
    "else:\n",
    "    print(f\"⚠ Warning: .env file not found at {env_path}\")\n",
    "    print(\"  Please copy .env.example to .env and fill in your values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ce3c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking required packages...\n",
      "\n",
      "  ✓ torch: 2.10.0\n",
      "  ✓ transformers: 5.0.0\n",
      "  ✓ datasets: 4.5.0\n",
      "  ✓ evaluate: 0.4.6\n",
      "  ✓ accelerate: 1.12.0\n",
      "  ✓ peft: 0.18.1\n",
      "  ✓ numpy: 2.4.2\n",
      "  ✓ pandas: 3.0.0\n",
      "  ✓ matplotlib: 3.10.8\n",
      "  ✓ scipy: 1.17.0\n",
      "  ✓ psutil: 7.2.2\n",
      "  ✓ pyyaml: 6.0.3\n",
      "  ✓ python-dotenv: unknown\n",
      "  ✓ tqdm: 4.67.2\n",
      "\n",
      "✓ All required packages are installed\n"
     ]
    }
   ],
   "source": [
    "# Verify required packages\n",
    "import importlib\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    (\"torch\", \"2.0.0\"),\n",
    "    (\"transformers\", \"4.30.0\"),\n",
    "    (\"datasets\", \"2.0.0\"),\n",
    "    (\"evaluate\", \"0.4.0\"),\n",
    "    (\"accelerate\", \"0.20.0\"),\n",
    "    (\"peft\", \"0.4.0\"),\n",
    "    (\"numpy\", \"1.20.0\"),\n",
    "    (\"pandas\", \"1.3.0\"),\n",
    "    (\"matplotlib\", \"3.4.0\"),\n",
    "    (\"scipy\", \"1.7.0\"),\n",
    "    (\"psutil\", \"5.8.0\"),\n",
    "    (\"pyyaml\", \"5.4.0\"),\n",
    "    (\"python-dotenv\", \"0.19.0\"),\n",
    "    (\"tqdm\", \"4.60.0\"),\n",
    "]\n",
    "\n",
    "print(\"Checking required packages...\\n\")\n",
    "all_ok = True\n",
    "\n",
    "for package, min_version in REQUIRED_PACKAGES:\n",
    "    try:\n",
    "        # Handle package name differences\n",
    "        import_name = package.replace(\"-\", \"_\")\n",
    "        if import_name == \"python_dotenv\":\n",
    "            import_name = \"dotenv\"\n",
    "        if import_name == \"pyyaml\":\n",
    "            import_name = \"yaml\"\n",
    "            \n",
    "        mod = importlib.import_module(import_name)\n",
    "        version = getattr(mod, \"__version__\", \"unknown\")\n",
    "        print(f\"  ✓ {package}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"  ✗ {package}: NOT INSTALLED\")\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\n✓ All required packages are installed\")\n",
    "else:\n",
    "    print(\"\\n⚠ Some packages are missing. Install with:\")\n",
    "    print(\"  pip install torch transformers datasets evaluate accelerate peft\")\n",
    "    print(\"  pip install numpy pandas matplotlib scipy psutil pyyaml python-dotenv tqdm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a021d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Device Availability Check\n",
      "==================================================\n",
      "\n",
      "MPS (Apple Silicon):\n",
      "  Built: True\n",
      "  Available: True\n",
      "\n",
      "CUDA:\n",
      "  Available: False\n",
      "\n",
      "→ Selected device: mps\n",
      "  ✓ Device test passed\n"
     ]
    }
   ],
   "source": [
    "# Check device availability\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Device Availability Check\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# MPS (Apple Silicon)\n",
    "mps_available = torch.backends.mps.is_available()\n",
    "mps_built = torch.backends.mps.is_built()\n",
    "print(f\"\\nMPS (Apple Silicon):\")\n",
    "print(f\"  Built: {mps_built}\")\n",
    "print(f\"  Available: {mps_available}\")\n",
    "\n",
    "# CUDA\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"\\nCUDA:\")\n",
    "print(f\"  Available: {cuda_available}\")\n",
    "if cuda_available:\n",
    "    print(f\"  Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"  Current device: {torch.cuda.current_device()}\")\n",
    "\n",
    "# Determine best device\n",
    "if mps_available:\n",
    "    DEVICE = \"mps\"\n",
    "elif cuda_available:\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "# Override from environment\n",
    "env_device = os.getenv(\"DEVICE\", \"\").lower()\n",
    "if env_device in [\"mps\", \"cuda\", \"cpu\"]:\n",
    "    if env_device == \"mps\" and not mps_available:\n",
    "        print(f\"\\n⚠ Requested MPS but not available, using {DEVICE}\")\n",
    "    elif env_device == \"cuda\" and not cuda_available:\n",
    "        print(f\"\\n⚠ Requested CUDA but not available, using {DEVICE}\")\n",
    "    else:\n",
    "        DEVICE = env_device\n",
    "\n",
    "print(f\"\\n→ Selected device: {DEVICE}\")\n",
    "\n",
    "# Test device\n",
    "try:\n",
    "    x = torch.randn(10, 10).to(DEVICE)\n",
    "    y = x @ x.T\n",
    "    print(f\"  ✓ Device test passed\")\n",
    "    del x, y\n",
    "    if DEVICE == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "except Exception as e:\n",
    "    print(f\"  ✗ Device test failed: {e}\")\n",
    "    DEVICE = \"cpu\"\n",
    "    print(f\"  → Falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a16aa473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "System Memory\n",
      "==================================================\n",
      "\n",
      "Total RAM: 36.0 GB\n",
      "Available RAM: 14.3 GB\n",
      "Used RAM: 16.5 GB (60.3%)\n",
      "\n",
      "Recommendations based on 14.3 GB available:\n",
      "  ✓ Moderate memory - FAST_MODE recommended\n",
      "  ✓ Can run 3B teacher models\n"
     ]
    }
   ],
   "source": [
    "# Check system memory\n",
    "import psutil\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"System Memory\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"\\nTotal RAM: {mem.total / (1024**3):.1f} GB\")\n",
    "print(f\"Available RAM: {mem.available / (1024**3):.1f} GB\")\n",
    "print(f\"Used RAM: {mem.used / (1024**3):.1f} GB ({mem.percent}%)\")\n",
    "\n",
    "# Recommendations based on memory\n",
    "available_gb = mem.available / (1024**3)\n",
    "print(f\"\\nRecommendations based on {available_gb:.1f} GB available:\")\n",
    "\n",
    "if available_gb < 8:\n",
    "    print(\"  ⚠ Low memory - use FAST_MODE=true and smaller models\")\n",
    "    print(\"  ⚠ Consider using teacher_model_local_fallback\")\n",
    "elif available_gb < 16:\n",
    "    print(\"  ✓ Moderate memory - FAST_MODE recommended\")\n",
    "    print(\"  ✓ Can run 3B teacher models\")\n",
    "elif available_gb < 32:\n",
    "    print(\"  ✓ Good memory - Can run most experiments\")\n",
    "    print(\"  ✓ May need gradient checkpointing for 7B models\")\n",
    "else:\n",
    "    print(\"  ✓ Excellent memory - Full experiments possible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8392b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded successfully\n",
      "\n",
      "Experiment: kd_thesis_experiments\n",
      "Fast mode: True\n",
      "Device: mps\n",
      "Precision: fp32\n",
      "\n",
      "Models:\n",
      "  Teacher (primary): meta-llama/Llama-3.2-8B-Instruct\n",
      "  Teacher (fallback): Qwen/Qwen2.5-3B-Instruct\n",
      "  Student S1: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  Student S2: quantized-proxy\n",
      "\n",
      "Training:\n",
      "  Epochs: 1\n",
      "  Batch size: 2\n",
      "  Seeds: [42]\n"
     ]
    }
   ],
   "source": [
    "# Load experiment configuration\n",
    "from config import load_config\n",
    "\n",
    "try:\n",
    "    config = load_config(str(ROOT_DIR / \"configs\" / \"experiment.yaml\"))\n",
    "    print(\"✓ Configuration loaded successfully\")\n",
    "    print(f\"\\nExperiment: {config.experiment_name}\")\n",
    "    print(f\"Fast mode: {config.fast_mode}\")\n",
    "    print(f\"Device: {config.get_device()}\")\n",
    "    print(f\"Precision: {config.get_precision()}\")\n",
    "    print(f\"\\nModels:\")\n",
    "    print(f\"  Teacher (primary): {config.teacher.primary}\")\n",
    "    print(f\"  Teacher (fallback): {config.teacher.local_fallback}\")\n",
    "    print(f\"  Student S1: {config.student_s1.name}\")\n",
    "    print(f\"  Student S2: {config.student_s2.name}\")\n",
    "    print(f\"\\nTraining:\")\n",
    "    print(f\"  Epochs: {config.get_epochs()}\")\n",
    "    print(f\"  Batch size: {config.get_batch_size()}\")\n",
    "    print(f\"  Seeds: {config.get_seeds()}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to load configuration: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a50b86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory structure...\n",
      "  ✓ results/raw_runs\n",
      "  ✓ results/summary\n",
      "  ✓ results/figures\n",
      "  ✓ results/models\n",
      "  ✓ results/teacher_cache\n",
      "  ✓ hf_cache\n",
      "\n",
      "✓ Directory structure created\n"
     ]
    }
   ],
   "source": [
    "# Create directory structure\n",
    "directories = [\n",
    "    ROOT_DIR / \"results\" / \"raw_runs\",\n",
    "    ROOT_DIR / \"results\" / \"summary\",\n",
    "    ROOT_DIR / \"results\" / \"figures\",\n",
    "    ROOT_DIR / \"results\" / \"models\",\n",
    "    ROOT_DIR / \"results\" / \"teacher_cache\",\n",
    "    ROOT_DIR / \"hf_cache\",\n",
    "]\n",
    "\n",
    "print(\"Creating directory structure...\")\n",
    "for dir_path in directories:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"  ✓ {dir_path.relative_to(ROOT_DIR)}\")\n",
    "\n",
    "# Create .gitkeep files\n",
    "for dir_path in directories:\n",
    "    gitkeep = dir_path / \".gitkeep\"\n",
    "    gitkeep.touch(exist_ok=True)\n",
    "\n",
    "print(\"\\n✓ Directory structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1655305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged in to Hugging Face as: pauljere97\n"
     ]
    }
   ],
   "source": [
    "# Test Hugging Face authentication\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\", \"\")\n",
    "\n",
    "if hf_token and hf_token != \"your_huggingface_token_here\":\n",
    "    try:\n",
    "        # Try to login\n",
    "        login(token=hf_token, add_to_git_credential=False)\n",
    "        api = HfApi()\n",
    "        user_info = api.whoami()\n",
    "        print(f\"✓ Logged in to Hugging Face as: {user_info.get('name', 'Unknown')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ HF authentication warning: {e}\")\n",
    "        print(\"  You may not be able to access gated models\")\n",
    "else:\n",
    "    print(\"⚠ No HF_TOKEN found in .env\")\n",
    "    print(\"  Some models may require authentication\")\n",
    "    print(\"  Get your token from: https://huggingface.co/settings/tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "452bbbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model loading...\n",
      "\n",
      "Loading tokenizer: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f497f4bbd76647f1bdc30af7f9323b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6fe2134ecd4eb9a3480f1d77c4a9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c47ef33687747728752b6a46925868d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632b938a891443689ed8dfe930e7c523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0e23226193453ca95db931e2de1316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Tokenizer loaded (vocab size: 32000)\n",
      "\n",
      "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265d364d20a7422eb42884507f42e51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1227604b701c4dd3b0135cd992fa4385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b7c2ce418643dfb138e58ea8b5f7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Model loaded (1.10B parameters)\n",
      "\n",
      "Testing inference on mps...\n",
      "  ✓ Generation test passed\n",
      "  Response: Hello, how are you?\n",
      "I am fine, thank you.\n",
      "How...\n",
      "\n",
      "✓ Model loading test passed\n"
     ]
    }
   ],
   "source": [
    "# Test model loading (with fallback)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gc\n",
    "\n",
    "print(\"Testing model loading...\\n\")\n",
    "\n",
    "# Try student model first (smaller)\n",
    "student_model_name = os.getenv(\"STUDENT_S1\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "try:\n",
    "    print(f\"Loading tokenizer: {student_model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        student_model_name,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "    )\n",
    "    \n",
    "    # Ensure pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"  ✓ Tokenizer loaded (vocab size: {tokenizer.vocab_size})\")\n",
    "    \n",
    "    print(f\"\\nLoading model: {student_model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        student_model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float32,  # Use fp32 for MPS stability\n",
    "        cache_dir=str(ROOT_DIR / \"hf_cache\")\n",
    "    )\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  ✓ Model loaded ({total_params / 1e9:.2f}B parameters)\")\n",
    "    \n",
    "    # Test inference\n",
    "    print(f\"\\nTesting inference on {DEVICE}...\")\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10)\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"  ✓ Generation test passed\")\n",
    "    print(f\"  Response: {response[:100]}...\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del model, tokenizer, inputs, outputs\n",
    "    gc.collect()\n",
    "    if DEVICE == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    print(\"\\n✓ Model loading test passed\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Model loading failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Check your internet connection\")\n",
    "    print(\"  2. Verify HF_TOKEN if using gated models\")\n",
    "    print(\"  3. Try a different model in .env\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b15610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENVIRONMENT SETUP COMPLETE\n",
      "============================================================\n",
      "\n",
      "Configuration:\n",
      "  Fast Mode: true\n",
      "  Device: mps\n",
      "  Precision: fp32 (recommended for MPS stability)\n",
      "\n",
      "Models (from .env):\n",
      "  Teacher Primary: meta-llama/Llama-3.2-8B-Instruct\n",
      "  Teacher Fallback: Qwen/Qwen2.5-3B-Instruct\n",
      "  Student S1: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  Student S2: quantized-proxy\n",
      "\n",
      "Directories:\n",
      "  Results: /Users/pjere/Workshop/thesis-exp/results\n",
      "  HF Cache: /Users/pjere/Workshop/thesis-exp/hf_cache\n",
      "\n",
      "Next Steps:\n",
      "  1. Run 02_data_prep_sst2.ipynb to prepare SST-2 data\n",
      "  2. Run 03_data_prep_squad.ipynb to prepare SQuAD data\n",
      "  3. Run 04_teacher_cache_outputs.ipynb to cache teacher outputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Environment Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"ENVIRONMENT SETUP COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Configuration:\n",
    "  Fast Mode: {os.getenv('FAST_MODE', 'true')}\n",
    "  Device: {DEVICE}\n",
    "  Precision: fp32 (recommended for MPS stability)\n",
    "\n",
    "Models (from .env):\n",
    "  Teacher Primary: {os.getenv('TEACHER_MODEL_PRIMARY', 'not set')}\n",
    "  Teacher Fallback: {os.getenv('TEACHER_MODEL_FALLBACK', 'not set')}\n",
    "  Student S1: {os.getenv('STUDENT_S1', 'not set')}\n",
    "  Student S2: {os.getenv('STUDENT_S2', 'not set')}\n",
    "\n",
    "Directories:\n",
    "  Results: {ROOT_DIR / 'results'}\n",
    "  HF Cache: {ROOT_DIR / 'hf_cache'}\n",
    "\n",
    "Next Steps:\n",
    "  1. Run 02_data_prep_sst2.ipynb to prepare SST-2 data\n",
    "  2. Run 03_data_prep_squad.ipynb to prepare SQuAD data\n",
    "  3. Run 04_teacher_cache_outputs.ipynb to cache teacher outputs\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
